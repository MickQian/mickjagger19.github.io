<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>MachineLearning on Mick&#39; Blog</title>
    <link>https://mickjagger19.github.io/tags/machinelearning/</link>
    <description>Recent content in MachineLearning on Mick&#39; Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 14 Jan 2024 14:05:03 +0800</lastBuildDate><atom:link href="https://mickjagger19.github.io/tags/machinelearning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>VAE</title>
      <link>https://mickjagger19.github.io/posts/ai/models/vae/</link>
      <pubDate>Sun, 14 Jan 2024 14:05:03 +0800</pubDate>
      
      <guid>https://mickjagger19.github.io/posts/ai/models/vae/</guid>
      <description>Takeaways from the maths of VAE</description>
      <content:encoded><![CDATA[<h2 id="terminology">Terminology</h2>
<table>
  <thead>
      <tr>
          <th>Notations</th>
          <th>Mean</th>
          <th></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>$X \sim p_{r}$</td>
          <td>the input data</td>
          <td></td>
      </tr>
      <tr>
          <td>$z$</td>
          <td>the encoded latent</td>
          <td></td>
      </tr>
      <tr>
          <td>$\theta$</td>
          <td>the parameterized model</td>
          <td></td>
      </tr>
      <tr>
          <td>$\phi$</td>
          <td>the encoder</td>
          <td></td>
      </tr>
      <tr>
          <td>$p_{\theta}(x)$</td>
          <td>the likelihood of the data-reconstruction</td>
          <td></td>
      </tr>
      <tr>
          <td>$p(z)$</td>
          <td>the distribution of latent variable $z$ as a prior, often $\mathcal{N}(0,1)$</td>
          <td></td>
      </tr>
      <tr>
          <td>$q_{\phi}(z|x)$</td>
          <td>variational distribution</td>
          <td></td>
      </tr>
      <tr>
          <td>$q_{\phi}(z|x)$</td>
          <td>variational distribution</td>
          <td></td>
      </tr>
      <tr>
          <td><strong>MDL</strong>(Minimum Description Length)</td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td><a href="https://en.wikipedia.org/wiki/Information_content">Self-Information $I$</a></td>
          <td>the amount of information, interpreted as the level of &ldquo;surprise&rdquo;<br>$$I(\mathcal{w}<em>{n}) = f(P(\mathcal{w}</em>{n})) = -\log(P(\mathcal{w}_{n})) \ge 0$$</td>
          <td></td>
      </tr>
      <tr>
          <td>Entropy $H(X)$</td>
          <td>the average amount of information in a message. A measure of <strong>uncertainty</strong>. <br>$$H(X) = E[I(X)] = E[-\ln(P(X))]$$<br></td>
          <td></td>
      </tr>
  </tbody>
</table>
<h2 id="background">Background</h2>
<p><a href="https://en.wikipedia.org/wiki/Autoencoder"><strong>AutoEncoder</strong></a> is proposed to compress data and reduct dimensionality as a generalization of PCA, and largely used in <strong>signal processing</strong>, until someone found new samples can be generated by adding noise to latents and decoded by decoder.</p>
<p>However, the ability of AutoEncoder to generate new samples by the distribution of the latents $z$, this is why &amp; when <strong>Variational AutoEncoder</strong> is developed.</p>
<blockquote>
<p>[!TIP]
AE is an approach of <strong>MDL</strong></p>
</blockquote>
<h2 id="requirements">Requirements</h2>
<ul>
<li>In order to be able to generate new samples using decoder, we will be happy if $z \sim \mathcal{N}(0, 1)$</li>
</ul>
<h2 id="modeling">Modeling</h2>
<p>We apply <strong>Maximum Likelihood Estimation</strong> here.</p>
<p><strong>Log Likelihood</strong> is defined as:
$$
Likelihood = \log P_{\theta}(X)
$$</p>
<p>which represents the ability of the model to reconstruct the input data.</p>
<p>Hence, from the definition of the loss function:</p>
<p>$$
\mathcal{L}(\theta) = - \mathbb E_{x \sim data}[\log p_{\theta}(x)]
$$</p>
<p>Normally, the $x\sim data$ is neglected.</p>
<p>Our goal is to minimize the loss function, in the mean time force encoder to encode $X$ as $z \sim \mathcal{N}(\mu, \sigma^{2}I)$</p>
<h3 id="implicit-model">Implicit Model</h3>
<p>We define $z$ as an implicit variable, making our model an <strong>implicit model</strong>.</p>
<p>Rewrite the log-likelihood:
$$
p_{\theta}(x) = \int{p_{\theta}}(x|z)p_{\theta }(z)dz
$$
where $\theta$ is the parameter of the implicit model (encoder and decoder).</p>
<p>However there&rsquo;s a common problem for implicit models: the integration relies on the exhaustion on implicit variable $z$.</p>
<p>In our case, as $z \sim \mathcal{N}(\mu, \sigma^{2}I)$, it is deem impossible.</p>
<h3 id="mc">MC</h3>
<p>Monte-Carlo is a method to approximate an intractable <del>equation</del>(integration) by sampling a lot of data ($p_{\theta}(x | z)$):
$$
\begin{align*}
p_{\theta}(x) &amp;= \int{p_{\theta}}(x|z)p_{\theta }(z)dz\\
&amp;\approx \frac{1}{m} \sum\limits_{j =1}^{m} p_{\theta}(x | z_{j})
\end{align*}
$$
But that does not enforce  $z \sim \mathcal{N}(\mu, \sigma^{2}I)$.</p>
<h3 id="variational-bayes">Variational Bayes</h3>
<h4 id="deriving-elbo">Deriving ELBO</h4>
<p>Considering the log-likelihood can be rewritten in the following process:</p>
<p>$$
\begin{align*}
\log p_{\theta}(x) &amp;= \log p_{\theta}(x) \int_{z}p_{\phi}(z|x)dz &amp;\text{Normalization}
\\
&amp;= \int_{z}p_{\theta}(z|x)\log p_{\theta}(x)dz
\\
&amp;=  \int_{z}p_{\theta}(z|x) \log \frac{p_{\theta}(x,z)}{p(z|x)} dz  &amp;\text{Bayes&rsquo; Theorem}
\\
&amp;= \int_{z}(p_{\theta}(z|x)\log p_{\theta}(x,z) - p_{\theta}(z|x)\log p(z|x))dz
\\
&amp;= \log p_{\theta}(x,z) - \log p_{\theta}(z|x)
\end{align*}
$$</p>
<p>Since the posterior $\log p_{\theta}(z|x)$ is intractable (only involves integration on latent variable $z$, see <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes&rsquo; Theorem</a>), a new distribution(which is easy to <em>learn</em>) $q_{\phi}(z|x)$ is used to approximate it, where $\phi$ is the encoder.</p>
<p>Let&rsquo;s continue by replacing:</p>

$$
\begin{align*}
\underbrace{\log p(x)}_{\text{evidence}} &= \log p_\theta(x,z) - \log q_{\phi}(z|x) \newline
&= \int_{z} q_{\phi}(z|x)\log\frac{p_{\theta}(x,z)}{q_{\phi}(z|x)}dz \newline
&= \int_{z}q_{\phi}(z|x)\log(\frac{p_{\theta}(x,z)}{q_{\phi}(z|x)} \cdot  \frac{q_{\phi}(z|x)}{p(z|x)})dz \newline
&= \int_{z}q_{\phi}(z|x)\log(\frac{p_{\theta}(x,z)}{q_{\phi}(z|x)})dz + \int_{z}q_{\phi}(z|x)\log(\frac{q_{\phi}(z|x)}{p(z|x)})dz \newline
&= \mathcal L(\theta,\phi; x) + D_{KL}(q_{\phi}, p_{\theta}) \newline
&\ge \underbrace{\mathcal L(\theta,\phi; x)}_{\text{ ELBO }} & \text{$D_{KL}\ge 0$} 
\end{align*} 
$$

<p>$\mathcal L(\theta, \phi; x) = \mathbb{E}_{z \sim q(.|x)}{\log \frac{p_{\theta}(x,z)}{q_{\phi}(z|x)}}$  is <strong>ELBO</strong>(Evidence Lower Bound), as it is the lower bound of evidence $\mathcal{L}(\theta)$, omitting the <strong>KL</strong> term. Maximizing ELBO is directly:</p>
<ul>
<li>maximizing log-likelihood</li>
<li>minimizing KL-Divergence of posterior $p_{\theta}$ and variational distribution $q_{\phi}$</li>
</ul>
<h4 id="maximizing-elbo">Maximizing ELBO</h4>
<p>And we can break it down further:

$$
\begin{align*}
\underbrace{\mathcal L(\theta, \phi; x)}_{\text{ELBO}} &= \int_{z}q_{\phi}(z|x)\log(\frac{p_{\theta}(x,z)}{q_{\phi}(z|x)})dz = \mathcal{H}[q_{\phi}(z|x)] + \mathbb{E}_{z}[p_{\theta}(x,z)] \\\\
&= \int_{z}q_{\phi}(z|x)\log(\frac{p(z) * p_{\theta}(x|z) }{q_{\phi}(z|x)})dz & \text{Bayes' Theorem}\\\\
&= \int_{z}q_{\phi}(z|x)\log\frac{p(z) }{q_{\phi}(z|x)}dz + \int_{z}q_{\phi}(z|x)\log p_{\theta}(x|z)dz\\\\ 
&= \underbrace{-D_{KL}(q_{\phi}(z|x), p(z))}_{\text{$\mathcal L_{reg}$}} + \underbrace{\mathbb E_{q_{\phi}(z|x)}[\log p_{\theta}(x|z)]}_{\text{$\mathcal L_{reconstruct}$}}\\\\ 
\end{align*}
$$
</p>
<blockquote>
<p>[!Note]
$\int_{z}p(z)*f(z)dz = \mathbb E_{z \sim p(.)}[f(z)]$, which is the expectation of p with z sampled from $p(z)$</p>
</blockquote>
<p>This is ELBO:</p>
<ul>
<li>$\mathcal L_{reg}$: the KL-divergence of variational distribution and prior distribution</li>
<li>$\mathcal L_{reconstruct}$: the Expectation of log reconstruct-likelihood under <em>variational distribution</em></li>
</ul>
<p>Since $\mathcal{L}(\theta) = -\log p(x) \le - \text{ELBO}$, by maximizing ELBO, we can indirectly minimize $L(\theta)$.</p>
<p>Hence, we define $\mathcal{L} = -\text{ELBO}$.</p>
<h3 id="training">Training</h3>
<p>$$
\begin{align*}
\text{ELBO} &amp;= \underbrace{-D_{KL}(q_{\phi}(z|x), p(z))}<em>{\text{$\mathcal L</em>{reg}$}} + \underbrace{\mathbb E_{q_{\phi}(z|x)}[\log p_{\theta}(x|z)]}<em>{\text{$\mathcal L</em>{reconstruct}$}}\\
&amp;= \underbrace{-D_{KL}(q_{\phi}(z|x), p(z))}<em>{\text{$\mathcal L</em>{reg}$}} + MSE(x, \hat x)
\end{align*}
$$</p>
<p>As $z$  is <strong>sampled</strong> from $\sim q_{\phi}(z|x)$, which is a variational distribution, the gradient of ELBO will not be able to propagate back to encoder $\phi$ (in-differentiable, chain rule).</p>
<p>Thus, <strong>re-parameterization</strong> is applied: $z = \mu + \epsilon \times \sigma, \hat z \sim \mathcal{N}(0, I)$, where $\phi(X) = (\mu, \epsilon)$. This way, the gradient is passed back to $\phi$, by representing $z$ with the output of $\phi$, where $z$ participates in the loss-calculation</p>
<h2 id="problems">Problems</h2>
<h3 id="blurry-output">Blurry output</h3>
<ul>
<li>the prior: $p(z) \sim \mathcal{N}(0, I)$</li>
<li>MSE is used to measure $L_{reconstruct}$</li>
<li></li>
</ul>
<p>DAE:
corrupt X，降低图片的冗余度（图片的冗余性一般都很高）</p>
<h2 id="dall-e">Dall E</h2>
<p>两阶段：</p>
<ol>
<li>clip 构造对比学习的正负样本对</li>
<li>文本 -&gt; clip encoder -&gt; text embedding -&gt; (diffusion) prior -&gt; image embedding -&gt; diffusion model decoder -&gt; image</li>
</ol>
<p>transformer encoder 本质上是自回归模型，可以基于自注意力和输入，自回归地生成同类型的内容</p>
<p>![[Pasted image 20230618153050.png]]</p>
<p>![[Pasted image 20230618154911.png]]</p>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
