<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Mamba on Mick&#39; Blog</title>
    <link>https://mickjagger19.github.io/tags/mamba/</link>
    <description>Recent content in Mamba on Mick&#39; Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Thu, 08 Feb 2024 14:41:55 +0800</lastBuildDate><atom:link href="https://mickjagger19.github.io/tags/mamba/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Mamba</title>
      <link>https://mickjagger19.github.io/posts/ai/models/mamba/</link>
      <pubDate>Thu, 08 Feb 2024 14:41:55 +0800</pubDate>
      
      <guid>https://mickjagger19.github.io/posts/ai/models/mamba/</guid>
      <description>Brief introduction to Mamba</description>
      <content:encoded><![CDATA[<hr>
<p>title: Mamba
summary: Mamba
tags:</p>
<ul>
<li>SSM</li>
<li>Mamba</li>
<li>RNN
author: Mick
draft: false
date: 2024-01-30T12:38:39+0800
math: true</li>
</ul>
<hr>
<h2 id="notations">Notations</h2>
<table>
<thead>
<tr>
<th style="text-align:left">Notations</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">SSM</td>
<td style="text-align:left">A math model, used to describe and analyze the behavior of dynamic system. In DL, it helps to process <strong>sequence data</strong>, by projecting the data into latent space. <strong>State Equation</strong> and <strong>Observation Equation</strong> are two important components of a SSM</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
</tr>
</tbody>
</table>
<h2 id="background">Background</h2>
<p><a href="AI/Models/The%20Transformer%20Family.md">The Transformer Family</a> is  notorious for $\mathcal{O}(N^{2})$ time complexity when inferencing, but many of the model targeting this problem has less performance[[ ]]than Transformer.</p>
<h2 id="state-space-models">State Space Models</h2>
<p><strong>SSMs(Structured State Space Sequence Models, S4)</strong> is defined by four <strong>invariant</strong> parameters: $(\Delta, A, B, C)$ and a sequence-to-sequence transformation consisting of 2 phases:
$$</p>
<p>$$</p>
<h3 id="linear-systems">Linear Systems</h3>
<h4 id="1-discretization">1. Discretization</h4>
<p>$$
\begin{align}
(\Delta, A, B) &amp;\to (\bar A, \bar B)\
f_{A}(\Delta, A) &amp;= \bar A \
f_{B}(\Delta, A, B) &amp;= \bar B
\end{align}
$$</p>
<p>where $(f_{A}, f_{B})$ is called <strong>discretization rules</strong>, where there are many candidates.</p>
<p><strong>Discretization</strong> has deep connections with <strong>continuous time system</strong>, giving it additional properties:</p>
<ul>
<li>resolution invariant</li>
<li>auto normalization</li>
</ul>
<blockquote>
<p>[!NOTE]
Discretization is necessary for calculation efficiency</p>
</blockquote>
<h4 id="2-computation">2. Computation</h4>
<p>There are two ways to compute</p>
<table>
<thead>
<tr>
<th style="text-align:left">Notations</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">$A(\cdot )$</td>
<td style="text-align:left">State matrix</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
</tr>
</tbody>
</table>
<h4 id="linear-time-invariance-system">Linear Time Invariance System</h4>
<p>The parameters calculated in <a href="/posts/ai/models/mamba/#discretization">Discretization</a> is fixed in all timestampes, this property is called <strong>LTI</strong>.</p>
<p>LTI fails from several reasons:</p>
<ul>
<li><strong>invariant dynamics</strong> can&rsquo;t select correct information from the context, nor can it change the hidden state with input</li>
<li>global convolution requires only time-awareness, but lacks content-awareness</li>
</ul>
<h2 id="mamba">Mamba</h2>
<h3 id="motive-selection-as-a-compression">Motive: Selection as a compression</h3>
<p>A basic model of sequence modeling: <em>Compress the context into a smaller state</em>.</p>
<p>Interpreting popular sequence models from this view:</p>
<ul>
<li>Attention: No explicit compression, but stores all the context in KV-cache</li>
<li>Recurrent Model: Limited state, linear time training</li>
</ul>
<p>Strucuted SSMs maps <strong>each channel</strong> of input $x$ to output $y$ with high-dimensional implicit state $h$</p>
<p>Prior work of SSms avoids the massive calculation of $DN$ with iterative computation path, with relies on time-invariant.</p>
<h3 id="selection-mechainism">Selection Mechainism</h3>
<h2 id="notations-1">Notations</h2>
<table>
<thead>
<tr>
<th style="text-align:left">Notations</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Input-dependent dynamics</td>
<td style="text-align:left">A dynamic enabling sequence model to depend on input</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
</tr>
</tbody>
</table>
<p><strong>Mamba</strong> promote a <strong>Selection Mechanism</strong>:  increase input-dependent dynamics, with requires a hardware-aware algorithm.</p>
<p>A way of blending selection mechanism into model, is to let the inputs affects the parameters of sequence co-effects, e.g. inserting into the recurrent dynamics of <strong>RNN</strong> or convolution kernel of <strong>CNN</strong>.</p>
<ul>
<li>B and C is changed, to make it selective</li>
<li>$\Delta_{t}$ controls the focusness/neglection to current input</li>
<li>$A$ is possibly selective, but for simplicity, it is not considered</li>
<li>variant gap filters out the irrelavant noise labels within inputs</li>
</ul>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
