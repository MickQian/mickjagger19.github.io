<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Generative on Mick&#39; Blog</title>
    <link>https://mickjagger19.github.io/tags/generative/</link>
    <description>Recent content in Generative on Mick&#39; Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 14 Jan 2024 14:31:43 +0800</lastBuildDate><atom:link href="https://mickjagger19.github.io/tags/generative/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Denoising Diffusion Models</title>
      <link>https://mickjagger19.github.io/posts/ai/models/denoising-diffusion-models/</link>
      <pubDate>Sun, 14 Jan 2024 14:31:43 +0800</pubDate>
      
      <guid>https://mickjagger19.github.io/posts/ai/models/denoising-diffusion-models/</guid>
      <description>Personal takeaways of DDIM/DDPM</description>
      <content:encoded><![CDATA[<h2 id="terminologies">Terminologies</h2>
<table>
<thead>
<tr>
<th style="text-align:left">Term</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Diffusion Models</td>
<td style="text-align:left">models that can sample from a highly complex probability distribution(e.g. images of cars)</td>
</tr>
<tr>
<td style="text-align:left">Non-equilibrium thermodynamics</td>
<td style="text-align:left">a branch of <a href="https://en.wikipedia.org/wiki/Thermodynamics" title="Thermodynamics">thermodynamics</a> that deals with physical systems that are not in <a href="https://en.wikipedia.org/wiki/Thermodynamic_equilibrium" title="Thermodynamic equilibrium">thermodynamic equilibrium</a>, where &ldquo;there are no net <a href="https://en.wikipedia.org/wiki/Macroscopic" title="Macroscopic">macroscopic</a> <a href="https://en.wikipedia.org/wiki/Flow_(mathematics)" title="Flow (mathematics)">flows</a> of <a href="https://en.wikipedia.org/wiki/Matter" title="Matter">matter</a> nor of energy within a system or between systems&rdquo;. <br>It is often used by diffusion models as a technique to sample from distribution.</td>
</tr>
<tr>
<td style="text-align:left">Diffusion</td>
<td style="text-align:left">the <strong>net movement</strong> of anything, generally from a region of higher <a href="https://en.wikipedia.org/wiki/Concentration" title="Concentration">concentration</a> to a region of lower concentration. <br>Also a technique of <strong>Non-equilibrium thermodynamics</strong></td>
</tr>
<tr>
<td style="text-align:left">DDPM</td>
<td style="text-align:left">model that improves the performance of <strong>diffusion models</strong> by <strong>variational inference</strong></td>
</tr>
<tr>
<td style="text-align:left">DDIM</td>
<td style="text-align:left">a generalized version of DDPM, with better performance and less diversity and quality</td>
</tr>
<tr>
<td style="text-align:left"><strong>Jensen</strong> inequality</td>
<td style="text-align:left">$f(\sum\limits a_{i}x_{i}) \le \sum\limits a_{i}f(x_{i})$, where $a \ge 0, \sum\limits a_{i} = 1$<br>In other words, the Expected Value of a convex function $\ge$ the value of the function at the Expected Input <br></td>
</tr>
<tr>
<td style="text-align:left"><a href="https://en.wikipedia.org/wiki/Evidence_lower_bound">Variational Lower Bound</a>(short for <strong>VLB</strong>, a.k.a. Evidence Lower BOund, short for <strong>ELBO</strong>)</td>
<td style="text-align:left">A easy-to-train lower bound of Log-Likelihood, derived by using a prior $p(z)$ to approximate (implies <em>variational</em>) an <del>intractable</del> posterior $q$.</td>
</tr>
<tr>
<td style="text-align:left"><a href="https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant"><strong>Jacobian matrix</strong></a></td>
<td style="text-align:left">A matrix derived from a vector of function of several variables, with all its first-order partial derivatives. Suppose $f: R^{n} \to R^{m}$:<br>$$<br>J = [\frac{\partial{f}}{\partial{x_{1}}}&hellip;\frac{\partial{f}}{\partial{x_{n}}}]<br>$$</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
</tr>
</tbody>
</table>
<h2 id="notations">Notations</h2>
<table>
<thead>
<tr>
<th style="text-align:left">Notation</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">$x_{0}$</td>
<td style="text-align:left">the data point, where $t$ is the total count of timestamp</td>
</tr>
<tr>
<td style="text-align:left">$x_{t}$</td>
<td style="text-align:left">the data after applying $t$ times of forward iteration</td>
</tr>
<tr>
<td style="text-align:left">$\epsilon_t$</td>
<td style="text-align:left">the (standard gaussian) noise</td>
</tr>
<tr>
<td style="text-align:left">$\epsilon_{\theta}(x_t,t)$</td>
<td style="text-align:left">our model to predict the noise at each timestamp</td>
</tr>
<tr>
<td style="text-align:left">$\mu_{\theta}(x_t, t)$</td>
<td style="text-align:left">parameterized model to predict $x_{t-1}$ at time $t$</td>
</tr>
<tr>
<td style="text-align:left">$p(x_{0:T})$</td>
<td style="text-align:left">the joint distribution of $x_{0}, x_{1} &hellip; x_{T}$</td>
</tr>
</tbody>
</table>
<h2 id="introduction">Introduction</h2>
<p>This article will introduce the definitions of <strong>DDPM</strong> and <strong>DDIM</strong>.</p>
<p>As stated earlier, the work of <strong>DDIM</strong> is based on <strong>DDPM</strong>.</p>
<h2 id="ddpm">DDPM</h2>
<p><strong>Diffusion Models</strong> often involves modeling two processes:</p>
<ul>
<li><strong>forward process</strong>: noise data($x_{0}$) to data point($x_{t}$)</li>
<li><strong>reverse process</strong>: data point to noise data, the reversion of <em>forward process</em></li>
</ul>
<h3 id="forward-process">Forward Process</h3>
<p>As a improvement of Diffusion Models, <strong>DDPM</strong> models the forward process as:</p>
<p>$$
\begin{equation}
x_{t} = \alpha_{t} x_{t-1} + \beta_{t}\epsilon_{t}, \epsilon_{t} \sim \mathcal{N}(0,1), 0 \le t \le T, t \in \mathbb{Z}
\end{equation}
$$</p>
<p>where $\alpha, \beta &gt; 0, \alpha_{t}^{2}+ \beta_{t}^{2} = 1$. This can be viewed as:</p>
<ul>
<li>the remains from the previous data: $\alpha_{t} x_{t-1}$</li>
<li>the destruction by introducing noise $\epsilon_{t}$</li>
</ul>
<p>Accordingly, the conditional probability of $x_t$ would be:
$$
p(x_{t}| x_{t-1}) = \mathcal{N}(x_{t};\alpha_{t}x_{t-1}, \beta_{t}^{2}I)
$$</p>
<blockquote>
<p>[!NOTE]</p>
<ul>
<li>All $\alpha, \beta, T$ are constants</li>
<li>Apparently, this is a <strong>Markovian process</strong></li>
</ul>
</blockquote>
<h3 id="reverse-process">Reverse Process</h3>
<p>By applying the forward process for $T$ times, we have $t$ pairs of $(x_{t-1}, x_{t})$. This is our training data.</p>
<p>Reversing the forward process, the task of the reverse process should be:
learn how to get  $x_0$ from $x_{t}$, formally $x_0 \to x_{t}$.</p>
<p><strong>DDPM</strong> splits this process into $t$ steps of  $x_t \to x_{t-1}$.</p>
<blockquote>
<p>[!TIP]  The Methodology of DDPM
<strong>DDPM</strong> is a Likelihood-based Model.</p>
</blockquote>
<p>In the paper, they model each single step as a <strong>gaussian transition</strong>:
$$
p_{\theta}(x_{t-1}|x_{t}) = \mathcal{N}(x_{t-1};\mu_{\theta}(x_{t}, t), \Sigma_{\theta}(x_{t},t))
$$
where:</p>
<ul>
<li>$\mu_{\theta}(x_{t}, t)$: the mean value</li>
<li>$\Sigma_{\theta}(x_{t},t)$: the variance predictor (of reverse process).</li>
</ul>
<blockquote>
<p>[!NOTE]</p>
<ul>
<li>The noise is not gaussian noise multiplied by a factor, but predicted directly.</li>
<li>Not to confuse:
* $\Sigma_{\theta}(x_{t},t)$: the noise in reverse process. It has been tested positive to the reverse process
* $\epsilon_\theta(x_{t},t) \to \epsilon_{t}$ : the noise predictor (<strong>of forward process</strong>)</li>
</ul>
</blockquote>
<h4 id="mean-value-predictor-mu_thetax_t-t">Mean Value Predictor: $\mu_{\theta}(x_{t}, t)$</h4>
<p>In order to model the $\mu_{\theta}(x_{t}, t)$, from Bayes&rsquo; Theorem we have:
$$
p(x_{t-1}| x_{t},x_0)= \frac{p(x_t|x_{t-1}) p(x_{t-1} | x_0)}{p(x_{t}|x_{0}) }
$$</p>
<p>The process of induction would be:</p>
<ol>
<li>Predict $x_t$, $x_{t-1}$ from $x_0$</li>
<li>Replace all the variables($x_{0}$) with $x_{t}$ in Equation 4</li>
</ol>
<h5 id="predict-x_0-with-x_t">Predict $x_{0}$ with $x_{t}$</h5>
<p>Applying forward process $p(x_{t}|x_{t-1})$ for $t$ times, we can rewrite $x_{t}$ as:</p>
<p>$$
x_{t} = \bar \alpha_{t} x_{0} + \bar{\beta_{t}} ^ {2}\epsilon_{t}, \bar \alpha_{t} = \prod \alpha_{i}, \bar \beta_{t} = \sqrt{1-\bar \alpha_{t}^{2}}
$$
And the probability version:
$$
p(x_t|x_{0}) = \mathcal{N}(x_{t}; \bar \alpha_{t} x_{0},  \bar \beta_{t} ^ {2}I)
$$</p>
<p>Now that we have $x_t$ from $x_0$, update Eq 4 (since $\mathcal{N}$ can be represented as probabilities, the result is conformed to $\mathcal{N}$ as well):</p>

$$
p(x_{t-1}| x_{t},x_{0}) = \mathcal{N}\left(x_{t-1}; \underbrace{\frac{\alpha_{t}\bar \beta_{t-1}^{2}}{\bar \beta_{t}^{2}}x_{t} + \frac{\bar \alpha_{t-1}\beta_{t}^{2}}{\bar \beta_{t}^{2}}x_{0}}_{\text{$\tilde \mu_t(x_{t}, x_{0})$}},\frac{\bar \beta_{t-1}^{2}\beta_{t}^{2}}{\bar \beta_{t}^{2}}I\right)
$$

<p>Let&rsquo;s define the predicted mean value of $x_{t-1}$ as $\tilde \mu_t(x_{t}, x_{0}) = \frac{\alpha_{t}\bar \beta_{t-1}^{2}}{\bar \beta_{t}^{2}}x_{t} + \frac{\bar \alpha_{t-1}\beta_{t}^{2}}{\bar \beta_{t}^{2}}x_{0}$.</p>
<p>Notice the meaning of it: <strong>With Bayes&rsquo; Theorem, using $x_{t}$ and $x_{0}$, we can derive the mean value of $x_{t-1}$.</strong></p>
<p>So naturally, we can make our $\mu_{\theta}(x_{t}, t)$, who have the same estimated output as  $\tilde \mu_t(x_{t}, x_{0})$, learn the distribution of it:
$$
\mu_{\theta}(x_{t}, t) = \tilde \mu_t(x_{t}, x_{0})
$$</p>
<blockquote>
<p>[!NOTE] Different ways of modeling $\mu_\theta$ is also acceptable, it&rsquo;s just that this is a better way (or not)</p>
</blockquote>
<p>However, we don&rsquo;t have $x_0$ to pass to $\tilde \mu_t(x_{t}, x_{0})$.  Luckily, we can <strong>predict</strong> $x_0$ from rewriting Equation 7:
$$
x_{0}= \frac{x_{t} - \sqrt{1- \bar \alpha_{t}}}{\sqrt{\bar \alpha_{t}}}\epsilon_{t}
$$</p>
<blockquote>
<p>[!TIP]
This is actually an embodiment of the <a href="https://en.wikipedia.org/wiki/Predictor%E2%80%93corrector_method"><strong>predictor–corrector</strong></a> method</p>
</blockquote>
<p>Since we don&rsquo;t have $\epsilon$ in the reverse process, we can make a neural work to learn it: $\epsilon_\theta(x_t,t) \to \epsilon_t$ :
$$
x_{0}= \frac{x_{t} - \sqrt{1- \bar \alpha_{t}}}{\sqrt{\bar \alpha_{t}}}\epsilon_{\theta}(x_{t}, t)
$$</p>
<p>Update the Eq 10:
$$
\mu_{\theta}(x_{t}, t) = \tilde \mu_t(x_{t}, x_{0}) = \tilde \mu_t\left(x_{t}, \frac{x_{t} - \sqrt{1- \bar \alpha_{t}}}{\sqrt{\bar \alpha_{t}}}\epsilon_{\theta}(x_{t}, t)\right)= \frac{1}{\alpha_{t}}\left(x_{t} - \frac{\beta_{t}^2}{\bar\beta_{t}}\epsilon_{\theta}(x_{t},y, t)\right)
$$</p>
<h4 id="reverse-noise-predictor-sigma_thetax_tt">Reverse Noise Predictor: $\Sigma_{\theta}(x_{t},t)$</h4>
<p>It still remains to design $\Sigma_{\theta}(x_{t},t)$, since it encourages diversity.</p>
<p>The DDPM paper suggested not learning it, since it:</p>
<blockquote>
<p>resulted in unstable training and poorer sample quality</p>
</blockquote>
<p>By fixing it at some value $\Sigma_{\theta}(x_{t},t) = \sigma_{t}^{2}I$ , where either $\sigma_{t}^{2} = \beta_{t}$ or $\tilde{\beta_t}$ yielded similar performance.</p>
<h3 id="training--defining-loss">Training &amp; Defining Loss</h3>
<p>Conclusively, we have only defined one trainable model: $\epsilon_{\theta}(x_t, t)$</p>
<h4 id="to-reconstruct-x_0">To reconstruct $x_{0}$</h4>
<p>The training target can be <strong>MLE</strong>, the objective function being log-likelihood of reconstructing $x_{0}$:</p>

$$
\begin{align*}
\ln p(x_{0}) &= \int{\ln p(x_{0:T})dx_{1:T}} & \text{marginalization of marginal distribution}\\\\
&=  \ln \int{p(x_{0:T})dx_{1:T}}\\\\\\
&=  \ln \mathbb{E}_{q(x_{1:T}|x_{0})}[\int{\frac{p(x_{0:T})}{q(x_{1:T}|x_{0})}}]\\\\
&\ge \mathbb{E}_{q(x_{1:T}|x_{0})}\left[\ln \frac{p(x_{0:T})}{q(x_{1:T}|x_{0})}\right] & \text{Jensen Inequality of $\log$}\\\\
&= \underbrace{\mathbb{E}_{q(x_{1}|x_{0})}\left[\ln p_{\theta}({x_{0}|x_{1}})\right]}_{\text{reconstruction term}} - \sum_{t = 2}^{T}\mathbb{E}_{q(x_{t}|x_{0})}\left[D_{KL}(q(x_{t-1}|x_{t},x_{0})||p_{\theta}(x_{t-1}|x_{t}))\right]\\\\
&= \sum\limits_{t=1}^{T}  \gamma \mathbb{E}_{q(x_{t}|x_{0})}[||\epsilon_{t} - \epsilon_{\theta}(x_{t}, t) || ^{2}] & \text{$\gamma$ being some constants}
\end{align*}
$$

<h4 id="to-optimize-the-pixels">To optimize the pixels</h4>
<p>We design the loss function of $\theta$ as the <strong>Euclidean distance</strong> of the true and predicted mean of  $x_{t-1}$:
$$
\begin{align*}
\ell  &amp;= ||x_{t-1} - \hat x_{t-1}|| ^ 2 \newline
&amp;= ||x_{t-1} - \mu_\theta(x_{t},t)|| ^ 2 \newline
&amp;=|| (\frac{1}{\alpha_{t}}(x_{t} - \beta_{t}\epsilon_{t}) ) ^ {2} - \frac{1}{\alpha_{t}}(x_{t} - \beta_{t}\epsilon_{\theta}(x_{t}, t)) ^ {2}||\newline
&amp;= \frac{\beta_{t}^{2}}{\alpha_{t}^{2}} ||\epsilon_{t} - \epsilon_{\theta}(x_{t}, t) || ^2
\end{align*}
$$</p>
<h2 id="ddim">DDIM</h2>
<p>While the original <strong>DDPM</strong> is capable to generate satisfying images, it is known for poor performance: since the denoising(reverse) diffusion process usually take $T \sim 1000$ times of noise-prediction.</p>
<p><strong>DDIM</strong> is proposed to boost the reverse process as <strong>Non-Markovian Process</strong>, by directly taking any model trained on <strong>DDPM</strong> and sampling only  $T_{ddim}, T_{ddim} \le T$ timestamps, with some timestamps skipped. As a side-effect, the quality is compromised a little.</p>
<h3 id="reverse-process-1">Reverse Process</h3>
<p>It still takes the same approach as DDPM: predict $x_0$ from $x_t$ first.</p>
<p>From Eq 4, we can see that the sampling/training do involves $x_t$, but doesn&rsquo;t actually involves $p(x_{t-1}|x_{t})$ (which is defined in our reverse model). Instead, it defines:</p>
<p>$$
q_\sigma(x_{t-1}|x_{t}, x_0) = \mathcal{N}(x_{t-1};\sqrt{\bar \alpha_{t-1}}x_0 + \sqrt{1 - \bar \alpha_{t-1} - \sigma_t^2}\frac{x_t - \sqrt{\bar \alpha_t}x_0}{\sqrt{1 - \bar \alpha_t}}, \sigma_t^2I)
$$</p>
<p>Hence, the relation between $x_{t-1}$ and $x_t$ is:</p>
<p>
$$
x_{t-1} = \sqrt{\alpha_{t-1}} \underbrace{\left( \frac{x_t - \sqrt{1 - \alpha_{t}}\epsilon_{\theta}(x_{t},t)} {\sqrt{\bar \alpha_{t}}} \right)}_{\text{predicted $x_0$}} + 
\underbrace{\sqrt{1 - \bar \alpha_{t-1} - \sigma_{t}^{2}\epsilon_{t}(x_{t},t)}}_{\text{predicted  noise}} + \underbrace{\sigma_{t} \epsilon_{t}}_{\text{random noise}}
$$


where:
$$
\sigma_t = \eta \sqrt{(\frac{1 - \bar \alpha_t}{1 - \bar \alpha_{t-1}}) \left(1 - \frac{\bar \alpha_t}{\bar \alpha_{t-1}}\right)}
$$
where $\eta \in (0,1)$ is a constant, indicating the level of random noise:</p>
<ul>
<li>$\eta = 1$: The random noise is maximized, which is <strong>DDPM</strong>.</li>
<li>$\eta = 0$: The random noise is totally removed, making it a deterministic process/Implicit model, which is <strong>DDIM</strong>. It relies entirely on the predicted noise, while sacrificing some diversity with lowering random noise level.</li>
</ul>
<p>As for the timestamps chosen, they are determined empirically.</p>
<style type="text/css">.notice{--root-color:#444;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#c33;--warning-content:#fee;--info-title:#fb7;--info-content:#fec;--note-title:#6be;--note-content:#e7f2fa;--tip-title:#5a5;--tip-content:#efe}@media (prefers-color-scheme:dark){.notice{--root-color:#ddd;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#800;--warning-content:#400;--info-title:#a50;--info-content:#420;--note-title:#069;--note-content:#023;--tip-title:#363;--tip-content:#121}}body.dark .notice{--root-color:#ddd;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#800;--warning-content:#400;--info-title:#a50;--info-content:#420;--note-title:#069;--note-content:#023;--tip-title:#363;--tip-content:#121}.notice{padding:18px;line-height:24px;margin-bottom:24px;border-radius:4px;color:var(--root-color);background:var(--root-background)}.notice p:last-child{margin-bottom:0}.notice-title{margin:-18px -18px 12px;padding:4px 18px;border-radius:4px 4px 0 0;font-weight:700;color:var(--title-color);background:var(--title-background)}.notice.warning .notice-title{background:var(--warning-title)}.notice.warning{background:var(--warning-content)}.notice.info .notice-title{background:var(--info-title)}.notice.info{background:var(--info-content)}.notice.note .notice-title{background:var(--note-title)}.notice.note{background:var(--note-content)}.notice.tip .notice-title{background:var(--tip-title)}.notice.tip{background:var(--tip-content)}.icon-notice{display:inline-flex;align-self:center;margin-right:8px}.icon-notice img,.icon-notice svg{height:1em;width:1em;fill:currentColor}.icon-notice img,.icon-notice.baseline svg{top:.125em;position:relative}</style>
<div><svg width="0" height="0" display="none" xmlns="http://www.w3.org/2000/svg"><symbol id="tip-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet"><path d="M504 256c0 136.967-111.033 248-248 248S8 392.967 8 256 119.033 8 256 8s248 111.033 248 248zM227.314 387.314l184-184c6.248-6.248 6.248-16.379 0-22.627l-22.627-22.627c-6.248-6.249-16.379-6.249-22.628 0L216 308.118l-70.059-70.059c-6.248-6.248-16.379-6.248-22.628 0l-22.627 22.627c-6.248 6.248-6.248 16.379 0 22.627l104 104c6.249 6.249 16.379 6.249 22.628.001z"/></symbol><symbol id="note-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet"><path d="M504 256c0 136.997-111.043 248-248 248S8 392.997 8 256C8 119.083 119.043 8 256 8s248 111.083 248 248zm-248 50c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"/></symbol><symbol id="warning-notice" viewBox="0 0 576 512" preserveAspectRatio="xMidYMid meet"><path d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"/></symbol><symbol id="info-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet"><path d="M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196 0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627 0 12 5.373 12 12v100h12c6.627 0 12 5.373 12 12v24z"/></symbol></svg></div><div class="notice tip" >
<p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#tip-notice"></use></svg></span>Tip</p><p>In fact, $\eta$ represents the degree of moving some of the noise from predicted noise $\epsilon_t$ to sampled noise $\epsilon$: the bigger the $\eta$, the less deterministic, the larger random noise will be introduced to the reverse process.</p></div>
<h2 id="short-summary">Short Summary</h2>
<p>Conclusively, both models apply the same forward process, and have the same target: $x_{t} \to x_{0}$, though they have differences in the reverse process:</p>
<ul>
<li><strong>DDPM</strong> maximize the random noise, and in order to mitigate the negative effects it has,  takes more timestamps in the reverse process</li>
<li><strong>DDIM</strong> boost the performance by only selecting some of the timestamps, and reduce the random noise level</li>
</ul>
<h2 id="conditioned-generation">Conditioned Generation</h2>
<p>While being able to generate high quality images with reasonable speed with the models mentioned above, it is a common feature to generated <strong>conditioned output</strong>.</p>
<p>Given condition $y$, our goal is to derive $p(x_{t-1}|x_{t},y)$</p>
<h3 id="classifier-guided-diffusion">Classifier Guided Diffusion</h3>
<p>Using bayes&rsquo; rule, we have:
$$
p(x_{t-1}|x_{t},y) = \frac{p(x_{t-1}|x_{t})p(y|x_{t-1},x_{t})}{p(y|x_{t})}
$$</p>
<p>Using the notations in <a href="/posts/ai/models/denoising-diffusion-models/#reverse-process">Reverse Process</a>:
$$
p(x_{t-1}|x_{t},y) \propto \exp(-||x_{t-1}-\mu(x_{t})-\Sigma_{t}^{2}\underbrace{\nabla_{x_{t}}\log p(y|x_{t})}<em>{\text{classifier}}||^{2}/2\Sigma</em>{t}^{2})
$$</p>
<p>So $\mu_{\theta}(x_{t}, t,y) = \mu(x_{t})+\Sigma_{t}^{2}\nabla_{x_{t}}\log p(y|x_{t})), \Sigma_{t} = \sigma_{t}$</p>
<blockquote>
<p>[!NOTE]</p>
<ul>
<li>The gradient of the prob is easy to get with <em>autograd</em>, if the classifier can output the prob</li>
<li>The classifier guides the model only when inferencing</li>
</ul>
</blockquote>
<h3 id="classifier-free-diffusion">Classifier-Free Diffusion</h3>
<p>To infer without a classifier, we need to blend the condition $y$ into training process.</p>
<p>By directly modeling the conditioned reverse process as  $p(x_{t-1}|x_{t},y) = \mathcal{N}(x_{t-1};\mu(x_{t},y), \sigma_{t}^{2}I)$, following the modeling of Eq. 11, we have:
$$
\mu(x_{t}, y) = \frac{1}{\alpha_{t}}\left(x_{t} - \frac{\beta_{t}^2}{\bar\beta_{t}}\epsilon_{\theta}(x_{t},y, t)\right)
$$</p>
<p>The $\epsilon_{\theta}(x_{t},y, t)$ can be trained to predict the noise under condition.</p>
<blockquote>
<p>[!WARNING]
The conditioned noise predictor depends on $y$, so retraining is required if the $y$ is changed</p>
</blockquote>
<h2 id="score-based-generative-models">Score-based generative models</h2>
<table>
<thead>
<tr>
<th style="text-align:left">Notation</th>
<th style="text-align:left">Meaning</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Ancestral Sampling</td>
<td style="text-align:left">A sample method, auto-regressive</td>
<td></td>
</tr>
<tr>
<td style="text-align:left">Score Distillation Sampling</td>
<td style="text-align:left">A sampling method(sampler) to generate samples from a diffusion model by <strong>optimizing a loss function</strong>. Basically, it utilizes(distills) the score function of a teacher diffusion model, to train a larger model, with the final result as a sample (as $t \to 0$).</td>
<td></td>
</tr>
<tr>
<td style="text-align:left"><a href="https://en.wikipedia.org/wiki/Informant_(statistics)">score function</a></td>
<td style="text-align:left">The gradient of the <strong>log</strong>-likelihood function</td>
<td></td>
</tr>
<tr>
<td style="text-align:left">$U(x) = -\log q(x)$</td>
<td style="text-align:left">An energy function. The lower the likelihood, the higher the energy</td>
<td></td>
</tr>
<tr>
<td style="text-align:left">$\mu_{\theta}(x_t, t)$</td>
<td style="text-align:left">parameterized model to predict $x_{t-1}$ at time $t$</td>
<td></td>
</tr>
<tr>
<td style="text-align:left">$p(x_{0:T})$</td>
<td style="text-align:left">the joint distribution of $x_{0}, x_{1} &hellip; x_{T}$</td>
<td></td>
</tr>
<tr>
<td style="text-align:left"><a href="https://en.wikipedia.org/wiki/Metropolis-adjusted_Langevin_algorithm">Langevin dynamic</a></td>
<td style="text-align:left">A Markov chain Monte Carlo(MCMC) method for obtraining random samples</td>
<td></td>
</tr>
<tr>
<td style="text-align:left"><a href="/">Fisher Divergence</a></td>
<td style="text-align:left"></td>
<td></td>
</tr>
</tbody>
</table>
<p>Most often, we don&rsquo;t care about the probability $q(x)$ of a certain input $x$, but how it changes through time: therefore, we can utilize score function(gradient, changes) $s(x):=\nabla_{x}\log q(x)$</p>
<blockquote>
<p>[!TIP]
It is also an advantage of modeling the <strong>score</strong>: don&rsquo;t have to make sure <strong>probability</strong> sum up to 1</p>
</blockquote>
<p>With $s(x)$ allowing us to sample from $q(x)$ using thermodynamics, our goals changes to: <strong>model $q(x)$</strong></p>
<p>$$
dx_{t} = \nabla \log q(x) d_{t} + d{W_{t}}
$$</p>
<h3 id="loss">Loss</h3>
<p>We learn a model $s_{\theta}$ to <strong>match</strong>(approximate) the <strong>score</strong> $\nabla \log q$:</p>
<p>$$
s_{\theta} \approx \nabla \log q(x)
$$
&ndash; This is score matching.</p>
<p>Typically, score matching is formalized as minimizing <strong>Fisher divergence</strong> function . By expanding the integral, and performing an integration by parts, we have our loss function:
$$
\mathcal{L} = \mathbb{E}<em>{q}[||s</em>{\theta}(x) - \nabla \log q(x)||^{2}]
$$
However, it&rsquo;s infeasible since it requires access to unknown score  $\nabla \log q(x)$.</p>
<p>Fortunately, we have <strong>score matching</strong> techniques(e.g. <a href="https://en.wikipedia.org/wiki/Scoring_rule#Hyv%C3%A4rinen_scoring_rule" title="Scoring rule">Hyvärinen scoring rule</a>) which minimize the Fisher divergence without knowledge of the gorund-truth score:
$$
\mathcal{L} = \mathbb{E}<em>{q}\left[\nabla</em>{x} s_{\theta}(x)+ \frac{1}{2}||s_{\theta}(x)||_{2}^{2}\right]
$$</p>
<p>Since $s_{\theta}$ is modeled by ourself, its output and gradients can be easily calculated. We use Monte-Carlo methods with gradient descent to optimize it.</p>
<h3 id="sample--inference">Sample / Inference</h3>
<p>But how do we generate a sample.</p>
<p>Once we have trained a score-based model $s_{\theta}(x)$, we can use an iterative procedure called <strong><a href="https://en.wikipedia.org/wiki/Metropolis-adjusted_Langevin_algorithm">Langevin dynamics</a></strong> to draw samples from it:
$$
x_{i + 1} \leftarrow x_{i} + \epsilon \nabla_{x} \log p(x) + \sqrt{2\epsilon} z_{i}, z_{i} \sim \mathcal{N}(0, I)
$$
Notice some white noise is injected, to avoid all samples collapse into some limited local optimas.</p>
<p>This seems decent, but in fact: in low-density regions, the estimated scores are inaccurate.</p>
<p>It&rsquo;s natural to augment the low-density regions by perturbing our datapoint: injecting $\mathcal{N}$. It can solve the problem in low-density, however since the training data is perturbed, the generated samples are too.</p>
<p>Multiple (decreasing) noise levels $\sigma$ are applied as an input to score funcion $s$, with the output of previous model $i$ as the input of the next model $i+1$. The whole process resembles an <strong>Anneald Langevn Dynamics</strong></p>
<h2 id="sde">SDE</h2>
<table>
<thead>
<tr>
<th style="text-align:left">Notation</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><a href="https://en.wikipedia.org/wiki/Stochastic_differential_equation">SDE</a></td>
<td style="text-align:left">A DE in which one or more of the terms is a <strong>stohastic</strong> process</td>
</tr>
<tr>
<td style="text-align:left">$\mathcal{U}(T_{a},T_{b})$</td>
<td style="text-align:left">Uniform distribution over the time interval $[T_{a}, T_{b}]$</td>
</tr>
<tr>
<td style="text-align:left">In <a href="/posts/ai/models/denoising-diffusion-models/#ddpm">DDPM</a>, we define $t$ as <em>discrete</em> timestamps, however it&rsquo;s more natural to model it as <em>continuous</em> time.</td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="forward-process-1">Forward Process</h3>
<p>With this premise, we model the forward process with <strong>Stochastic</strong> DE(Differential equation), but not funtion on timestamps:
$$
dx = f_{t}(x) + g_{t}dw
$$</p>
<h3 id="reverse-process-2">Reverse Process</h3>
<p>Similarly, we want to model $p(x_{t}|x_{t + \Delta{t}})$:
$$
\begin{align}
p(x_{t}|x_{t + \Delta{t}}) &amp;= \frac{p(x_{t + \Delta_{t}} | x_{t})p(x_{t})}{p(x_{t+ \Delta{t}})} \\
&amp;= p(x_{t + \Delta{t}} | x_{t})\exp(\log p(x_{t}) - \log p(x_{t+\Delta{t}})) \\
&amp;\propto \left(-\frac{||x_{t + \Delta_{t}} - x_{t} - f_{t}(x_{t})\Delta{t}||^{2}}{2g_{t}^{2}\Delta t}+  \log p(x_{t}) - \log p(x_{t+\Delta{t}})\right)
\end{align}
$$</p>
<p>In order to calculate the unknown diff, we apply <strong>Taylor expansion</strong>:
$$
\log p(x_{t+\Delta{t}}) \approx \log p(x_{t}) + (x_{t+\Delta t} - x_{t}) \cdot \nabla_{x_{t}}\log p(x_{t}) + \underbrace{\Delta t \frac{\partial \log p(x_{t})}{\partial t}}<em>{\text{$x</em>{t}$&rsquo;s deritive of $t$}}
$$</p>
<p>Update Equation 26-3 with it, we have:
$$
p(x_{t}|x_{t + \Delta{t}})  \sim \mathcal{N}(f_{t+\Delta t}(x_{t + \Delta t}) - g_{t + \Delta t}^{2}\nabla_{x_{t + \Delta t}} \log p(x_{t + \Delta t})\Delta t; g_{t + \Delta t}^{2}\Delta t I)
$$</p>
<p>and the SDE of <strong>reverse process</strong>:
$$
dx = [f_{t}(x) - g_{t}^{2}\nabla_{x}\log p_{t}(x)]dt + g_{t}dw
$$</p>
<h3 id="training">Training</h3>
<p>$$
\mathcal{L} = \mathbb{E}<em>{t \in \mathcal{U}(0, T)}\mathbb{E}</em>{p_{t}(x)}[\lambda(t)||\nabla_{x}\log p_{t}(x) - s_{\theta}(x,t)||^{2}_{2}]
$$
where:</p>
<ul>
<li>$\lambda : \mathbb{R} \to \mathbb{R}_{&gt;0}$  is a positive weighting function</li>
</ul>
<h2 id="probability-flow-ode">Probability flow ODE</h2>
<table>
<thead>
<tr>
<th style="text-align:left">Notation</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">PF(Probability flow) ODE</td>
<td style="text-align:left">The ODE of an SDE</td>
</tr>
</tbody>
</table>
<p>Despite capable of generating high-quality samples, <em>samplers</em> based on Langevin MCMC and SDE solvers do not provide a way to compute the exact log-likelihood of score-based generative models.</p>
<p>It has been proved that, it is possible to convert any SDE into an ODE(ordinary differential equation) without changing its marginal distributions $p_{t}(x)$</p>
<h3 id="forward-process-2">Forward Process</h3>
<p>With a sequence of complex calculations(including F-P function &amp; Dirac function), we have:</p>
<p>$$
dx = [f(x,t) - \frac{1}{2}(g^{2}(t)-\sigma_{t}^{2})\nabla_{x}\log p_{t}(x)]dt
$$</p>
<h3 id="reverse-process-3">Reverse Process</h3>
<p>The reverse process of PF-ODE is given by:</p>
<p>$$
dx = [f(x,t) - \frac{1}{2}g^{2}(t)\nabla_{x}\log p_{t}(x)]dt
$$</p>
<blockquote>
<p>[!TIP]
When $\nabla_{x}\log p_{t}(x)$ replaces $s_{\theta}(x,t)$, PF ODE becomes a special case of a neural ODE</p>
</blockquote>
<h2 id="samplers">Samplers</h2>
<h3 id="euclidean">Euclidean</h3>
<p>$$
\begin{equation}\left.\frac{d\boldsymbol{x}<em>t}{dt}\right|</em>{t=t_{n+1}}\approx \frac{\boldsymbol{x}<em>{t</em>{n+1}} - \boldsymbol{x}<em>{t_n}}{t</em>{n+1} - t_n}\end{equation}
$$
一阶近似</p>
<h3 id="heun-solver">Heun solver</h3>
<h3 id="dpm-solver">DPM solver</h3>
<h3 id="amed-solver">AMED solver</h3>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
