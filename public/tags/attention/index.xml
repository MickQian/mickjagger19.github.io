<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Attention on Mick&#39; Blog</title>
    <link>https://mickjagger19.github.io/tags/attention/</link>
    <description>Recent content in Attention on Mick&#39; Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 19 Feb 2024 16:23:58 +0800</lastBuildDate><atom:link href="https://mickjagger19.github.io/tags/attention/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>DiT, Sora, and more</title>
      <link>https://mickjagger19.github.io/posts/ai/models/dit-sora-and-more/</link>
      <pubDate>Mon, 19 Feb 2024 16:23:58 +0800</pubDate>
      
      <guid>https://mickjagger19.github.io/posts/ai/models/dit-sora-and-more/</guid>
      <description>Review on DiT and sora</description>
      <content:encoded><![CDATA[<blockquote>
<p>[! WARNING]
This work is in progress</p>
</blockquote>
<table>
<thead>
<tr>
<th style="text-align:left">Notations</th>
<th style="text-align:left">Meaning</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Classifier-guided Diffusion</td>
<td style="text-align:left">An implementation of Conditional Diffusion Models, which requires a classifier $\nabla_{x}p(x|y)$ to guide its reverse process</td>
<td></td>
</tr>
<tr>
<td style="text-align:left">Classifier-free Diffusion</td>
<td style="text-align:left">An implementation of Conditional Diffusion Models, which doesn&rsquo;t require a <strong>classfier</strong>, the condition serves as an input to its <strong>noise predictor</strong></td>
<td></td>
</tr>
<tr>
<td style="text-align:left">Latent Diffusion Models(LDM)</td>
<td style="text-align:left">A type of Diffusion Models where diffusion processes are done in <strong>latent space</strong></td>
<td></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
<td></td>
</tr>
</tbody>
</table>
<h1 id="introduction">Introduction</h1>
<p>Diffusion models usually choose a <strong>UNet</strong> as its backbone for Noise Predictor, first adopted by Ho et al [^1] , which was inherited from <strong>Pixel-CNN++</strong>(widely used as the generator in VAE) with a few changes. Although some works have introduced attention blocks into low-level design, its high-level remains intact.</p>
<p><strong>DiT</strong> is proposed to apply Transformer into Diffusion Models, adhering to the best practices of <strong>Vision Transformers(ViTs)</strong></p>
<p>Also, the scaling behavios of transformers is also explored in DiT</p>
<h1 id="dit-design-space">DiT Design Space</h1>
<table>
<thead>
<tr>
<th style="text-align:left">Notations</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">$C$</td>
<td style="text-align:left">the channels of input image</td>
</tr>
<tr>
<td style="text-align:left">$I$</td>
<td style="text-align:left">the dimension of $z$</td>
</tr>
<tr>
<td style="text-align:left">$z \in R^{I \times I \times C}$</td>
<td style="text-align:left">The latents, also the input of $DiT$</td>
</tr>
<tr>
<td style="text-align:left">Patch</td>
<td style="text-align:left">The unit of input.</td>
</tr>
<tr>
<td style="text-align:left">$p$</td>
<td style="text-align:left">the dimension of a single patch</td>
</tr>
<tr>
<td style="text-align:left">Classifier-free Diffusion</td>
<td style="text-align:left">An implementation of Conditional Diffusion Models, which doesn&rsquo;t require a <strong>classfier</strong>, the condition serves as an input to its <strong>noise predictor</strong></td>
</tr>
<tr>
<td style="text-align:left">Latent Diffusion Models(LDM)</td>
<td style="text-align:left">A type of Diffusion Models where diffusion processes are done in <strong>latent space</strong></td>
</tr>
</tbody>
</table>
<h2 id="1-patchify">1. Patchify</h2>
<p><strong>Patchify</strong> is the first layer of DiT, which converts the spatial input $z$ into a sequence to $T$ tokens, each of dimension $d$:
$$
z \in R^{I \times I \times C} \to T \cdot Token
$$
where:</p>
<ul>
<li>$Token \in R^{p \times p \times C}$</li>
<li>$T = (\frac{I}{p})^2$</li>
</ul>
<h2 id="2-positional-embeddings">2. Positional Embeddings</h2>
<p>Following patchify, we apply standard ViT <strong>frequency-based positional embeddings</strong> (the sine-cosine version) to all input tokens: $Token \to Patch$</p>
<h2 id="3-transformer-blocks">3. Transformer Blocks</h2>
<p>Following patchify, the input tokens are processed by a sequence of transformer blocks.</p>
<h3 id="in-context-conditioning">In-context conditioning</h3>
<p>In addition to noised image inputs, diffusion models sometimes process additional conditional information:</p>
<ul>
<li>$t$: noise timestamps (of DDPM)</li>
<li>$c$: class labels c(of input images)</li>
<li>natural language description(or caption)</li>
</ul>
<p>The conditions are represented as additional $Token$s, <strong>appened</strong> to the [input sequence](## 2. Positional Embeddings)</p>
<h3 id="cross-attention-block">Cross-attention block</h3>
<p>The conditional tokens are send to the cross-attention block of transformer block</p>
<h3 id="adaptie-layer-normadaln-block">Adaptie Layer Norm(adaLN) Block</h3>
<p>The <strong>Adaptive Layer Norm</strong> replaces the standard layer norm</p>
<blockquote>
<p>[! NOTE]
To be completed</p>
</blockquote>
<h2 id="4-transformer-decoder">4. Transformer Decoder</h2>
<p>After the transformer blocks, a <strong>transformer decoder</strong> is reponsible for decoding the each latent token back to tensor of size $p \times p \times C$.</p>
<p>The decoder is simply a <strong>standrad linear layer</strong></p>
<h1 id="sora">Sora</h1>
<p>The following is some major takeaways of <a href="https://openai.com/research/video-generation-models-as-world-simulators">the tenichal report of Sora</a></p>
<p>Sora is a diffusion model/diffusion transformer based on <strong>DiT</strong></p>
<h2 id="unified-representation-of-visual-data">Unified Representation of Visual Data</h2>
<p>The major part of the technial report is about the <strong>Unified Representation</strong>, which is the training data of Sora</p>
<h3 id="sources">Sources</h3>
<blockquote>
<p>We take inspiration from large language models which acquire generalist capabilities by training on internet-scale data</p>
</blockquote>
<p>Based on the description, Sora might crawl a huge amount of data from internet</p>
<h3 id="patch">Patch</h3>
<p>Following the text token concept of LLM, and patch concept from [DiT](##1. Patchify), Sora has visual <em>patches</em>.</p>
<p>The transformation of videos into patches went through 2 steps:</p>
<ol>
<li>Compress videos into lower-dimensional latents</li>
<li>Decompose the latents into <strong>spacetime</strong> patches</li>
</ol>
<blockquote>
<p>[! TIP]
<strong>spacetime</strong> implies that the tokens are embedded with timestep information</p>
</blockquote>
<p>While the original DiT tokens are fixed-sized based on the size of the latents, the <em>spacetime</em> patches used by Sora is derived from videos/images of <strong>variable resolutions, durations and aspect ratios</strong>. This gives huge flexibility in inference time, since the output is formed with any patches you like.</p>
<h2 id="scaling-transformers">Scaling Transformers</h2>
<p>They find transformers scaled effectively as video models, same as in other domains, including language modeling,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-13">13</a>,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-14">14</a> computer vision,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-15">15</a>,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-16">16</a>,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-17">17</a>,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-18">18</a> and image generation.<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-27">27</a>,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-28">28</a>,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-29">29</a></p>
<h2 id="data-preprocessing">Data preprocessing</h2>
<h3 id="native-size">Native size</h3>
<p>Different from prior approaches, which crops videos/images to standard size(e.g., 4 seconds videos at 256 * 256 resolution), they find that training on data at its <strong>native size</strong> benefits.</p>
<h3 id="native-aspect-ratios">Native aspect ratios</h3>
<p>They empirically find that training on videos at their native aspect ratios improves composition and framing. The model trained on square crops sometimes generates videos where the subject is only <strong>partially</strong> in view.</p>
<h2 id="language-understanding">Language understanding</h2>
<p>The language-understanding-ability is a crucial part of text-to-video models, as text is the major input</p>
<h3 id="re-captioning">Re-captioning</h3>
<p><strong>Re-captioning</strong> is a technique to generate descriptive captions for images/videos with the help of a <strong>highly descriptive captioner model</strong></p>
<p>First introduced in the training of DALL$\cdot$E, it is used in Sora, where GPT serves the role of <strong>captioner</strong>, who <em>turn short user prompts into longer detailed captions</em></p>
<h2 id="prompting-with-images-and-videos">Prompting with images and videos</h2>
<p>Being abled to be prompted with inputs other than text, including images and videos, Sora can perform a wide range of image and video editing tasks.</p>
<h3 id="some-applications-not-mentioned-in-the-report">Some applications not mentioned in the report</h3>
<p>Based on the presented applications, Sora might be able to do the tasks of:</p>
<ul>
<li>video generation conditioned on text/image: generate videos based on the given future/previous text and image</li>
<li></li>
</ul>
<blockquote>
<p> Model and implementation details are not included in this report.
 &ndash; video-generation-models-as-world-simulators</p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>The Attention Family</title>
      <link>https://mickjagger19.github.io/posts/ai/models/the-attention-mechanism/</link>
      <pubDate>Sun, 21 Jan 2024 12:47:01 +0800</pubDate>
      
      <guid>https://mickjagger19.github.io/posts/ai/models/the-attention-mechanism/</guid>
      <description>Attention</description>
      <content:encoded><![CDATA[<h2 id="terminologies">Terminologies</h2>
<table>
<thead>
<tr>
<th style="text-align:left">Term</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><a href="https://en.wikipedia.org/wiki/Receptive_field">Receptive field</a>(a.k.a. sensory space)</td>
<td style="text-align:left">A concept originally from biology, adopted in modern artificial deep neural networks (especially <strong>CNN</strong>), describing the size of input image which can affects the output of <strong>neurons</strong></td>
</tr>
<tr>
<td style="text-align:left">$d_{model}$</td>
<td style="text-align:left">dimension of the word embedding(usually 512 = 64 * 8)</td>
</tr>
<tr>
<td style="text-align:left">$d_{k}$</td>
<td style="text-align:left">dimension of $w_q, w_k$</td>
</tr>
<tr>
<td style="text-align:left">$d_{v}$</td>
<td style="text-align:left">dimension of $w_v$</td>
</tr>
<tr>
<td style="text-align:left">$w_{q} \in \mathbb{R}^{d_{model} \times d_{k}}$</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">$w_k \in \mathbb{R}^{d_{model} \times d_{k}}$</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">$w_{v} \in \mathbb{R}^{d_{model} \times d_{v}}$</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">$B$</td>
<td style="text-align:left">Batch size</td>
</tr>
<tr>
<td style="text-align:left">$S$</td>
<td style="text-align:left">Sequence Length</td>
</tr>
<tr>
<td style="text-align:left">$X$</td>
<td style="text-align:left">input</td>
</tr>
<tr>
<td style="text-align:left">Structural Prior</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">translation equivariance</td>
<td style="text-align:left">An attribute of model, the ability of model to recognize objects does not varies with the <strong>geometric</strong> transformations of the input(shift, rotate, projection, etc)</td>
</tr>
</tbody>
</table>
<h2 id="the-attention-of-human">The Attention of human</h2>
<p>Selective attention is a mechanism unique to human vision. By swiftly scanning the image, human acquires important areas(a.k.a. focus). After this, human pays more attention to these areas, as there are more valuable information.</p>
<h2 id="self-attention">Self-Attention</h2>
<p>Self <em>Scaled-Dot</em> Attention:</p>
<p>$$
\begin{align}
X &amp;= X_{text_encoding}+ X_{positional_embedding}\
Q &amp;= X \cdot w_{q} \in \mathbb{R}^{S \times d_{k}}\
K &amp;= X \cdot w_{k} \in \mathbb{R}^{S \times d_{k}}\
V &amp;= X \cdot w_{v} \in \mathbb{R}^{S \times d_{v}}\
a &amp;= softmax\left(\frac{Q \cdot K^{\top}}{d_{k}}\right)\in \mathbb{R}^{S \times S}\
Attention &amp;= a V\in \mathbb{R}^{S \times d_{k}}\
\end{align}
$$</p>
<blockquote>
<p>[!TIP]</p>
<ul>
<li>Attention is the weighted (attention score) v of each token over other tokens</li>
</ul>
</blockquote>
<p>The major different between attention and typical <strong>RNN</strong> is: The generation of next token doesn&rsquo;t rely on hidden state from previous timestamp, but instead alter the embedding of the token directly with positional embedding.</p>
<blockquote>
<p>[!TIP]
Some researchers seems Attention as a kind of soft addressing</p>
</blockquote>
<h2 id="cross-attention">Cross-Attention</h2>
<p>Different from <a href="/posts/ai/models/the-attention-mechanism/#self-attention">Self-Attention</a>, the $Q, K$ of cross-attention comes from another sequence($X_{2}$):
$$
\begin{align}
Q &amp;= X_{1} \cdot w_{q} \in \mathbb{R}^{n \times d_{k}}\
K &amp;= X_{2} \cdot w_{k} \in \mathbb{R}^{m \times d_{k}}\
V &amp;= X_{2} \cdot w_{v} \in \mathbb{R}^{m \times d_{v}}\
a &amp;= softmax\left(\frac{Q \cdot K^{\top}}{d_{k}}\right)\in \mathbb{R}^{m \times n}\
\end{align}
$$</p>
<p>Thus, the attention matrix $a$ represents the attention between $X_{1}$ and $X_{2}$</p>
<h2 id="features">Features</h2>
<h3 id="multi-head">Multi-head</h3>
<p>$$
MultiHeadAttention(Q, K, V) = Concat(Attention_{i})W^{O}, i \in (0, h)
$$</p>
<ul>
<li>let each head focus on one part of input, concatenating and increasing the <strong>receptive field</strong> of the NN</li>
<li><strong>Grammar &amp; Context &amp; Rare words</strong> are what heads are focusing on</li>
<li>Compared to multi-layer attention, it can be trained parallelly</li>
<li></li>
</ul>
<h3 id="parallelism">Parallelism</h3>
<p>$$
Attention_{0:t, 0:t} =concat(Attention_{0:t-1, 0:t-1}, Attention_{0:t-1, t})
$$</p>
<p>making it possible to train parallelly.</p>
<h3 id="scaling-with-d_k">Scaling with $d_{k}$</h3>
<ul>
<li>without scaling, Softmax can easily causes gradient vanishing</li>
<li>$\sqrt{d_{k}}$: the variance of $q \cdot k \to d_{k}$, to let the variance close to 1:
$Var(A \cdot B) = Var\left(\sum\limits{A_{ij}B_{ji}}\right)=\sum\limits Var(A_{ij}B_{ji}) = d_{k}^{2}$</li>
</ul>
<h3 id="production--multiplication">Production &amp; Multiplication</h3>
<ul>
<li>production: increase representation ability?</li>
<li>multiplication: faster. performance increase along with $d_{k}$</li>
</ul>
<h3 id="layer-norm--batch-norm">Layer Norm &amp; Batch Norm</h3>
<ul>
<li>LN: Apply normalization to <strong>a whole batch</strong>.</li>
<li>BN: Apply normalization to <strong>one position</strong> across different batches</li>
</ul>
<p>BN is often applied by <strong>CNN</strong></p>
<p>For sequences with <strong>different lengths</strong>, same feature across sequences is irrelevant(BN is not designed to deal with variant-length sequences), so NLP prefers normalization within a sequence : LN.</p>
<h3 id="parallelism-1">Parallelism</h3>
<ul>
<li>Encoder: sequential</li>
<li>Decoder: only in training, using sequence mask(predicting next tokens of different sequences at the same time)</li>
</ul>
<h3 id="long-distance-dependency">Long-Distance Dependency</h3>
<p>Self-Attention can capture the co-dependent features in long-distance, since it avoids accumulating &amp; calculating hidden states for several timestamps.</p>
<h2 id="fundamental-ideas">Fundamental Ideas</h2>
<p>$Q, K ,V$ is essentially a database with global semantic relationship.</p>
<h2 id="sparse-attention">Sparse Attention</h2>
<p>In standard attention mechanism, the attention between tokens are pair-wise</p>
<p>However, it is observed that most of the time, the attention matrix $A$ is <em>sparse</em></p>
<h3 id="sparse-attention-1">Sparse Attention</h3>
<table>
<thead>
<tr>
<th style="text-align:left">Notation</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Attention kernel</td>
<td style="text-align:left">The tokens required for the attention to predict next token</td>
</tr>
</tbody>
</table>
<p>OpenAI reduces the time complexity by &ldquo;keep the value in small region, enforcing most elements as zero&rdquo;</p>
<p>It is observed that attention has gained inductive bias similar in CNN:</p>
<ul>
<li>shallow layers: patterns in local connection</li>
<li>deep layers: global patterns</li>
</ul>
<p>To introduce the sparse feature of CNN into attention, they introduce a concept: <strong>Connectivity Pattern</strong>:
$$
S = {S_{1}, &hellip; , S_{n}}
$$</p>
<p>where $S_{i}$ is the indices at timestamp i.</p>
<p>And attention is transformed to:</p>
<p>$$
\begin{align}
a(x_{i}, S_{i}) = softmax\left(\frac{(W_{q}x_{i})K_{S_{i}}^{\top}}{\sqrt{d}}\right)V_{S_{i}}\
\end{align}
$$</p>
<ul>
<li>decompose the full attention with sparse attentions</li>
<li></li>
</ul>
<p>However:</p>
<ul>
<li>the kept attention region is decided by human, not dynamic</li>
</ul>
<h3 id="position-based-sparse-attention">Position-based Sparse Attention</h3>
<h4 id="atomic-sparse-attention">Atomic Sparse Attention</h4>
<p>Single-form connection of attention units</p>
<ol>
<li>Global Attention: adding some global nodes as the center of information broadcast</li>
<li>Band Attention(a.k.a sliding window attention, or local attention): Due to the strong locality of data, limit the query of its neighboring nodes</li>
<li>Dilated Attention: Increase inductive field by using expanding window with a hole</li>
<li>Random Attention: 为了增加非局部交互的能力，每个查询随机采样一些边缘。这是基于观察<strong>随机图</strong>可以与完整图具有相似的光谱属性</li>
<li>Block Local Attention: Split the input sequence into query blocks, each block is associated with a local memory block. A query block would only focus on the <em>key</em> from its memory block</li>
</ol>
<h4 id="compound-sparse-attention">Compound Sparse Attention</h4>
<p>Combination of the atomic attentions mentioned above</p>
<h4 id="extended-sparse-attention">Extended Sparse Attention</h4>
<p>Design special sparse structure for specific data</p>
<h3 id="content-based-sparse-attention">Content-based Sparse Attention</h3>
<p>Build sparse graph on input content</p>
<h2 id="linearized-attention">Linearized Attention</h2>
<p>Though being able to parallize, it has the complexity of $\mathcal{O}(n^{2})$ in both time and space.</p>
<p>Some work have achieved linear complexity in various way</p>
<blockquote>
<p>[!TIP]
The time complexity of multipling $\mathbb{R}^{a \times b} \cdot \mathbb{R}^{b \times c}$ is $\mathcal{O}(abc)$</p>
</blockquote>
<h3 id="remove-softmax">Remove Softmax</h3>
<p>The existence of Softmax enforce the calculation of $QK^{\top}$, which is the source of $\mathcal{O}(n^{2})$.</p>
<p>Removing the softmax, the attention became: $A = QK^{\top}V$:</p>
<ul>
<li>Calculate $K^{\top}V$: $\mathcal{O}(d^{2}n) \approx \mathcal{O}(n)$</li>
<li>Calculate $Q(K^{\top}V)$: $\mathcal{O}(nd^{2}) \approx \mathcal{O}(n)$</li>
</ul>
<h3 id="replace-softmax-with-sim">Replace Softmax with sim</h3>
<p>By rewriting the $e^{q_{i}^{\top}k_{j}}$ with normal similarity function:
$$
\begin{equation}
Attention(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V})<em>{i} = \frac{\sum\limits</em>{j=1}^{n} \text{sim}(\boldsymbol{q}<em>{i}, \boldsymbol{k}<em>j)\boldsymbol{v}</em>{j}}{\sum\limits</em>{j=1}^{n} \text{sim}(\boldsymbol{q}<em>{i}, \boldsymbol{k}</em>{j})}
\end{equation}
$$</p>
<ul>
<li>Adding <em>non-negative</em> activation function(kernal method) to $q, k$: $sim(q_{i}, k_{j}) = \phi(q_{i})\varphi(k_{j})^{\top}$
<ul>
<li><a href="https://arxiv.org/abs/2006.16236">《Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention》</a>: $\phi(x) = \varphi(x) = \text{elu}(x) + 1$</li>
</ul>
</li>
<li>Apply Softmax to $Q, K$ separately: $\begin{equation}Attention(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V}) = softmax_2\left(\boldsymbol{Q}\right)softmax_1(\boldsymbol{K})^{\top}\boldsymbol{V}\end{equation}$ , where $softmax_{i}$ means softmax in $i$-th dimension
<ul>
<li><a href="https://arxiv.org/abs/1812.01243">《Efficient Attention: Attention with Linear Complexities》</a></li>
</ul>
</li>
<li>Apply taylor expansion to $e^{\boldsymbol{q}_i^{\top}\boldsymbol{k}_j}$:  $\begin{equation}\text{sim}(\boldsymbol{q}_i, \boldsymbol{k}_j) = 1 + \left( \frac{\boldsymbol{q}_i}{\Vert \boldsymbol{q}_i\Vert}\right)^{\top}\left(\frac{\boldsymbol{k}_j}{\Vert \boldsymbol{k}_j\Vert}\right)\end{equation}$</li>
</ul>
<h3 id="reformer">Reformer</h3>
<p>Approximately find the maximum Attention quickly, by LSH(Locality Sensitive Hashing)</p>
<p>This inspires us to reduce time complexity by:</p>
<ul>
<li>introducing the sparse bias into attention mechanism</li>
<li>combine structural bias(deleting connections in some neurons)</li>
</ul>
<h3 id="linformer">Linformer</h3>
<p>Project $K, V$ with two matrixes before Attention:</p>
<p>$$
\begin{align}
E, F &amp;\in \mathbb{R}^{m \times n}\
Attention(Q, K,V) &amp;= softmax(Q(EK)^{\top})FV
\end{align}
$$</p>
<blockquote>
<p>[!TIP]
Linformer is sub-sampling the sequence, so it&rsquo;s nature to think of pooling</p>
</blockquote>
<p>by disentangle attention matrix with kernal feature map</p>
<h2 id="low-rank-self-attention">Low-rank Self-Attention</h2>
<p>it is observed that most of the time, the attention matrix $a$ is <em>low-rank</em></p>
<h3 id="low-rank-parameterization">Low-rank Parameterization</h3>
<p>Parameterize the attention matrix with simpler structure, as an <em>inductive bias</em></p>
<h3 id="low-rank-approximation">Low-rank Approximation</h3>
<p>Approximate attention matrix with a low-rank matrix</p>
<h2 id="attention-with-prior">Attention with Prior</h2>
<p>Replace standard attention with prior attention distribution</p>
<h3 id="prior-that-models-locality">Prior that Models locality</h3>
<h3 id="prior-from-lower-modules">Prior from Lower Modules</h3>
<p>Adopt prior(attention) from previous attention layer</p>
<h3 id="attention-with-prior-only">Attention with Prior Only</h3>
<p>Derive the attention only with prior, not from the pari-wise dependency of input sequence</p>
<h2 id="improved-multi-head">Improved multi-head</h2>
<h3 id="head-behavior-modeling">Head Behavior Modeling</h3>
<p>It is not guaranteed that <a href="/posts/ai/models/the-attention-mechanism/#multi-head">multi-head</a> can increase the inductive field, some works have tried:</p>
<ul>
<li>Increase the feature representation ability of each head</li>
<li>guide the interactions between each heads
to achieve taht.</li>
</ul>
<h3 id="multi-head-with-restricted-spans">Multi-head with restricted Spans</h3>
<p>It might be helpful to combine global heads and local heads, reasons being:</p>
<ul>
<li>Locality</li>
<li>Effenciency</li>
</ul>
<h3 id="multi-head-with-refined-aggregation">Multi-head with Refined Aggregation</h3>
<p>Aggregate the output of each head with more complexity, rather simple <em>concatenating</em> them.</p>
<h2 id="improved-ffn">Improved FFN</h2>
<h3 id="activation">Activation</h3>
<h3 id="position-wise-ffn">Position-wise FFN</h3>
<h2 id="variants">Variants</h2>
<h3 id="glu">GLU</h3>
<p>[GLU](Gated Linear Unit) replaces the FFN with:
$$
O = (U \odot V)W_{o}, U = \phi_{u}(XW_{u}), V = \phi_{v}(XW_{v})
$$
In comparison, FFN:
$$
O = \phi(XW_{u})W_{o}
$$</p>
<blockquote>
<p>[!NOTE]
GLU replaces first MLP with the dot-product of two <strong>MLPs</strong></p>
</blockquote>
<h3 id="flash-attention">Flash Attention</h3>
<h4 id="gau">GAU</h4>
<p>Simplified attention:
$$
A = \frac{1}{n}\text{relu}^{2}\left(\frac{Q(Z)K(Z)^{T}}{\sqrt{s}}\right)= \frac{1}{ns}\text{relu}^{2}(Q(Z)K(Z)^{T}), Z = \phi_{z}(XW_{z})
$$</p>
<ul>
<li>Move the activation before QK-step</li>
<li>Removes the V</li>
<li>replace softmax with relu</li>
</ul>
<p>With Time Complexity $O(n^2)$</p>
<h2 id="applications">Applications</h2>
<h3 id="nlp">NLP</h3>
<h3 id="computer-vision">Computer Vision</h3>
<h3 id="audio">Audio</h3>
<h3 id="multi-modal">Multi-modal</h3>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
