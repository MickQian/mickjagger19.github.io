<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>VideoGeneration on Mick&#39; Blog</title>
    <link>https://mickjagger19.github.io/tags/videogeneration/</link>
    <description>Recent content in VideoGeneration on Mick&#39; Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 19 Feb 2024 16:23:58 +0800</lastBuildDate><atom:link href="https://mickjagger19.github.io/tags/videogeneration/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>DiT, Sora, and more</title>
      <link>https://mickjagger19.github.io/posts/ai/models/dit-sora-and-more/</link>
      <pubDate>Mon, 19 Feb 2024 16:23:58 +0800</pubDate>
      
      <guid>https://mickjagger19.github.io/posts/ai/models/dit-sora-and-more/</guid>
      <description>Review on DiT and sora</description>
      <content:encoded><![CDATA[<blockquote>
<p>[! WARNING]
This work is in progress</p>
</blockquote>
<table>
  <thead>
      <tr>
          <th style="text-align: left">Notations</th>
          <th style="text-align: left">Meaning</th>
          <th></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">Classifier-guided Diffusion</td>
          <td style="text-align: left">An implementation of Conditional Diffusion Models, which requires a classifier $\nabla_{x}p(x|y)$ to guide its reverse process</td>
          <td></td>
      </tr>
      <tr>
          <td style="text-align: left">Classifier-free Diffusion</td>
          <td style="text-align: left">An implementation of Conditional Diffusion Models, which doesn&rsquo;t require a <strong>classfier</strong>, the condition serves as an input to its <strong>noise predictor</strong></td>
          <td></td>
      </tr>
      <tr>
          <td style="text-align: left">Latent Diffusion Models(LDM)</td>
          <td style="text-align: left">A type of Diffusion Models where diffusion processes are done in <strong>latent space</strong></td>
          <td></td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
          <td></td>
      </tr>
  </tbody>
</table>
<h1 id="introduction">Introduction</h1>
<p>Diffusion models usually choose a <strong>UNet</strong> as its backbone for Noise Predictor, first adopted by Ho et al [^1] , which was inherited from <strong>Pixel-CNN++</strong>(widely used as the generator in VAE) with a few changes. Although some works have introduced attention blocks into low-level design, its high-level remains intact.</p>
<p><strong>DiT</strong> is proposed to apply Transformer into Diffusion Models, adhering to the best practices of <strong>Vision Transformers(ViTs)</strong></p>
<p>Also, the scaling behavios of transformers is also explored in DiT</p>
<h1 id="dit-design-space">DiT Design Space</h1>
<table>
  <thead>
      <tr>
          <th style="text-align: left">Notations</th>
          <th style="text-align: left">Meaning</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">$C$</td>
          <td style="text-align: left">the channels of input image</td>
      </tr>
      <tr>
          <td style="text-align: left">$I$</td>
          <td style="text-align: left">the dimension of $z$</td>
      </tr>
      <tr>
          <td style="text-align: left">$z \in R^{I \times I \times C}$</td>
          <td style="text-align: left">The latents, also the input of $DiT$</td>
      </tr>
      <tr>
          <td style="text-align: left">Patch</td>
          <td style="text-align: left">The unit of input.</td>
      </tr>
      <tr>
          <td style="text-align: left">$p$</td>
          <td style="text-align: left">the dimension of a single patch</td>
      </tr>
      <tr>
          <td style="text-align: left">Classifier-free Diffusion</td>
          <td style="text-align: left">An implementation of Conditional Diffusion Models, which doesn&rsquo;t require a <strong>classfier</strong>, the condition serves as an input to its <strong>noise predictor</strong></td>
      </tr>
      <tr>
          <td style="text-align: left">Latent Diffusion Models(LDM)</td>
          <td style="text-align: left">A type of Diffusion Models where diffusion processes are done in <strong>latent space</strong></td>
      </tr>
  </tbody>
</table>
<h2 id="1-patchify">1. Patchify</h2>
<p><strong>Patchify</strong> is the first layer of DiT, which converts the spatial input $z$ into a sequence to $T$ tokens, each of dimension $d$:
$$
z \in R^{I \times I \times C} \to T \cdot Token
$$
where:</p>
<ul>
<li>$Token \in R^{p \times p \times C}$</li>
<li>$T = (\frac{I}{p})^2$</li>
</ul>
<h2 id="2-positional-embeddings">2. Positional Embeddings</h2>
<p>Following patchify, we apply standard ViT <strong>frequency-based positional embeddings</strong> (the sine-cosine version) to all input tokens: $Token \to Patch$</p>
<h2 id="3-transformer-blocks">3. Transformer Blocks</h2>
<p>Following patchify, the input tokens are processed by a sequence of transformer blocks.</p>
<h3 id="in-context-conditioning">In-context conditioning</h3>
<p>In addition to noised image inputs, diffusion models sometimes process additional conditional information:</p>
<ul>
<li>$t$: noise timestamps (of DDPM)</li>
<li>$c$: class labels c(of input images)</li>
<li>natural language description(or caption)</li>
</ul>
<p>The conditions are represented as additional $Token$s, <strong>appened</strong> to the [input sequence](## 2. Positional Embeddings)</p>
<h3 id="cross-attention-block">Cross-attention block</h3>
<p>The conditional tokens are send to the cross-attention block of transformer block</p>
<h3 id="adaptie-layer-normadaln-block">Adaptie Layer Norm(adaLN) Block</h3>
<p>The <strong>Adaptive Layer Norm</strong> replaces the standard layer norm</p>
<blockquote>
<p>[! NOTE]
To be completed</p>
</blockquote>
<h2 id="4-transformer-decoder">4. Transformer Decoder</h2>
<p>After the transformer blocks, a <strong>transformer decoder</strong> is reponsible for decoding the each latent token back to tensor of size $p \times p \times C$.</p>
<p>The decoder is simply a <strong>standrad linear layer</strong></p>
<h1 id="sora">Sora</h1>
<p>The following is some major takeaways of <a href="https://openai.com/research/video-generation-models-as-world-simulators">the tenichal report of Sora</a></p>
<p>Sora is a diffusion model/diffusion transformer based on <strong>DiT</strong></p>
<h2 id="unified-representation-of-visual-data">Unified Representation of Visual Data</h2>
<p>The major part of the technial report is about the <strong>Unified Representation</strong>, which is the training data of Sora</p>
<h3 id="sources">Sources</h3>
<blockquote>
<p>We take inspiration from large language models which acquire generalist capabilities by training on internet-scale data</p>
</blockquote>
<p>Based on the description, Sora might crawl a huge amount of data from internet</p>
<h3 id="patch">Patch</h3>
<p>Following the text token concept of LLM, and patch concept from [DiT](##1. Patchify), Sora has visual <em>patches</em>.</p>
<p>The transformation of videos into patches went through 2 steps:</p>
<ol>
<li>Compress videos into lower-dimensional latents</li>
<li>Decompose the latents into <strong>spacetime</strong> patches</li>
</ol>
<blockquote>
<p>[! TIP]
<strong>spacetime</strong> implies that the tokens are embedded with timestep information</p>
</blockquote>
<p>While the original DiT tokens are fixed-sized based on the size of the latents, the <em>spacetime</em> patches used by Sora is derived from videos/images of <strong>variable resolutions, durations and aspect ratios</strong>. This gives huge flexibility in inference time, since the output is formed with any patches you like.</p>
<h2 id="scaling-transformers">Scaling Transformers</h2>
<p>They find transformers scaled effectively as video models, same as in other domains, including language modeling,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-13">13</a>,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-14">14</a> computer vision,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-15">15</a>,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-16">16</a>,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-17">17</a>,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-18">18</a> and image generation.<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-27">27</a>,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-28">28</a>,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-29">29</a></p>
<h2 id="data-preprocessing">Data preprocessing</h2>
<h3 id="native-size">Native size</h3>
<p>Different from prior approaches, which crops videos/images to standard size(e.g., 4 seconds videos at 256 * 256 resolution), they find that training on data at its <strong>native size</strong> benefits.</p>
<h3 id="native-aspect-ratios">Native aspect ratios</h3>
<p>They empirically find that training on videos at their native aspect ratios improves composition and framing. The model trained on square crops sometimes generates videos where the subject is only <strong>partially</strong> in view.</p>
<h2 id="language-understanding">Language understanding</h2>
<p>The language-understanding-ability is a crucial part of text-to-video models, as text is the major input</p>
<h3 id="re-captioning">Re-captioning</h3>
<p><strong>Re-captioning</strong> is a technique to generate descriptive captions for images/videos with the help of a <strong>highly descriptive captioner model</strong></p>
<p>First introduced in the training of DALL$\cdot$E, it is used in Sora, where GPT serves the role of <strong>captioner</strong>, who <em>turn short user prompts into longer detailed captions</em></p>
<h2 id="prompting-with-images-and-videos">Prompting with images and videos</h2>
<p>Being abled to be prompted with inputs other than text, including images and videos, Sora can perform a wide range of image and video editing tasks.</p>
<h3 id="some-applications-not-mentioned-in-the-report">Some applications not mentioned in the report</h3>
<p>Based on the presented applications, Sora might be able to do the tasks of:</p>
<ul>
<li>video generation conditioned on text/image: generate videos based on the given future/previous text and image</li>
<li></li>
</ul>
<blockquote>
<p> Model and implementation details are not included in this report.
 &ndash; video-generation-models-as-world-simulators</p>
</blockquote>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
