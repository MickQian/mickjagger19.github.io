<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>The Attention Family | Mick&#39; Blog</title>
<meta name="keywords" content="Attention, Transformer">
<meta name="description" content="Attention">
<meta name="author" content="Mick">
<link rel="canonical" href="https://mickjagger19.github.io/posts/ai/models/the-attention-mechanism/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.7da7716a1f2d0725f74c6ae7f8d6adafc43aabe2b366b65bfbf433448e2a2001.css" integrity="sha256-fadxah8tByX3TGrn&#43;Natr8Q6q&#43;KzZrZb&#43;/QzRI4qIAE=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://mickjagger19.github.io/favicon.ico">
<link rel="apple-touch-icon" href="https://mickjagger19.github.io/apple-touch-icon.png">
<link rel="alternate" hreflang="en" href="https://mickjagger19.github.io/posts/ai/models/the-attention-mechanism/">

<meta name="twitter:title" content="The Attention Family | Mick&#39; Blog" />
<meta name="twitter:description" content="Attention" />
<meta property="og:title" content="The Attention Family | Mick&#39; Blog" />
<meta property="og:description" content="Attention" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://mickjagger19.github.io/posts/ai/models/the-attention-mechanism/" />
<meta property="article:section" content="posts" />
  <meta property="article:published_time" content="2024-01-21T12:47:01&#43;08:00" />
  <meta property="article:modified_time" content="2024-01-21T12:47:01&#43;08:00" />


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://mickjagger19.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "The Attention Family",
      "item": "https://mickjagger19.github.io/posts/ai/models/the-attention-mechanism/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "The Attention Family | Mick' Blog",
  "name": "The Attention Family",
  "description": "Attention",
  "keywords": [
    "Attention", "Transformer"
  ],
  "wordCount" : "1287",
  "inLanguage": "en",
  "datePublished": "2024-01-21T12:47:01+08:00",
  "dateModified": "2024-01-21T12:47:01+08:00",
  "author":{
    "@type": "Person",
    "name": "Mick"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://mickjagger19.github.io/posts/ai/models/the-attention-mechanism/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Mick' Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://mickjagger19.github.io/favicon.ico"
    }
  }
}
</script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"
      integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"
        integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8"
        crossorigin="anonymous"></script>


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
        integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        renderMathInElement(document.body, {
            
            
            delimiters: [
                {left: '$$', right: '$$', display: true},
                {left: '$', right: '$', display: false},
                {left: '\\(', right: '\\)', display: false},
                {left: '\\[', right: '\\]', display: true}
            ],
            
            throwOnError: false
        });
    });
</script>












































































































<link rel="stylesheet" href="/css/main.css">
<link rel="stylesheet" href="/assets/scss/main.scss">
<link rel="stylesheet" href="/css/custom.css">


<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>

</head>

<body class=" dark type-posts kind-page layout-" id="top"><script data-no-instant>
    function switchTheme(theme) {
        switch (theme) {
            case 'light':
                document.body.classList.remove('dark');
                break;
            case 'dark':
                document.body.classList.add('dark');
                break;
            
            default:
                if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
                    document.body.classList.add('dark');
                }
        }
    }

    function isDarkTheme() {
        return document.body.className.includes("dark");
    }

    function getPrefTheme() {
        return localStorage.getItem("pref-theme");
    }

    function setPrefTheme(theme) {
        switchTheme(theme)
        localStorage.setItem("pref-theme", theme);
    }

    const toggleThemeCallbacks = {}
    toggleThemeCallbacks['main'] = (isDark) => {
        
        if (isDark) {
            setPrefTheme('light');
        } else {
            setPrefTheme('dark');
        }
    }

    
    
    
    window.addEventListener('toggle-theme', function () {
        
        const isDark = isDarkTheme()
        for (const key in toggleThemeCallbacks) {
            toggleThemeCallbacks[key](isDark)
        }
    });

    
    function toggleThemeListener() {
        
        window.dispatchEvent(new CustomEvent('toggle-theme'));
    }

</script>
<script>
    
    (function () {
        const defaultTheme = 'dark';
        const prefTheme = getPrefTheme();
        const theme = prefTheme ? prefTheme : defaultTheme;

        switchTheme(theme);
    })();
</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://mickjagger19.github.io/" accesskey="h" title="Mick&#39; Blog (Alt + H)">Mick&#39; Blog</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                         fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                         stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                         fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                         stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://mickjagger19.github.io/posts/" title="Posts" class="active"
                >
                <span>Posts
                </span>
                </a>
            </li>
            <li>
                <a href="https://mickjagger19.github.io/archives/" title="Archive"
                >
                <span>Archive
                </span>
                </a>
            </li>
            <li>
                <a href="https://mickjagger19.github.io/search/" title="Search (Alt &#43; /)"data-no-instant accesskey=/
                >
                <span>Search
                </span>
                </a>
            </li>
            <li>
                <a href="https://mickjagger19.github.io/tags/" title="Tags"
                >
                <span>Tags
                </span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main post">

<article class="post-single">
    <header class="post-header">
        <h1 class="post-title">The Attention Family</h1>
        <div class="post-meta"><span class="meta-item">
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
       stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar"
       style="user-select: text;"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"
                                        style="user-select: text;"></rect><line x1="16" y1="2" x2="16" y2="6"
                                                                                style="user-select: text;"></line><line
          x1="8" y1="2" x2="8" y2="6" style="user-select: text;"></line><line x1="3" y1="10" x2="21" y2="10"
                                                                              style="user-select: text;"></line></svg>
  <span>January 21, 2024</span></span><span class="meta-item">
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke="currentColor"
       stroke-width="2" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"
                                                                                         fill="none"></path><circle
          cx="12" cy="12" r="9"></circle><polyline points="12 7 12 12 15 15"></polyline></svg>
  <span>7 min</span></span><span class="meta-item">
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" stroke="currentColor"
       stroke-width="1" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"
                                                                                         fill="none"></path><circle
          cx="12" cy="7" r="4"></circle><path d="M6 21v-2a4 4 0 0 1 4 -4h4a4 4 0 0 1 4 4v2"></path></svg>Mick</span>

            
            
        </div>
    </header> <div class="toc side right">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#terminologies" aria-label="Terminologies">Terminologies</a></li>
                <li>
                    <a href="#the-attention-of-human" aria-label="The Attention of human">The Attention of human</a></li>
                <li>
                    <a href="#self-attention" aria-label="Self-Attention">Self-Attention</a></li>
                <li>
                    <a href="#cross-attention" aria-label="Cross-Attention">Cross-Attention</a></li>
                <li>
                    <a href="#features" aria-label="Features">Features</a><ul>
                        
                <li>
                    <a href="#multi-head" aria-label="Multi-head">Multi-head</a></li>
                <li>
                    <a href="#parallelism" aria-label="Parallelism">Parallelism</a></li>
                <li>
                    <a href="#scaling-with-d_k" aria-label="Scaling with $d_{k}$">Scaling with $d_{k}$</a></li>
                <li>
                    <a href="#production--multiplication" aria-label="Production &amp; Multiplication">Production &amp; Multiplication</a></li>
                <li>
                    <a href="#layer-norm--batch-norm" aria-label="Layer Norm &amp; Batch Norm">Layer Norm &amp; Batch Norm</a></li>
                <li>
                    <a href="#parallelism-1" aria-label="Parallelism">Parallelism</a></li>
                <li>
                    <a href="#long-distance-dependency" aria-label="Long-Distance Dependency">Long-Distance Dependency</a></li></ul>
                </li>
                <li>
                    <a href="#fundamental-ideas" aria-label="Fundamental Ideas">Fundamental Ideas</a></li>
                <li>
                    <a href="#sparse-attention" aria-label="Sparse Attention">Sparse Attention</a><ul>
                        
                <li>
                    <a href="#sparse-attention-1" aria-label="Sparse Attention">Sparse Attention</a></li>
                <li>
                    <a href="#position-based-sparse-attention" aria-label="Position-based Sparse Attention">Position-based Sparse Attention</a><ul>
                        
                <li>
                    <a href="#atomic-sparse-attention" aria-label="Atomic Sparse Attention">Atomic Sparse Attention</a></li>
                <li>
                    <a href="#compound-sparse-attention" aria-label="Compound Sparse Attention">Compound Sparse Attention</a></li>
                <li>
                    <a href="#extended-sparse-attention" aria-label="Extended Sparse Attention">Extended Sparse Attention</a></li></ul>
                </li>
                <li>
                    <a href="#content-based-sparse-attention" aria-label="Content-based Sparse Attention">Content-based Sparse Attention</a></li></ul>
                </li>
                <li>
                    <a href="#linearized-attention" aria-label="Linearized Attention">Linearized Attention</a><ul>
                        
                <li>
                    <a href="#remove-softmax" aria-label="Remove Softmax">Remove Softmax</a></li>
                <li>
                    <a href="#replace-softmax-with-sim" aria-label="Replace Softmax with sim">Replace Softmax with sim</a></li>
                <li>
                    <a href="#reformer" aria-label="Reformer">Reformer</a></li>
                <li>
                    <a href="#linformer" aria-label="Linformer">Linformer</a></li></ul>
                </li>
                <li>
                    <a href="#low-rank-self-attention" aria-label="Low-rank Self-Attention">Low-rank Self-Attention</a><ul>
                        
                <li>
                    <a href="#low-rank-parameterization" aria-label="Low-rank Parameterization">Low-rank Parameterization</a></li>
                <li>
                    <a href="#low-rank-approximation" aria-label="Low-rank Approximation">Low-rank Approximation</a></li></ul>
                </li>
                <li>
                    <a href="#attention-with-prior" aria-label="Attention with Prior">Attention with Prior</a><ul>
                        
                <li>
                    <a href="#prior-that-models-locality" aria-label="Prior that Models locality">Prior that Models locality</a></li>
                <li>
                    <a href="#prior-from-lower-modules" aria-label="Prior from Lower Modules">Prior from Lower Modules</a></li>
                <li>
                    <a href="#attention-with-prior-only" aria-label="Attention with Prior Only">Attention with Prior Only</a></li></ul>
                </li>
                <li>
                    <a href="#improved-multi-head" aria-label="Improved multi-head">Improved multi-head</a><ul>
                        
                <li>
                    <a href="#head-behavior-modeling" aria-label="Head Behavior Modeling">Head Behavior Modeling</a></li>
                <li>
                    <a href="#multi-head-with-restricted-spans" aria-label="Multi-head with restricted Spans">Multi-head with restricted Spans</a></li>
                <li>
                    <a href="#multi-head-with-refined-aggregation" aria-label="Multi-head with Refined Aggregation">Multi-head with Refined Aggregation</a></li></ul>
                </li>
                <li>
                    <a href="#improved-ffn" aria-label="Improved FFN">Improved FFN</a><ul>
                        
                <li>
                    <a href="#activation" aria-label="Activation">Activation</a></li>
                <li>
                    <a href="#position-wise-ffn" aria-label="Position-wise FFN">Position-wise FFN</a></li></ul>
                </li>
                <li>
                    <a href="#variants" aria-label="Variants">Variants</a><ul>
                        
                <li>
                    <a href="#glu" aria-label="GLU">GLU</a></li>
                <li>
                    <a href="#flash-attention" aria-label="Flash Attention">Flash Attention</a><ul>
                        
                <li>
                    <a href="#gau" aria-label="GAU">GAU</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#applications" aria-label="Applications">Applications</a><ul>
                        
                <li>
                    <a href="#nlp" aria-label="NLP">NLP</a></li>
                <li>
                    <a href="#computer-vision" aria-label="Computer Vision">Computer Vision</a></li>
                <li>
                    <a href="#audio" aria-label="Audio">Audio</a></li>
                <li>
                    <a href="#multi-modal" aria-label="Multi-modal">Multi-modal</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

    <div class="post-content"><h2 id="terminologies">Terminologies<a hidden class="anchor" aria-hidden="true" href="#terminologies">¶</a></h2>
<table>
  <thead>
      <tr>
          <th style="text-align: left">Term</th>
          <th style="text-align: left">Meaning</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left"><a href="https://en.wikipedia.org/wiki/Receptive_field">Receptive field</a>(a.k.a. sensory space)</td>
          <td style="text-align: left">A concept originally from biology, adopted in modern artificial deep neural networks (especially <strong>CNN</strong>), describing the size of input image which can affects the output of <strong>neurons</strong></td>
      </tr>
      <tr>
          <td style="text-align: left">$d_{model}$</td>
          <td style="text-align: left">dimension of the word embedding(usually 512 = 64 * 8)</td>
      </tr>
      <tr>
          <td style="text-align: left">$d_{k}$</td>
          <td style="text-align: left">dimension of $w_q, w_k$</td>
      </tr>
      <tr>
          <td style="text-align: left">$d_{v}$</td>
          <td style="text-align: left">dimension of $w_v$</td>
      </tr>
      <tr>
          <td style="text-align: left">$w_{q} \in \mathbb{R}^{d_{model} \times d_{k}}$</td>
          <td style="text-align: left"></td>
      </tr>
      <tr>
          <td style="text-align: left">$w_k \in \mathbb{R}^{d_{model} \times d_{k}}$</td>
          <td style="text-align: left"></td>
      </tr>
      <tr>
          <td style="text-align: left">$w_{v} \in \mathbb{R}^{d_{model} \times d_{v}}$</td>
          <td style="text-align: left"></td>
      </tr>
      <tr>
          <td style="text-align: left">$B$</td>
          <td style="text-align: left">Batch size</td>
      </tr>
      <tr>
          <td style="text-align: left">$S$</td>
          <td style="text-align: left">Sequence Length</td>
      </tr>
      <tr>
          <td style="text-align: left">$X$</td>
          <td style="text-align: left">input</td>
      </tr>
      <tr>
          <td style="text-align: left">Structural Prior</td>
          <td style="text-align: left"></td>
      </tr>
      <tr>
          <td style="text-align: left">translation equivariance</td>
          <td style="text-align: left">An attribute of model, the ability of model to recognize objects does not varies with the <strong>geometric</strong> transformations of the input(shift, rotate, projection, etc)</td>
      </tr>
  </tbody>
</table>
<h2 id="the-attention-of-human">The Attention of human<a hidden class="anchor" aria-hidden="true" href="#the-attention-of-human">¶</a></h2>
<p>Selective attention is a mechanism unique to human vision. By swiftly scanning the image, human acquires important areas(a.k.a. focus). After this, human pays more attention to these areas, as there are more valuable information.</p>
<h2 id="self-attention">Self-Attention<a hidden class="anchor" aria-hidden="true" href="#self-attention">¶</a></h2>
<p>Self <em>Scaled-Dot</em> Attention:</p>
<p>$$
\begin{align}
X &amp;= X_{text_encoding}+ X_{positional_embedding}\
Q &amp;= X \cdot w_{q} \in \mathbb{R}^{S \times d_{k}}\
K &amp;= X \cdot w_{k} \in \mathbb{R}^{S \times d_{k}}\
V &amp;= X \cdot w_{v} \in \mathbb{R}^{S \times d_{v}}\
a &amp;= softmax\left(\frac{Q \cdot K^{\top}}{d_{k}}\right)\in \mathbb{R}^{S \times S}\
Attention &amp;= a V\in \mathbb{R}^{S \times d_{k}}\
\end{align}
$$</p>
<blockquote>
<p>[!TIP]</p>
<ul>
<li>Attention is the weighted (attention score) v of each token over other tokens</li>
</ul>
</blockquote>
<p>The major different between attention and typical <strong>RNN</strong> is: The generation of next token doesn&rsquo;t rely on hidden state from previous timestamp, but instead alter the embedding of the token directly with positional embedding.</p>
<blockquote>
<p>[!TIP]
Some researchers seems Attention as a kind of soft addressing</p>
</blockquote>
<h2 id="cross-attention">Cross-Attention<a hidden class="anchor" aria-hidden="true" href="#cross-attention">¶</a></h2>
<p>Different from <a href="/posts/ai/models/the-attention-mechanism/#self-attention">Self-Attention</a>, the $Q, K$ of cross-attention comes from another sequence($X_{2}$):
$$
\begin{align}
Q &amp;= X_{1} \cdot w_{q} \in \mathbb{R}^{n \times d_{k}}\
K &amp;= X_{2} \cdot w_{k} \in \mathbb{R}^{m \times d_{k}}\
V &amp;= X_{2} \cdot w_{v} \in \mathbb{R}^{m \times d_{v}}\
a &amp;= softmax\left(\frac{Q \cdot K^{\top}}{d_{k}}\right)\in \mathbb{R}^{m \times n}\
\end{align}
$$</p>
<p>Thus, the attention matrix $a$ represents the attention between $X_{1}$ and $X_{2}$</p>
<h2 id="features">Features<a hidden class="anchor" aria-hidden="true" href="#features">¶</a></h2>
<h3 id="multi-head">Multi-head<a hidden class="anchor" aria-hidden="true" href="#multi-head">¶</a></h3>
<p>$$
MultiHeadAttention(Q, K, V) = Concat(Attention_{i})W^{O}, i \in (0, h)
$$</p>
<ul>
<li>let each head focus on one part of input, concatenating and increasing the <strong>receptive field</strong> of the NN</li>
<li><strong>Grammar &amp; Context &amp; Rare words</strong> are what heads are focusing on</li>
<li>Compared to multi-layer attention, it can be trained parallelly</li>
<li></li>
</ul>
<h3 id="parallelism">Parallelism<a hidden class="anchor" aria-hidden="true" href="#parallelism">¶</a></h3>
<p>$$
Attention_{0:t, 0:t} =concat(Attention_{0:t-1, 0:t-1}, Attention_{0:t-1, t})
$$</p>
<p>making it possible to train parallelly.</p>
<h3 id="scaling-with-d_k">Scaling with $d_{k}$<a hidden class="anchor" aria-hidden="true" href="#scaling-with-d_k">¶</a></h3>
<ul>
<li>without scaling, Softmax can easily causes gradient vanishing</li>
<li>$\sqrt{d_{k}}$: the variance of $q \cdot k \to d_{k}$, to let the variance close to 1:
$Var(A \cdot B) = Var\left(\sum\limits{A_{ij}B_{ji}}\right)=\sum\limits Var(A_{ij}B_{ji}) = d_{k}^{2}$</li>
</ul>
<h3 id="production--multiplication">Production &amp; Multiplication<a hidden class="anchor" aria-hidden="true" href="#production--multiplication">¶</a></h3>
<ul>
<li>production: increase representation ability?</li>
<li>multiplication: faster. performance increase along with $d_{k}$</li>
</ul>
<h3 id="layer-norm--batch-norm">Layer Norm &amp; Batch Norm<a hidden class="anchor" aria-hidden="true" href="#layer-norm--batch-norm">¶</a></h3>
<ul>
<li>LN: Apply normalization to <strong>a whole batch</strong>.</li>
<li>BN: Apply normalization to <strong>one position</strong> across different batches</li>
</ul>
<p>BN is often applied by <strong>CNN</strong></p>
<p>For sequences with <strong>different lengths</strong>, same feature across sequences is irrelevant(BN is not designed to deal with variant-length sequences), so NLP prefers normalization within a sequence : LN.</p>
<h3 id="parallelism-1">Parallelism<a hidden class="anchor" aria-hidden="true" href="#parallelism-1">¶</a></h3>
<ul>
<li>Encoder: sequential</li>
<li>Decoder: only in training, using sequence mask(predicting next tokens of different sequences at the same time)</li>
</ul>
<h3 id="long-distance-dependency">Long-Distance Dependency<a hidden class="anchor" aria-hidden="true" href="#long-distance-dependency">¶</a></h3>
<p>Self-Attention can capture the co-dependent features in long-distance, since it avoids accumulating &amp; calculating hidden states for several timestamps.</p>
<h2 id="fundamental-ideas">Fundamental Ideas<a hidden class="anchor" aria-hidden="true" href="#fundamental-ideas">¶</a></h2>
<p>$Q, K ,V$ is essentially a database with global semantic relationship.</p>
<h2 id="sparse-attention">Sparse Attention<a hidden class="anchor" aria-hidden="true" href="#sparse-attention">¶</a></h2>
<p>In standard attention mechanism, the attention between tokens are pair-wise</p>
<p>However, it is observed that most of the time, the attention matrix $A$ is <em>sparse</em></p>
<h3 id="sparse-attention-1">Sparse Attention<a hidden class="anchor" aria-hidden="true" href="#sparse-attention-1">¶</a></h3>
<table>
  <thead>
      <tr>
          <th style="text-align: left">Notation</th>
          <th style="text-align: left">Meaning</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">Attention kernel</td>
          <td style="text-align: left">The tokens required for the attention to predict next token</td>
      </tr>
  </tbody>
</table>
<p>OpenAI reduces the time complexity by &ldquo;keep the value in small region, enforcing most elements as zero&rdquo;</p>
<p>It is observed that attention has gained inductive bias similar in CNN:</p>
<ul>
<li>shallow layers: patterns in local connection</li>
<li>deep layers: global patterns</li>
</ul>
<p>To introduce the sparse feature of CNN into attention, they introduce a concept: <strong>Connectivity Pattern</strong>:
$$
S = {S_{1}, &hellip; , S_{n}}
$$</p>
<p>where $S_{i}$ is the indices at timestamp i.</p>
<p>And attention is transformed to:</p>
<p>$$
\begin{align}
a(x_{i}, S_{i}) = softmax\left(\frac{(W_{q}x_{i})K_{S_{i}}^{\top}}{\sqrt{d}}\right)V_{S_{i}}\
\end{align}
$$</p>
<ul>
<li>decompose the full attention with sparse attentions</li>
<li></li>
</ul>
<p>However:</p>
<ul>
<li>the kept attention region is decided by human, not dynamic</li>
</ul>
<h3 id="position-based-sparse-attention">Position-based Sparse Attention<a hidden class="anchor" aria-hidden="true" href="#position-based-sparse-attention">¶</a></h3>
<h4 id="atomic-sparse-attention">Atomic Sparse Attention<a hidden class="anchor" aria-hidden="true" href="#atomic-sparse-attention">¶</a></h4>
<p>Single-form connection of attention units</p>
<ol>
<li>Global Attention: adding some global nodes as the center of information broadcast</li>
<li>Band Attention(a.k.a sliding window attention, or local attention): Due to the strong locality of data, limit the query of its neighboring nodes</li>
<li>Dilated Attention: Increase inductive field by using expanding window with a hole</li>
<li>Random Attention: 为了增加非局部交互的能力，每个查询随机采样一些边缘。这是基于观察<strong>随机图</strong>可以与完整图具有相似的光谱属性</li>
<li>Block Local Attention: Split the input sequence into query blocks, each block is associated with a local memory block. A query block would only focus on the <em>key</em> from its memory block</li>
</ol>
<h4 id="compound-sparse-attention">Compound Sparse Attention<a hidden class="anchor" aria-hidden="true" href="#compound-sparse-attention">¶</a></h4>
<p>Combination of the atomic attentions mentioned above</p>
<h4 id="extended-sparse-attention">Extended Sparse Attention<a hidden class="anchor" aria-hidden="true" href="#extended-sparse-attention">¶</a></h4>
<p>Design special sparse structure for specific data</p>
<h3 id="content-based-sparse-attention">Content-based Sparse Attention<a hidden class="anchor" aria-hidden="true" href="#content-based-sparse-attention">¶</a></h3>
<p>Build sparse graph on input content</p>
<h2 id="linearized-attention">Linearized Attention<a hidden class="anchor" aria-hidden="true" href="#linearized-attention">¶</a></h2>
<p>Though being able to parallize, it has the complexity of $\mathcal{O}(n^{2})$ in both time and space.</p>
<p>Some work have achieved linear complexity in various way</p>
<blockquote>
<p>[!TIP]
The time complexity of multipling $\mathbb{R}^{a \times b} \cdot \mathbb{R}^{b \times c}$ is $\mathcal{O}(abc)$</p>
</blockquote>
<h3 id="remove-softmax">Remove Softmax<a hidden class="anchor" aria-hidden="true" href="#remove-softmax">¶</a></h3>
<p>The existence of Softmax enforce the calculation of $QK^{\top}$, which is the source of $\mathcal{O}(n^{2})$.</p>
<p>Removing the softmax, the attention became: $A = QK^{\top}V$:</p>
<ul>
<li>Calculate $K^{\top}V$: $\mathcal{O}(d^{2}n) \approx \mathcal{O}(n)$</li>
<li>Calculate $Q(K^{\top}V)$: $\mathcal{O}(nd^{2}) \approx \mathcal{O}(n)$</li>
</ul>
<h3 id="replace-softmax-with-sim">Replace Softmax with sim<a hidden class="anchor" aria-hidden="true" href="#replace-softmax-with-sim">¶</a></h3>
<p>By rewriting the $e^{q_{i}^{\top}k_{j}}$ with normal similarity function:
$$
\begin{equation}
Attention(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V})<em>{i} = \frac{\sum\limits</em>{j=1}^{n} \text{sim}(\boldsymbol{q}<em>{i}, \boldsymbol{k}<em>j)\boldsymbol{v}</em>{j}}{\sum\limits</em>{j=1}^{n} \text{sim}(\boldsymbol{q}<em>{i}, \boldsymbol{k}</em>{j})}
\end{equation}
$$</p>
<ul>
<li>Adding <em>non-negative</em> activation function(kernal method) to $q, k$: $sim(q_{i}, k_{j}) = \phi(q_{i})\varphi(k_{j})^{\top}$
<ul>
<li><a href="https://arxiv.org/abs/2006.16236">《Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention》</a>: $\phi(x) = \varphi(x) = \text{elu}(x) + 1$</li>
</ul>
</li>
<li>Apply Softmax to $Q, K$ separately: $\begin{equation}Attention(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V}) = softmax_2\left(\boldsymbol{Q}\right)softmax_1(\boldsymbol{K})^{\top}\boldsymbol{V}\end{equation}$ , where $softmax_{i}$ means softmax in $i$-th dimension
<ul>
<li><a href="https://arxiv.org/abs/1812.01243">《Efficient Attention: Attention with Linear Complexities》</a></li>
</ul>
</li>
<li>Apply taylor expansion to $e^{\boldsymbol{q}_i^{\top}\boldsymbol{k}_j}$:  $\begin{equation}\text{sim}(\boldsymbol{q}_i, \boldsymbol{k}_j) = 1 + \left( \frac{\boldsymbol{q}_i}{\Vert \boldsymbol{q}_i\Vert}\right)^{\top}\left(\frac{\boldsymbol{k}_j}{\Vert \boldsymbol{k}_j\Vert}\right)\end{equation}$</li>
</ul>
<h3 id="reformer">Reformer<a hidden class="anchor" aria-hidden="true" href="#reformer">¶</a></h3>
<p>Approximately find the maximum Attention quickly, by LSH(Locality Sensitive Hashing)</p>
<p>This inspires us to reduce time complexity by:</p>
<ul>
<li>introducing the sparse bias into attention mechanism</li>
<li>combine structural bias(deleting connections in some neurons)</li>
</ul>
<h3 id="linformer">Linformer<a hidden class="anchor" aria-hidden="true" href="#linformer">¶</a></h3>
<p>Project $K, V$ with two matrixes before Attention:</p>
<p>$$
\begin{align}
E, F &amp;\in \mathbb{R}^{m \times n}\
Attention(Q, K,V) &amp;= softmax(Q(EK)^{\top})FV
\end{align}
$$</p>
<blockquote>
<p>[!TIP]
Linformer is sub-sampling the sequence, so it&rsquo;s nature to think of pooling</p>
</blockquote>
<p>by disentangle attention matrix with kernal feature map</p>
<h2 id="low-rank-self-attention">Low-rank Self-Attention<a hidden class="anchor" aria-hidden="true" href="#low-rank-self-attention">¶</a></h2>
<p>it is observed that most of the time, the attention matrix $a$ is <em>low-rank</em></p>
<h3 id="low-rank-parameterization">Low-rank Parameterization<a hidden class="anchor" aria-hidden="true" href="#low-rank-parameterization">¶</a></h3>
<p>Parameterize the attention matrix with simpler structure, as an <em>inductive bias</em></p>
<h3 id="low-rank-approximation">Low-rank Approximation<a hidden class="anchor" aria-hidden="true" href="#low-rank-approximation">¶</a></h3>
<p>Approximate attention matrix with a low-rank matrix</p>
<h2 id="attention-with-prior">Attention with Prior<a hidden class="anchor" aria-hidden="true" href="#attention-with-prior">¶</a></h2>
<p>Replace standard attention with prior attention distribution</p>
<h3 id="prior-that-models-locality">Prior that Models locality<a hidden class="anchor" aria-hidden="true" href="#prior-that-models-locality">¶</a></h3>
<h3 id="prior-from-lower-modules">Prior from Lower Modules<a hidden class="anchor" aria-hidden="true" href="#prior-from-lower-modules">¶</a></h3>
<p>Adopt prior(attention) from previous attention layer</p>
<h3 id="attention-with-prior-only">Attention with Prior Only<a hidden class="anchor" aria-hidden="true" href="#attention-with-prior-only">¶</a></h3>
<p>Derive the attention only with prior, not from the pari-wise dependency of input sequence</p>
<h2 id="improved-multi-head">Improved multi-head<a hidden class="anchor" aria-hidden="true" href="#improved-multi-head">¶</a></h2>
<h3 id="head-behavior-modeling">Head Behavior Modeling<a hidden class="anchor" aria-hidden="true" href="#head-behavior-modeling">¶</a></h3>
<p>It is not guaranteed that <a href="/posts/ai/models/the-attention-mechanism/#multi-head">multi-head</a> can increase the inductive field, some works have tried:</p>
<ul>
<li>Increase the feature representation ability of each head</li>
<li>guide the interactions between each heads
to achieve taht.</li>
</ul>
<h3 id="multi-head-with-restricted-spans">Multi-head with restricted Spans<a hidden class="anchor" aria-hidden="true" href="#multi-head-with-restricted-spans">¶</a></h3>
<p>It might be helpful to combine global heads and local heads, reasons being:</p>
<ul>
<li>Locality</li>
<li>Effenciency</li>
</ul>
<h3 id="multi-head-with-refined-aggregation">Multi-head with Refined Aggregation<a hidden class="anchor" aria-hidden="true" href="#multi-head-with-refined-aggregation">¶</a></h3>
<p>Aggregate the output of each head with more complexity, rather simple <em>concatenating</em> them.</p>
<h2 id="improved-ffn">Improved FFN<a hidden class="anchor" aria-hidden="true" href="#improved-ffn">¶</a></h2>
<h3 id="activation">Activation<a hidden class="anchor" aria-hidden="true" href="#activation">¶</a></h3>
<h3 id="position-wise-ffn">Position-wise FFN<a hidden class="anchor" aria-hidden="true" href="#position-wise-ffn">¶</a></h3>
<h2 id="variants">Variants<a hidden class="anchor" aria-hidden="true" href="#variants">¶</a></h2>
<h3 id="glu">GLU<a hidden class="anchor" aria-hidden="true" href="#glu">¶</a></h3>
<p>[GLU](Gated Linear Unit) replaces the FFN with:
$$
O = (U \odot V)W_{o}, U = \phi_{u}(XW_{u}), V = \phi_{v}(XW_{v})
$$
In comparison, FFN:
$$
O = \phi(XW_{u})W_{o}
$$</p>
<blockquote>
<p>[!NOTE]
GLU replaces first MLP with the dot-product of two <strong>MLPs</strong></p>
</blockquote>
<h3 id="flash-attention">Flash Attention<a hidden class="anchor" aria-hidden="true" href="#flash-attention">¶</a></h3>
<h4 id="gau">GAU<a hidden class="anchor" aria-hidden="true" href="#gau">¶</a></h4>
<p>Simplified attention:
$$
A = \frac{1}{n}\text{relu}^{2}\left(\frac{Q(Z)K(Z)^{T}}{\sqrt{s}}\right)= \frac{1}{ns}\text{relu}^{2}(Q(Z)K(Z)^{T}), Z = \phi_{z}(XW_{z})
$$</p>
<ul>
<li>Move the activation before QK-step</li>
<li>Removes the V</li>
<li>replace softmax with relu</li>
</ul>
<p>With Time Complexity $O(n^2)$</p>
<h2 id="applications">Applications<a hidden class="anchor" aria-hidden="true" href="#applications">¶</a></h2>
<h3 id="nlp">NLP<a hidden class="anchor" aria-hidden="true" href="#nlp">¶</a></h3>
<h3 id="computer-vision">Computer Vision<a hidden class="anchor" aria-hidden="true" href="#computer-vision">¶</a></h3>
<h3 id="audio">Audio<a hidden class="anchor" aria-hidden="true" href="#audio">¶</a></h3>
<h3 id="multi-modal">Multi-modal<a hidden class="anchor" aria-hidden="true" href="#multi-modal">¶</a></h3>


    </div>

    <footer class="post-footer"><ul class="post-tags"><li>
                <a href="https://mickjagger19.github.io/tags/attention/">Attention</a>
            </li><li>
                <a href="https://mickjagger19.github.io/tags/transformer/">Transformer</a>
            </li></ul>
<nav class="paginav">
    <a class="prev" href="https://mickjagger19.github.io/posts/ai/rl/peft/">
    <span class="title">
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none"
           stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"
           class="feather feather-arrow-left" style="user-select: text;"><line x1="19" y1="12" x2="5" y2="12"
                                                                               style="user-select: text;"></line><polyline
              points="12 19 5 12 12 5" style="user-select: text;"></polyline></svg>&nbsp;
    </span>
        <br>
        <span>Tuning</span>
    </a>
    <a class="next" href="https://mickjagger19.github.io/posts/ai/models/the-gan-family/">
    <span class="title">
      &nbsp;<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none"
                 stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"
                 class="feather feather-arrow-right" style="user-select: text;"><line x1="5" y1="12" x2="19" y2="12"
                                                                                      style="user-select: text;"></line><polyline
            points="12 5 19 12 12 19" style="user-select: text;"></polyline></svg>
    </span>
        <br>
        <span>GAN &amp; Variants</span>
    </a>
</nav>

    </footer>
    <div class="comments-separator"></div>
</article>
    </main>
    
<footer class="footer">
  <span>&copy; 2025 <a href="https://mickjagger19.github.io/">Mick&#39; Blog</a></span><span style="display: inline-block; margin-left: 1em;">
    <a href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA</a>
  </span>
  <span style="display: inline-block; margin-left: 1em;">
    Powered by
    <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
    <a href="https://github.com/reorx/hugo-PaperModX/" rel="noopener" target="_blank">PaperModX</a>
  </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
    <path d="M12 6H0l6-6z" />
  </svg>
</a>

<script>
  (function() {
     
    const disableThemeToggle = '' == '1';
    if (disableThemeToggle) {
      return;
    }

    let button = document.getElementById("theme-toggle")
    
    button.removeEventListener('click', toggleThemeListener)
    
    button.addEventListener('click', toggleThemeListener)
  })();
</script>

<script>
  (function () {
    let menu = document.getElementById('menu')
    if (menu) {
      menu.scrollLeft = localStorage.getItem("menu-scroll-position");
      menu.onscroll = function () {
        localStorage.setItem("menu-scroll-position", menu.scrollLeft);
      }
    }

    const disableSmoothScroll = '' == '1';
    const enableInstantClick = '' == '1';
    
    if (window.matchMedia('(prefers-reduced-motion: reduce)').matches || disableSmoothScroll || enableInstantClick) {
      return;
    }
    
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener("click", function (e) {
        e.preventDefault();
        var id = this.getAttribute("href").substr(1);
        document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
          behavior: "smooth"
        });
        if (id === "top") {
          history.replaceState(null, null, " ");
        } else {
          history.pushState(null, null, `#${id}`);
        }
      });
    });
  })();
</script>
<script>
  var mybutton = document.getElementById("top-link");
  window.onscroll = function () {
    if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
      mybutton.style.visibility = "visible";
      mybutton.style.opacity = "1";
    } else {
      mybutton.style.visibility = "hidden";
      mybutton.style.opacity = "0";
    }
  };
</script>
<script>
  if (window.scrollListeners) {
    
    for (const listener of scrollListeners) {
      window.removeEventListener('scroll', listener)
    }
  }
  window.scrollListeners = []
</script>



<script src="/js/medium-zoom.min.js" data-no-instant
></script>
<script>
  document.querySelectorAll('pre > code').forEach((codeblock) => {
    const container = codeblock.parentNode.parentNode;

    const copybutton = document.createElement('button');
    copybutton.classList.add('copy-code');
    copybutton.innerText = 'copy';

    function copyingDone() {
      copybutton.innerText = 'copied!';
      setTimeout(() => {
        copybutton.innerText = 'copy';
      }, 2000);
    }

    copybutton.addEventListener('click', (cb) => {
      if ('clipboard' in navigator) {
        navigator.clipboard.writeText(codeblock.textContent);
        copyingDone();
        return;
      }

      const range = document.createRange();
      range.selectNodeContents(codeblock);
      const selection = window.getSelection();
      selection.removeAllRanges();
      selection.addRange(range);
      try {
        document.execCommand('copy');
        copyingDone();
      } catch (e) { };
      selection.removeRange(range);
    });

    if (container.classList.contains("highlight")) {
      container.appendChild(copybutton);
    } else if (container.parentNode.firstChild == container) {
      
    } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
      
      codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
    } else {
      
      codeblock.parentNode.appendChild(copybutton);
    }
  });
</script>




<script>
  
  
  (function() {
    const enableTocScroll = '1' == '1'
    if (!enableTocScroll) {
      return
    }
    if (!document.querySelector('.toc')) {
      console.log('no toc found, ignore toc scroll')
      return
    }
    

    
    const scrollListeners = window.scrollListeners
    const headings = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id]');
    const activeClass = 'active';

    
    let activeHeading = headings[0];
    getLinkByHeading(activeHeading).classList.add(activeClass);

    const onScroll = () => {
      const passedHeadings = [];
      for (const h of headings) {
        
        if (getOffsetTop(h) < 5) {
          passedHeadings.push(h)
        } else {
          break;
        }
      }
      if (passedHeadings.length > 0) {
        newActiveHeading = passedHeadings[passedHeadings.length - 1];
      } else {
        newActiveHeading = headings[0];
      }
      if (activeHeading != newActiveHeading) {
        getLinkByHeading(activeHeading).classList.remove(activeClass);
        activeHeading = newActiveHeading;
        getLinkByHeading(activeHeading).classList.add(activeClass);
      }
    }

    let timer = null;
    const scrollListener = () => {
      if (timer !== null) {
        clearTimeout(timer)
      }
      timer = setTimeout(onScroll, 50)
    }
    window.addEventListener('scroll', scrollListener, false);
    scrollListeners.push(scrollListener)

    function getLinkByHeading(heading) {
      const id = encodeURI(heading.getAttribute('id')).toLowerCase();
      return document.querySelector(`.toc ul li a[href="#${id}"]`);
    }

    function getOffsetTop(heading) {
      if (!heading.getClientRects().length) {
        return 0;
      }
      let rect = heading.getBoundingClientRect();
      return rect.top
    }
  })();
  </script>

</body>

</html>
