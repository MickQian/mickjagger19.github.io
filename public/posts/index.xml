<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Mick&#39; Blog</title>
    <link>https://mickjagger19.github.io/posts/</link>
    <description>Recent content in Posts on Mick&#39; Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sat, 20 Jul 2024 20:16:13 +0800</lastBuildDate><atom:link href="https://mickjagger19.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>My Gears</title>
      <link>https://mickjagger19.github.io/posts/music/my-gears/</link>
      <pubDate>Sat, 20 Jul 2024 19:53:50 +0800</pubDate>
      
      <guid>https://mickjagger19.github.io/posts/music/my-gears/</guid>
      <description>A (brief) introduction of my gears</description>
      <content:encoded><![CDATA[<h2 id="rocky">Rocky</h2>
<p><a href="https://www.fender.com/en-US/george-harrison-rocky-stratocaster.html"><strong>Rocky</strong></a> 是 The Beatles 吉他手 <strong>George Harrison</strong> 的电吉他</p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/BmU4eSv0zXM?si=kmdp2USeukhhFwSr" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
<p>George Harrison 本人曾在多个场合使用它：</p>
<p>















  
  
      
      
  <figure align=center class="figure d-block text-center">
  <picture align=center >
  <img class="figure-img img-fluid" src="https://mickjagger19.github.io/Attachments/Music/My%20Gears/IMG-20240720203521507_scaled.jpg?v=94562a842ccb99020a904374509ca22e#center" alt="George Harrison Performing With Rocky in 《Im a Walrus》" loading="lazy" height="380px" width="480px" />
</picture>

  <figcaption class="figure-caption"><p>George Harrison Performing With Rocky in 《Im a Walrus》</p></figcaption>
</figure>
</p>
<p>















  
  
      
      
  <figure align=center class="figure d-block text-center">
  <picture align=center >
  <img class="figure-img img-fluid" src="https://mickjagger19.github.io/Attachments/Music/My%20Gears/IMG-20240720205140321.png?v=94562a842ccb99020a904374509ca22e#center" alt="George Harrison With Rocky" loading="lazy" height="650px" width="960px" />
</picture>

  <figcaption class="figure-caption"><p>George Harrison With Rocky</p></figcaption>
</figure>
</p>
<p>















  
  
      
      
  <figure align=center class="figure d-block text-center">
  <picture align=center >
  <img class="figure-img img-fluid" src="https://mickjagger19.github.io/Attachments/Music/My%20Gears/IMG-20240720205008915.png?v=94562a842ccb99020a904374509ca22e#center" alt="George Harrison Playing Rock in Abbey Road Studio" loading="lazy" height="1128px" width="910px" />
</picture>

  <figcaption class="figure-caption"><p>George Harrison Playing Rock in Abbey Road Studio</p></figcaption>
</figure>
</p>
<p>有关它的来源：</p>
<blockquote>
<p>Stratocaster® guitars were almost impossible to find in England in the late 1950s and early 1960s, so when George Harrison actually found one in a shop during the pre-fame early days of the Beatles, he meant to get it but was scooped by the guitarist for Rory Storm and the Hurricanes (whose drummer went by the stage name Ringo Starr).</p>
<p>A few dizzying years later, when the sessions for Beatles <em>Help!</em> album got under way in early 1965, Harrison had better luck - he and John Lennon sent roadie Mal Evans out to get one for each of them, and Evans soon returned with a matching pair of Sonic Blue Strat® guitars. Harrison&rsquo;s guitar, serial number 83840, still bore a decal from a music store where it was purchased at one point - &ldquo;Grimwoods; The music people; Maidstone and Whitstable&rdquo;. Thus, <em>Help!</em> marks the first appearance of a Stratocaster in Beatles music; heard in the low drone throughout that album&rsquo;s &ldquo;Ticket to Ride&rdquo; and in the solo for &ldquo;You&rsquo;re Going to Lose That Girl.&rdquo; Near the end of 1965, both Strats were put to even more prominent use on groundbreaking album <em>Rubber Soul</em>, most notably on the ringing chordal solo in &ldquo;Nowhere Man&rdquo;, and again on mid-1966&rsquo;s <em>Revolver</em>.</p>
<p>In 1967, sometime between the end of the <em>Sgt. Pepper&rsquo;s Lonely Hearts Club Band</em> sessions and the June 25 live worldwide telecast of &ldquo;All You Need is Love&rdquo;, Harrison took up paint and brush himself to give his Stratocaster a multicolored psychedelic dayglo paint job. It also appeared prominently in the &ldquo;I Am the Walrus&rdquo; segment of 1967&rsquo;s <em>Magical Mystery Tour</em> film. The guitar remained a favorite of Harrison&rsquo;s for the rest of the decade, and by December 1969 Harrison had painted &ldquo;Bebopalula&rdquo; on the upper body, &ldquo;Go Cat Go&rdquo; on the pickguard and &ldquo;Rocky&rdquo; - the guitar&rsquo;s nickname - on the headstock.</p>
<p>&mdash; 摘取自 <a href="https://www.fendercustomshop.com/series/limited-edition/limited-edition-george-harrison-rocky-strat/"># LIMITED EDITION GEORGE HARRISON ROCKY STRAT®</a></p>
</blockquote>
<p>其中提到，Rocky 是由一把 61&rsquo; Stratocaster, 并且是由 George Harrison 本人改造而来。</p>
<p>而关于改装的细节（主要关于颜料）：</p>
<blockquote>
<p>“During ’67, everybody started painting everything,” Harrison says, “and I decided to paint it. I got some <strong>Day-Glo</strong> paint, which was quite a new invention in them days, and just sat up late one night and did it.” (Harrison points out that some of his ex-wife Patti Boyd’s nail polish was used to paint the headstock.)</p>
<p>The guitar made appearances that year in the Beatles’ live performance of “All You Need Is Love” on Our World, the first global satellite TV program, and in the film <em>Magical Mystery Tour</em>, in the segment where the Beatles mime to “I Am the Walrus,”</p>
</blockquote>
<iframe width="560" height="315" src="https://www.youtube.com/embed/O7_XuXP0AaU?si=Jq-AZPG4fAT9VNdT" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
<p>Rocky 的主要颜料是 Day-Glo (<em>Day-Glo Color Corp</em>. 生产的一种荧光涂料), 而琴颈部位则是用他当时妻子 <strong>Patti Boyd</strong> 的指甲油涂绘而成</p>
<p>我本人非常喜欢 Rocky 的配色，奈何负担不起 Rocky Custom Shop 的高昂费用，也由于墨产 Rocky Player Series 的 低性价比而未选择入手</p>
<p>恰巧我有一把 <a href="https://www.fender.com/en-US/electric-guitars/stratocaster/vintera-ii-60s-stratocaster/0149020302.html">Fender Vintera 60&rsquo;s Stratocaster</a>, 从年代和琴型上都与原版相对接近。于是我把它送到一位网友那里，完成 Rocky 的复刻</p>
<p>在改造前，它是一把日落色的，大概这样：</p>
<p>















  
  
      
      
  <figure align=center class="figure d-block text-center">
  <picture align=center >
  <img class="figure-img img-fluid" src="https://mickjagger19.github.io/Attachments/Music/My%20Gears/IMG-20240720210023358.jpg?v=94562a842ccb99020a904374509ca22e" alt="日落色" loading="lazy" height="776px" width="2400px" />
</picture>

  <figcaption class="figure-caption"><p>日落色</p></figcaption>
</figure>
</p>
<p>在使用 硝基漆 和一些荧光涂料改造之后：</p>
<p>















  
  
      
      
  <picture align=center  class="d-block text-center">
  <img class="img-fluid" src="https://mickjagger19.github.io/Attachments/Music/My%20Gears/IMG-20240720210455390.jpg?v=e2a12d9e97e559021052cf6953ba4839" alt="" loading="lazy" height="300px" width="400px" />
</picture>
</p>
<p>















  
  
      
      
  <picture align=center  class="d-block text-center">
  <img class="img-fluid" src="https://mickjagger19.github.io/Attachments/Music/My%20Gears/IMG-20240720210308000.jpg?v=94562a842ccb99020a904374509ca22e" alt="" loading="lazy" height="3024px" width="4032px" />
</picture>
</p>
<p>喷绘的效果还是挺令我满意的</p>
<p>我带着它参加了一些小型演出</p>
<h2 id="fender-blues-junior-tweed-deluxe">Fender Blues Junior Tweed Deluxe</h2>
]]></content:encoded>
    </item>
    
    <item>
      <title>DiT, Sora, and more</title>
      <link>https://mickjagger19.github.io/posts/ai/models/dit-sora-and-more/</link>
      <pubDate>Mon, 19 Feb 2024 16:23:58 +0800</pubDate>
      
      <guid>https://mickjagger19.github.io/posts/ai/models/dit-sora-and-more/</guid>
      <description>Review on DiT and sora</description>
      <content:encoded><![CDATA[<blockquote>
<p>[! WARNING]
This work is in progress</p>
</blockquote>
<table>
<thead>
<tr>
<th style="text-align:left">Notations</th>
<th style="text-align:left">Meaning</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Classifier-guided Diffusion</td>
<td style="text-align:left">An implementation of Conditional Diffusion Models, which requires a classifier $\nabla_{x}p(x|y)$ to guide its reverse process</td>
<td></td>
</tr>
<tr>
<td style="text-align:left">Classifier-free Diffusion</td>
<td style="text-align:left">An implementation of Conditional Diffusion Models, which doesn&rsquo;t require a <strong>classfier</strong>, the condition serves as an input to its <strong>noise predictor</strong></td>
<td></td>
</tr>
<tr>
<td style="text-align:left">Latent Diffusion Models(LDM)</td>
<td style="text-align:left">A type of Diffusion Models where diffusion processes are done in <strong>latent space</strong></td>
<td></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
<td></td>
</tr>
</tbody>
</table>
<h1 id="introduction">Introduction</h1>
<p>Diffusion models usually choose a <strong>UNet</strong> as its backbone for Noise Predictor, first adopted by Ho et al [^1] , which was inherited from <strong>Pixel-CNN++</strong>(widely used as the generator in VAE) with a few changes. Although some works have introduced attention blocks into low-level design, its high-level remains intact.</p>
<p><strong>DiT</strong> is proposed to apply Transformer into Diffusion Models, adhering to the best practices of <strong>Vision Transformers(ViTs)</strong></p>
<p>Also, the scaling behavios of transformers is also explored in DiT</p>
<h1 id="dit-design-space">DiT Design Space</h1>
<table>
<thead>
<tr>
<th style="text-align:left">Notations</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">$C$</td>
<td style="text-align:left">the channels of input image</td>
</tr>
<tr>
<td style="text-align:left">$I$</td>
<td style="text-align:left">the dimension of $z$</td>
</tr>
<tr>
<td style="text-align:left">$z \in R^{I \times I \times C}$</td>
<td style="text-align:left">The latents, also the input of $DiT$</td>
</tr>
<tr>
<td style="text-align:left">Patch</td>
<td style="text-align:left">The unit of input.</td>
</tr>
<tr>
<td style="text-align:left">$p$</td>
<td style="text-align:left">the dimension of a single patch</td>
</tr>
<tr>
<td style="text-align:left">Classifier-free Diffusion</td>
<td style="text-align:left">An implementation of Conditional Diffusion Models, which doesn&rsquo;t require a <strong>classfier</strong>, the condition serves as an input to its <strong>noise predictor</strong></td>
</tr>
<tr>
<td style="text-align:left">Latent Diffusion Models(LDM)</td>
<td style="text-align:left">A type of Diffusion Models where diffusion processes are done in <strong>latent space</strong></td>
</tr>
</tbody>
</table>
<h2 id="1-patchify">1. Patchify</h2>
<p><strong>Patchify</strong> is the first layer of DiT, which converts the spatial input $z$ into a sequence to $T$ tokens, each of dimension $d$:
$$
z \in R^{I \times I \times C} \to T \cdot Token
$$
where:</p>
<ul>
<li>$Token \in R^{p \times p \times C}$</li>
<li>$T = (\frac{I}{p})^2$</li>
</ul>
<h2 id="2-positional-embeddings">2. Positional Embeddings</h2>
<p>Following patchify, we apply standard ViT <strong>frequency-based positional embeddings</strong> (the sine-cosine version) to all input tokens: $Token \to Patch$</p>
<h2 id="3-transformer-blocks">3. Transformer Blocks</h2>
<p>Following patchify, the input tokens are processed by a sequence of transformer blocks.</p>
<h3 id="in-context-conditioning">In-context conditioning</h3>
<p>In addition to noised image inputs, diffusion models sometimes process additional conditional information:</p>
<ul>
<li>$t$: noise timestamps (of DDPM)</li>
<li>$c$: class labels c(of input images)</li>
<li>natural language description(or caption)</li>
</ul>
<p>The conditions are represented as additional $Token$s, <strong>appened</strong> to the [input sequence](## 2. Positional Embeddings)</p>
<h3 id="cross-attention-block">Cross-attention block</h3>
<p>The conditional tokens are send to the cross-attention block of transformer block</p>
<h3 id="adaptie-layer-normadaln-block">Adaptie Layer Norm(adaLN) Block</h3>
<p>The <strong>Adaptive Layer Norm</strong> replaces the standard layer norm</p>
<blockquote>
<p>[! NOTE]
To be completed</p>
</blockquote>
<h2 id="4-transformer-decoder">4. Transformer Decoder</h2>
<p>After the transformer blocks, a <strong>transformer decoder</strong> is reponsible for decoding the each latent token back to tensor of size $p \times p \times C$.</p>
<p>The decoder is simply a <strong>standrad linear layer</strong></p>
<h1 id="sora">Sora</h1>
<p>The following is some major takeaways of <a href="https://openai.com/research/video-generation-models-as-world-simulators">the tenichal report of Sora</a></p>
<p>Sora is a diffusion model/diffusion transformer based on <strong>DiT</strong></p>
<h2 id="unified-representation-of-visual-data">Unified Representation of Visual Data</h2>
<p>The major part of the technial report is about the <strong>Unified Representation</strong>, which is the training data of Sora</p>
<h3 id="sources">Sources</h3>
<blockquote>
<p>We take inspiration from large language models which acquire generalist capabilities by training on internet-scale data</p>
</blockquote>
<p>Based on the description, Sora might crawl a huge amount of data from internet</p>
<h3 id="patch">Patch</h3>
<p>Following the text token concept of LLM, and patch concept from [DiT](##1. Patchify), Sora has visual <em>patches</em>.</p>
<p>The transformation of videos into patches went through 2 steps:</p>
<ol>
<li>Compress videos into lower-dimensional latents</li>
<li>Decompose the latents into <strong>spacetime</strong> patches</li>
</ol>
<blockquote>
<p>[! TIP]
<strong>spacetime</strong> implies that the tokens are embedded with timestep information</p>
</blockquote>
<p>While the original DiT tokens are fixed-sized based on the size of the latents, the <em>spacetime</em> patches used by Sora is derived from videos/images of <strong>variable resolutions, durations and aspect ratios</strong>. This gives huge flexibility in inference time, since the output is formed with any patches you like.</p>
<h2 id="scaling-transformers">Scaling Transformers</h2>
<p>They find transformers scaled effectively as video models, same as in other domains, including language modeling,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-13">13</a>,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-14">14</a> computer vision,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-15">15</a>,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-16">16</a>,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-17">17</a>,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-18">18</a> and image generation.<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-27">27</a>,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-28">28</a>,<a href="https://openai.com/research/video-generation-models-as-world-simulators#fn-29">29</a></p>
<h2 id="data-preprocessing">Data preprocessing</h2>
<h3 id="native-size">Native size</h3>
<p>Different from prior approaches, which crops videos/images to standard size(e.g., 4 seconds videos at 256 * 256 resolution), they find that training on data at its <strong>native size</strong> benefits.</p>
<h3 id="native-aspect-ratios">Native aspect ratios</h3>
<p>They empirically find that training on videos at their native aspect ratios improves composition and framing. The model trained on square crops sometimes generates videos where the subject is only <strong>partially</strong> in view.</p>
<h2 id="language-understanding">Language understanding</h2>
<p>The language-understanding-ability is a crucial part of text-to-video models, as text is the major input</p>
<h3 id="re-captioning">Re-captioning</h3>
<p><strong>Re-captioning</strong> is a technique to generate descriptive captions for images/videos with the help of a <strong>highly descriptive captioner model</strong></p>
<p>First introduced in the training of DALL$\cdot$E, it is used in Sora, where GPT serves the role of <strong>captioner</strong>, who <em>turn short user prompts into longer detailed captions</em></p>
<h2 id="prompting-with-images-and-videos">Prompting with images and videos</h2>
<p>Being abled to be prompted with inputs other than text, including images and videos, Sora can perform a wide range of image and video editing tasks.</p>
<h3 id="some-applications-not-mentioned-in-the-report">Some applications not mentioned in the report</h3>
<p>Based on the presented applications, Sora might be able to do the tasks of:</p>
<ul>
<li>video generation conditioned on text/image: generate videos based on the given future/previous text and image</li>
<li></li>
</ul>
<blockquote>
<p> Model and implementation details are not included in this report.
 &ndash; video-generation-models-as-world-simulators</p>
</blockquote>
]]></content:encoded>
    </item>
    
    <item>
      <title>Mamba</title>
      <link>https://mickjagger19.github.io/posts/ai/models/mamba/</link>
      <pubDate>Thu, 08 Feb 2024 14:41:55 +0800</pubDate>
      
      <guid>https://mickjagger19.github.io/posts/ai/models/mamba/</guid>
      <description>Brief introduction to Mamba</description>
      <content:encoded><![CDATA[<hr>
<p>title: Mamba
summary: Mamba
tags:</p>
<ul>
<li>SSM</li>
<li>Mamba</li>
<li>RNN
author: Mick
draft: false
date: 2024-01-30T12:38:39+0800
math: true</li>
</ul>
<hr>
<h2 id="notations">Notations</h2>
<table>
<thead>
<tr>
<th style="text-align:left">Notations</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">SSM</td>
<td style="text-align:left">A math model, used to describe and analyze the behavior of dynamic system. In DL, it helps to process <strong>sequence data</strong>, by projecting the data into latent space. <strong>State Equation</strong> and <strong>Observation Equation</strong> are two important components of a SSM</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
</tr>
</tbody>
</table>
<h2 id="background">Background</h2>
<p><a href="AI/Models/The%20Transformer%20Family.md">The Transformer Family</a> is  notorious for $\mathcal{O}(N^{2})$ time complexity when inferencing, but many of the model targeting this problem has less performance[[ ]]than Transformer.</p>
<h2 id="state-space-models">State Space Models</h2>
<p><strong>SSMs(Structured State Space Sequence Models, S4)</strong> is defined by four <strong>invariant</strong> parameters: $(\Delta, A, B, C)$ and a sequence-to-sequence transformation consisting of 2 phases:
$$</p>
<p>$$</p>
<h3 id="linear-systems">Linear Systems</h3>
<h4 id="1-discretization">1. Discretization</h4>
<p>$$
\begin{align}
(\Delta, A, B) &amp;\to (\bar A, \bar B)\
f_{A}(\Delta, A) &amp;= \bar A \
f_{B}(\Delta, A, B) &amp;= \bar B
\end{align}
$$</p>
<p>where $(f_{A}, f_{B})$ is called <strong>discretization rules</strong>, where there are many candidates.</p>
<p><strong>Discretization</strong> has deep connections with <strong>continuous time system</strong>, giving it additional properties:</p>
<ul>
<li>resolution invariant</li>
<li>auto normalization</li>
</ul>
<blockquote>
<p>[!NOTE]
Discretization is necessary for calculation efficiency</p>
</blockquote>
<h4 id="2-computation">2. Computation</h4>
<p>There are two ways to compute</p>
<table>
<thead>
<tr>
<th style="text-align:left">Notations</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">$A(\cdot )$</td>
<td style="text-align:left">State matrix</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
</tr>
</tbody>
</table>
<h4 id="linear-time-invariance-system">Linear Time Invariance System</h4>
<p>The parameters calculated in <a href="/posts/ai/models/mamba/#discretization">Discretization</a> is fixed in all timestampes, this property is called <strong>LTI</strong>.</p>
<p>LTI fails from several reasons:</p>
<ul>
<li><strong>invariant dynamics</strong> can&rsquo;t select correct information from the context, nor can it change the hidden state with input</li>
<li>global convolution requires only time-awareness, but lacks content-awareness</li>
</ul>
<h2 id="mamba">Mamba</h2>
<h3 id="motive-selection-as-a-compression">Motive: Selection as a compression</h3>
<p>A basic model of sequence modeling: <em>Compress the context into a smaller state</em>.</p>
<p>Interpreting popular sequence models from this view:</p>
<ul>
<li>Attention: No explicit compression, but stores all the context in KV-cache</li>
<li>Recurrent Model: Limited state, linear time training</li>
</ul>
<p>Strucuted SSMs maps <strong>each channel</strong> of input $x$ to output $y$ with high-dimensional implicit state $h$</p>
<p>Prior work of SSms avoids the massive calculation of $DN$ with iterative computation path, with relies on time-invariant.</p>
<h3 id="selection-mechainism">Selection Mechainism</h3>
<h2 id="notations-1">Notations</h2>
<table>
<thead>
<tr>
<th style="text-align:left">Notations</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Input-dependent dynamics</td>
<td style="text-align:left">A dynamic enabling sequence model to depend on input</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
</tr>
</tbody>
</table>
<p><strong>Mamba</strong> promote a <strong>Selection Mechanism</strong>:  increase input-dependent dynamics, with requires a hardware-aware algorithm.</p>
<p>A way of blending selection mechanism into model, is to let the inputs affects the parameters of sequence co-effects, e.g. inserting into the recurrent dynamics of <strong>RNN</strong> or convolution kernel of <strong>CNN</strong>.</p>
<ul>
<li>B and C is changed, to make it selective</li>
<li>$\Delta_{t}$ controls the focusness/neglection to current input</li>
<li>$A$ is possibly selective, but for simplicity, it is not considered</li>
<li>variant gap filters out the irrelavant noise labels within inputs</li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>Tuning</title>
      <link>https://mickjagger19.github.io/posts/ai/peft/</link>
      <pubDate>Wed, 24 Jan 2024 19:19:32 +0800</pubDate>
      
      <guid>https://mickjagger19.github.io/posts/ai/peft/</guid>
      <description>PEFTs</description>
      <content:encoded><![CDATA[<h2 id="terminologies">Terminologies</h2>
<table>
<thead>
<tr>
<th style="text-align:left">Term</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Full fine-tuning</td>
<td style="text-align:left">Fine-Tune all the weights of a pretrained model</td>
</tr>
<tr>
<td style="text-align:left"><a href="https://en.wikipedia.org/wiki/Intrinsic_dimension">Intrinsic dimension</a></td>
<td style="text-align:left">An attribute of a dataset, essentially the minimum variable needed to encode the data</td>
</tr>
<tr>
<td style="text-align:left">low intrinsic dimension</td>
<td style="text-align:left">A description of a dataset, describing that the intrinsic dimension of the dataset is low</td>
</tr>
<tr>
<td style="text-align:left">$h$</td>
<td style="text-align:left">The output of the model</td>
</tr>
</tbody>
</table>
<h2 id="introduction">Introduction</h2>
<p><strong>PEFT</strong>(Parameter Efficient Fine-Tuning) is a technique to reduce the training cost of full fine-tuning by minimize the
parameter count and the computation complexity.</p>
<p>According to <a href="/posts/ai/peft/#unipelt">UniPELT</a>, existing PELT usually involves following variants:</p>
<ul>
<li>The functional form of $\Delta h$</li>
<li>The form of insertion into Transformer
<ul>
<li>Parallel: At <strong>input</strong> layer</li>
<li>Sequential: At <strong>output</strong> layer</li>
</ul>
</li>
<li>The representation modifies
<ul>
<li>attention layer</li>
<li>ffn llayer</li>
</ul>
</li>
<li>Composition function of $h$ and $\Delta h$</li>
</ul>
<h2 id="adapter-tuning">Adapter Tuning</h2>
<p>Only fine-tune the parameters of the layers close to downstream tasks.</p>
<p>While training, the parameter of the original pre-train model is frozen, with a newly-added adapter structure:</p>
<ol>
<li>Down-project layer: project the high-dim feature to lower dimension</li>
<li>Non-linear</li>
<li>Up-project layer: project back to high-dim</li>
<li>Skip-connection: $identity$ in the worst case</li>
</ol>
<h2 id="prefix-tuning">Prefix Tuning</h2>
<ul>
<li>Prefix: Prepend learnable task-related <strong>virtual tokens</strong> to input tokens at $W_{k} &amp; W_{v}$ of <strong>each layer</strong></li>
<li>An MLP after prefix layer(only in training): down-project a smaller prefix $P_{\theta}^{&rsquo;}$ to actual prefix$P_
{\theta}$, to stablize the training</li>
</ul>
<blockquote>
<p>[! NOTE]
Similar to <em>text prompt</em>, but <em>continuous</em> and <em>implicit</em></p>
</blockquote>
<h2 id="prompt-tuning">Prompt Tuning</h2>
<p>A simplified version of <a href="/posts/ai/peft/#prefix-tuning">Prefix Tuning</a>, with:</p>
<ul>
<li>Prefix virtual tokens prepended only at input layer</li>
<li>MLP removed.</li>
</ul>
<h2 id="p-tuning">P-Tuning</h2>
<p>Notice the problem of LLM: The expression of the prompt has a significant impact on downstream tasks</p>
<p><strong>P-Tuning</strong> is proposed to change the <em>input</em> Prompt to learnable embedding.</p>
<h2 id="lora">LoRA</h2>
<p>All of the PEFT methods mentioned above has some problems:</p>
<ul>
<li>either: increase the model depth and inference time, e.g.<a href="/posts/ai/peft/#adapter-tuning">Adapter Tuning</a></li>
<li>or: with learnable parameters which are hard to train</li>
</ul>
<p>It is observed that <strong>low intrinsic dimension</strong> is the key part of LLMs. Based on this observation, the attention matrix
can be re-designed as:
$$
h = \underbrace{W_{0}}<em>{\text{original weight}}x + \underbrace{\Delta W}</em>{\text{Adapte}}x = W_{0}x + BAx
$$
where:</p>
<ul>
<li>$A \in \mathbb{R}^{d \times r} \sim \mathcal{N}(0, \sigma^{2})$</li>
<li>$B \in \mathbb{R}^{r \times d}$</li>
<li>$d &gt; r$</li>
</ul>
<p>Advantages being:</p>
<ul>
<li>No additional depth introduced</li>
</ul>
<h2 id="unipelt">UniPELT</h2>
<p><strong>UniPELT</strong> provides a unified view of existing PEFTs, and compares each choices of variants:</p>
<ul>
<li>Parallel insertion form is bettern than Sequantial</li>
<li>Modified representation:
<ul>
<li>When the amout of parameter modified is huge, ffn is better
<ul>
<li>ffn is task-related</li>
</ul>
</li>
<li>Otherwise Attention
<ul>
<li>attention captures the text pattern</li>
</ul>
</li>
</ul>
</li>
<li>Scaling composition function is better</li>
</ul>
]]></content:encoded>
    </item>
    
    <item>
      <title>The Attention Family</title>
      <link>https://mickjagger19.github.io/posts/ai/models/the-attention-mechanism/</link>
      <pubDate>Sun, 21 Jan 2024 12:47:01 +0800</pubDate>
      
      <guid>https://mickjagger19.github.io/posts/ai/models/the-attention-mechanism/</guid>
      <description>Attention</description>
      <content:encoded><![CDATA[<h2 id="terminologies">Terminologies</h2>
<table>
<thead>
<tr>
<th style="text-align:left">Term</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><a href="https://en.wikipedia.org/wiki/Receptive_field">Receptive field</a>(a.k.a. sensory space)</td>
<td style="text-align:left">A concept originally from biology, adopted in modern artificial deep neural networks (especially <strong>CNN</strong>), describing the size of input image which can affects the output of <strong>neurons</strong></td>
</tr>
<tr>
<td style="text-align:left">$d_{model}$</td>
<td style="text-align:left">dimension of the word embedding(usually 512 = 64 * 8)</td>
</tr>
<tr>
<td style="text-align:left">$d_{k}$</td>
<td style="text-align:left">dimension of $w_q, w_k$</td>
</tr>
<tr>
<td style="text-align:left">$d_{v}$</td>
<td style="text-align:left">dimension of $w_v$</td>
</tr>
<tr>
<td style="text-align:left">$w_{q} \in \mathbb{R}^{d_{model} \times d_{k}}$</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">$w_k \in \mathbb{R}^{d_{model} \times d_{k}}$</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">$w_{v} \in \mathbb{R}^{d_{model} \times d_{v}}$</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">$B$</td>
<td style="text-align:left">Batch size</td>
</tr>
<tr>
<td style="text-align:left">$S$</td>
<td style="text-align:left">Sequence Length</td>
</tr>
<tr>
<td style="text-align:left">$X$</td>
<td style="text-align:left">input</td>
</tr>
<tr>
<td style="text-align:left">Structural Prior</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">translation equivariance</td>
<td style="text-align:left">An attribute of model, the ability of model to recognize objects does not varies with the <strong>geometric</strong> transformations of the input(shift, rotate, projection, etc)</td>
</tr>
</tbody>
</table>
<h2 id="the-attention-of-human">The Attention of human</h2>
<p>Selective attention is a mechanism unique to human vision. By swiftly scanning the image, human acquires important areas(a.k.a. focus). After this, human pays more attention to these areas, as there are more valuable information.</p>
<h2 id="self-attention">Self-Attention</h2>
<p>Self <em>Scaled-Dot</em> Attention:</p>
<p>$$
\begin{align}
X &amp;= X_{text_encoding}+ X_{positional_embedding}\
Q &amp;= X \cdot w_{q} \in \mathbb{R}^{S \times d_{k}}\
K &amp;= X \cdot w_{k} \in \mathbb{R}^{S \times d_{k}}\
V &amp;= X \cdot w_{v} \in \mathbb{R}^{S \times d_{v}}\
a &amp;= softmax\left(\frac{Q \cdot K^{\top}}{d_{k}}\right)\in \mathbb{R}^{S \times S}\
Attention &amp;= a V\in \mathbb{R}^{S \times d_{k}}\
\end{align}
$$</p>
<blockquote>
<p>[!TIP]</p>
<ul>
<li>Attention is the weighted (attention score) v of each token over other tokens</li>
</ul>
</blockquote>
<p>The major different between attention and typical <strong>RNN</strong> is: The generation of next token doesn&rsquo;t rely on hidden state from previous timestamp, but instead alter the embedding of the token directly with positional embedding.</p>
<blockquote>
<p>[!TIP]
Some researchers seems Attention as a kind of soft addressing</p>
</blockquote>
<h2 id="cross-attention">Cross-Attention</h2>
<p>Different from <a href="/posts/ai/models/the-attention-mechanism/#self-attention">Self-Attention</a>, the $Q, K$ of cross-attention comes from another sequence($X_{2}$):
$$
\begin{align}
Q &amp;= X_{1} \cdot w_{q} \in \mathbb{R}^{n \times d_{k}}\
K &amp;= X_{2} \cdot w_{k} \in \mathbb{R}^{m \times d_{k}}\
V &amp;= X_{2} \cdot w_{v} \in \mathbb{R}^{m \times d_{v}}\
a &amp;= softmax\left(\frac{Q \cdot K^{\top}}{d_{k}}\right)\in \mathbb{R}^{m \times n}\
\end{align}
$$</p>
<p>Thus, the attention matrix $a$ represents the attention between $X_{1}$ and $X_{2}$</p>
<h2 id="features">Features</h2>
<h3 id="multi-head">Multi-head</h3>
<p>$$
MultiHeadAttention(Q, K, V) = Concat(Attention_{i})W^{O}, i \in (0, h)
$$</p>
<ul>
<li>let each head focus on one part of input, concatenating and increasing the <strong>receptive field</strong> of the NN</li>
<li><strong>Grammar &amp; Context &amp; Rare words</strong> are what heads are focusing on</li>
<li>Compared to multi-layer attention, it can be trained parallelly</li>
<li></li>
</ul>
<h3 id="parallelism">Parallelism</h3>
<p>$$
Attention_{0:t, 0:t} =concat(Attention_{0:t-1, 0:t-1}, Attention_{0:t-1, t})
$$</p>
<p>making it possible to train parallelly.</p>
<h3 id="scaling-with-d_k">Scaling with $d_{k}$</h3>
<ul>
<li>without scaling, Softmax can easily causes gradient vanishing</li>
<li>$\sqrt{d_{k}}$: the variance of $q \cdot k \to d_{k}$, to let the variance close to 1:
$Var(A \cdot B) = Var\left(\sum\limits{A_{ij}B_{ji}}\right)=\sum\limits Var(A_{ij}B_{ji}) = d_{k}^{2}$</li>
</ul>
<h3 id="production--multiplication">Production &amp; Multiplication</h3>
<ul>
<li>production: increase representation ability?</li>
<li>multiplication: faster. performance increase along with $d_{k}$</li>
</ul>
<h3 id="layer-norm--batch-norm">Layer Norm &amp; Batch Norm</h3>
<ul>
<li>LN: Apply normalization to <strong>a whole batch</strong>.</li>
<li>BN: Apply normalization to <strong>one position</strong> across different batches</li>
</ul>
<p>BN is often applied by <strong>CNN</strong></p>
<p>For sequences with <strong>different lengths</strong>, same feature across sequences is irrelevant(BN is not designed to deal with variant-length sequences), so NLP prefers normalization within a sequence : LN.</p>
<h3 id="parallelism-1">Parallelism</h3>
<ul>
<li>Encoder: sequential</li>
<li>Decoder: only in training, using sequence mask(predicting next tokens of different sequences at the same time)</li>
</ul>
<h3 id="long-distance-dependency">Long-Distance Dependency</h3>
<p>Self-Attention can capture the co-dependent features in long-distance, since it avoids accumulating &amp; calculating hidden states for several timestamps.</p>
<h2 id="fundamental-ideas">Fundamental Ideas</h2>
<p>$Q, K ,V$ is essentially a database with global semantic relationship.</p>
<h2 id="sparse-attention">Sparse Attention</h2>
<p>In standard attention mechanism, the attention between tokens are pair-wise</p>
<p>However, it is observed that most of the time, the attention matrix $A$ is <em>sparse</em></p>
<h3 id="sparse-attention-1">Sparse Attention</h3>
<table>
<thead>
<tr>
<th style="text-align:left">Notation</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Attention kernel</td>
<td style="text-align:left">The tokens required for the attention to predict next token</td>
</tr>
</tbody>
</table>
<p>OpenAI reduces the time complexity by &ldquo;keep the value in small region, enforcing most elements as zero&rdquo;</p>
<p>It is observed that attention has gained inductive bias similar in CNN:</p>
<ul>
<li>shallow layers: patterns in local connection</li>
<li>deep layers: global patterns</li>
</ul>
<p>To introduce the sparse feature of CNN into attention, they introduce a concept: <strong>Connectivity Pattern</strong>:
$$
S = {S_{1}, &hellip; , S_{n}}
$$</p>
<p>where $S_{i}$ is the indices at timestamp i.</p>
<p>And attention is transformed to:</p>
<p>$$
\begin{align}
a(x_{i}, S_{i}) = softmax\left(\frac{(W_{q}x_{i})K_{S_{i}}^{\top}}{\sqrt{d}}\right)V_{S_{i}}\
\end{align}
$$</p>
<ul>
<li>decompose the full attention with sparse attentions</li>
<li></li>
</ul>
<p>However:</p>
<ul>
<li>the kept attention region is decided by human, not dynamic</li>
</ul>
<h3 id="position-based-sparse-attention">Position-based Sparse Attention</h3>
<h4 id="atomic-sparse-attention">Atomic Sparse Attention</h4>
<p>Single-form connection of attention units</p>
<ol>
<li>Global Attention: adding some global nodes as the center of information broadcast</li>
<li>Band Attention(a.k.a sliding window attention, or local attention): Due to the strong locality of data, limit the query of its neighboring nodes</li>
<li>Dilated Attention: Increase inductive field by using expanding window with a hole</li>
<li>Random Attention: 为了增加非局部交互的能力，每个查询随机采样一些边缘。这是基于观察<strong>随机图</strong>可以与完整图具有相似的光谱属性</li>
<li>Block Local Attention: Split the input sequence into query blocks, each block is associated with a local memory block. A query block would only focus on the <em>key</em> from its memory block</li>
</ol>
<h4 id="compound-sparse-attention">Compound Sparse Attention</h4>
<p>Combination of the atomic attentions mentioned above</p>
<h4 id="extended-sparse-attention">Extended Sparse Attention</h4>
<p>Design special sparse structure for specific data</p>
<h3 id="content-based-sparse-attention">Content-based Sparse Attention</h3>
<p>Build sparse graph on input content</p>
<h2 id="linearized-attention">Linearized Attention</h2>
<p>Though being able to parallize, it has the complexity of $\mathcal{O}(n^{2})$ in both time and space.</p>
<p>Some work have achieved linear complexity in various way</p>
<blockquote>
<p>[!TIP]
The time complexity of multipling $\mathbb{R}^{a \times b} \cdot \mathbb{R}^{b \times c}$ is $\mathcal{O}(abc)$</p>
</blockquote>
<h3 id="remove-softmax">Remove Softmax</h3>
<p>The existence of Softmax enforce the calculation of $QK^{\top}$, which is the source of $\mathcal{O}(n^{2})$.</p>
<p>Removing the softmax, the attention became: $A = QK^{\top}V$:</p>
<ul>
<li>Calculate $K^{\top}V$: $\mathcal{O}(d^{2}n) \approx \mathcal{O}(n)$</li>
<li>Calculate $Q(K^{\top}V)$: $\mathcal{O}(nd^{2}) \approx \mathcal{O}(n)$</li>
</ul>
<h3 id="replace-softmax-with-sim">Replace Softmax with sim</h3>
<p>By rewriting the $e^{q_{i}^{\top}k_{j}}$ with normal similarity function:
$$
\begin{equation}
Attention(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V})<em>{i} = \frac{\sum\limits</em>{j=1}^{n} \text{sim}(\boldsymbol{q}<em>{i}, \boldsymbol{k}<em>j)\boldsymbol{v}</em>{j}}{\sum\limits</em>{j=1}^{n} \text{sim}(\boldsymbol{q}<em>{i}, \boldsymbol{k}</em>{j})}
\end{equation}
$$</p>
<ul>
<li>Adding <em>non-negative</em> activation function(kernal method) to $q, k$: $sim(q_{i}, k_{j}) = \phi(q_{i})\varphi(k_{j})^{\top}$
<ul>
<li><a href="https://arxiv.org/abs/2006.16236">《Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention》</a>: $\phi(x) = \varphi(x) = \text{elu}(x) + 1$</li>
</ul>
</li>
<li>Apply Softmax to $Q, K$ separately: $\begin{equation}Attention(\boldsymbol{Q},\boldsymbol{K},\boldsymbol{V}) = softmax_2\left(\boldsymbol{Q}\right)softmax_1(\boldsymbol{K})^{\top}\boldsymbol{V}\end{equation}$ , where $softmax_{i}$ means softmax in $i$-th dimension
<ul>
<li><a href="https://arxiv.org/abs/1812.01243">《Efficient Attention: Attention with Linear Complexities》</a></li>
</ul>
</li>
<li>Apply taylor expansion to $e^{\boldsymbol{q}_i^{\top}\boldsymbol{k}_j}$:  $\begin{equation}\text{sim}(\boldsymbol{q}_i, \boldsymbol{k}_j) = 1 + \left( \frac{\boldsymbol{q}_i}{\Vert \boldsymbol{q}_i\Vert}\right)^{\top}\left(\frac{\boldsymbol{k}_j}{\Vert \boldsymbol{k}_j\Vert}\right)\end{equation}$</li>
</ul>
<h3 id="reformer">Reformer</h3>
<p>Approximately find the maximum Attention quickly, by LSH(Locality Sensitive Hashing)</p>
<p>This inspires us to reduce time complexity by:</p>
<ul>
<li>introducing the sparse bias into attention mechanism</li>
<li>combine structural bias(deleting connections in some neurons)</li>
</ul>
<h3 id="linformer">Linformer</h3>
<p>Project $K, V$ with two matrixes before Attention:</p>
<p>$$
\begin{align}
E, F &amp;\in \mathbb{R}^{m \times n}\
Attention(Q, K,V) &amp;= softmax(Q(EK)^{\top})FV
\end{align}
$$</p>
<blockquote>
<p>[!TIP]
Linformer is sub-sampling the sequence, so it&rsquo;s nature to think of pooling</p>
</blockquote>
<p>by disentangle attention matrix with kernal feature map</p>
<h2 id="low-rank-self-attention">Low-rank Self-Attention</h2>
<p>it is observed that most of the time, the attention matrix $a$ is <em>low-rank</em></p>
<h3 id="low-rank-parameterization">Low-rank Parameterization</h3>
<p>Parameterize the attention matrix with simpler structure, as an <em>inductive bias</em></p>
<h3 id="low-rank-approximation">Low-rank Approximation</h3>
<p>Approximate attention matrix with a low-rank matrix</p>
<h2 id="attention-with-prior">Attention with Prior</h2>
<p>Replace standard attention with prior attention distribution</p>
<h3 id="prior-that-models-locality">Prior that Models locality</h3>
<h3 id="prior-from-lower-modules">Prior from Lower Modules</h3>
<p>Adopt prior(attention) from previous attention layer</p>
<h3 id="attention-with-prior-only">Attention with Prior Only</h3>
<p>Derive the attention only with prior, not from the pari-wise dependency of input sequence</p>
<h2 id="improved-multi-head">Improved multi-head</h2>
<h3 id="head-behavior-modeling">Head Behavior Modeling</h3>
<p>It is not guaranteed that <a href="/posts/ai/models/the-attention-mechanism/#multi-head">multi-head</a> can increase the inductive field, some works have tried:</p>
<ul>
<li>Increase the feature representation ability of each head</li>
<li>guide the interactions between each heads
to achieve taht.</li>
</ul>
<h3 id="multi-head-with-restricted-spans">Multi-head with restricted Spans</h3>
<p>It might be helpful to combine global heads and local heads, reasons being:</p>
<ul>
<li>Locality</li>
<li>Effenciency</li>
</ul>
<h3 id="multi-head-with-refined-aggregation">Multi-head with Refined Aggregation</h3>
<p>Aggregate the output of each head with more complexity, rather simple <em>concatenating</em> them.</p>
<h2 id="improved-ffn">Improved FFN</h2>
<h3 id="activation">Activation</h3>
<h3 id="position-wise-ffn">Position-wise FFN</h3>
<h2 id="variants">Variants</h2>
<h3 id="glu">GLU</h3>
<p>[GLU](Gated Linear Unit) replaces the FFN with:
$$
O = (U \odot V)W_{o}, U = \phi_{u}(XW_{u}), V = \phi_{v}(XW_{v})
$$
In comparison, FFN:
$$
O = \phi(XW_{u})W_{o}
$$</p>
<blockquote>
<p>[!NOTE]
GLU replaces first MLP with the dot-product of two <strong>MLPs</strong></p>
</blockquote>
<h3 id="flash-attention">Flash Attention</h3>
<h4 id="gau">GAU</h4>
<p>Simplified attention:
$$
A = \frac{1}{n}\text{relu}^{2}\left(\frac{Q(Z)K(Z)^{T}}{\sqrt{s}}\right)= \frac{1}{ns}\text{relu}^{2}(Q(Z)K(Z)^{T}), Z = \phi_{z}(XW_{z})
$$</p>
<ul>
<li>Move the activation before QK-step</li>
<li>Removes the V</li>
<li>replace softmax with relu</li>
</ul>
<p>With Time Complexity $O(n^2)$</p>
<h2 id="applications">Applications</h2>
<h3 id="nlp">NLP</h3>
<h3 id="computer-vision">Computer Vision</h3>
<h3 id="audio">Audio</h3>
<h3 id="multi-modal">Multi-modal</h3>
]]></content:encoded>
    </item>
    
    <item>
      <title>GAN &amp; Variants</title>
      <link>https://mickjagger19.github.io/posts/ai/models/the-gan-family/</link>
      <pubDate>Fri, 19 Jan 2024 20:27:05 +0800</pubDate>
      
      <guid>https://mickjagger19.github.io/posts/ai/models/the-gan-family/</guid>
      <description>Personal takeaways of GAN &amp;amp; its variants</description>
      <content:encoded><![CDATA[<h2 id="introduction">Introduction</h2>
<p>The mathematics in GAN and WGAN</p>
<h2 id="notations">Notations</h2>
<table>
<thead>
<tr>
<th>Notation</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>Jensen-Shannon Divergence</td>
<td>An improved, symmetric version of  $KL$ Divergence:<br>$$D_{JS}(p||q) = \frac{1}{2} D_{KL}\left(p | \frac{p+q}{2}\right)+  \frac{1}{2} D_{KL}\left(q | \frac{p+q}{2}\right)$$</td>
</tr>
</tbody>
</table>
<h2 id="gan">GAN</h2>
<table>
<thead>
<tr>
<th>Notation</th>
<th>Meaning</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>$p_{z}$</td>
<td>distribution of noise input $z$</td>
<td></td>
</tr>
<tr>
<td>$p_{g}$</td>
<td>distribution of generator output $x$</td>
<td></td>
</tr>
<tr>
<td>$p_{r}$</td>
<td>distribution of real sample $x$</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="arch">Arch</h3>
<p>The original GAN is an <strong>architecture</strong> (does not define the implementation details) consisting of two models:</p>
<ul>
<li>Discriminator $D$: Estimates the probability of <strong>a given sample comes from the real dataset</strong></li>
<li>Generator $G$: Generates synthetic samples with a noise variable input $z$ (for diversity)</li>
</ul>
<p>These two models compete against each other, forming an <strong>adversarial</strong> relationship.</p>
<h3 id="objective-design">Objective Design</h3>
<p>The objective function is defined as the combinations of performances of $D$ and $G$:</p>

$$
\mathcal{L}(D,G) = \underbrace{\mathbb{E}_{x \sim p_{r}(r)}[\log D(x)]}_{\text{for  D   to   identify  real  samples}} +  \underbrace{\mathbb{E}_{z \sim p_{z}(z)}[\log (1 - D(G(z)))]}_{\text{for  G  to  generate fake  samples}}
$$

<p>Using the same objective function, the direction of optimization varies:</p>
<ul>
<li>$D$: $D(x) \to 1, D(G(z)) \to 0$, so for $D$ it&rsquo;s <strong>maximize</strong></li>
<li>$G$: $p_{g} \to p_{r}$, since $D(p_{r}) \to 1$, so for $G$ it&rsquo;s <strong>minimize</strong></li>
</ul>
<blockquote>
<p>[!TIP] Other interpretations of objective function:</p>
<ul>
<li>The JS-Divergence of $p_{r}, p_{g}$: $G$ tries to minimize</li>
<li>Energy: $D$ tries to model energe-function $U(x)$, $G$ tries to generate sample $\hat{x}$ with (local) minimum energy $U(x)$</li>
</ul>
</blockquote>
<h3 id="optimal">Optimal</h3>
<p>We derive the optimal status for both models for xxx reasons.</p>
<h4 id="discriminator">Discriminator</h4>
<p>Rewrite objective function $\mathcal{L}(D,G)$:</p>

$$
\begin{align*}
\mathcal{L}(D,G) &= \mathbb{E}_{x \sim p_{r}}[\log D(x)] +  \mathbb{E}_{z \sim p_{z}}[\log (1 - D(G(z)))]\\\\
&= \mathbb{E}_{x \sim p_{r}}[\log D(x)] +  \mathbb{E}_{x \sim p_{g}}[\log (1 - D(x))]\\\\
 &= \int_{x}{p_{r}\log D(x) + p_{g}\log (1 - D(x))}& \text{actually L(D)}\\\\
\end{align*}
$$

<p>$$
\begin{align*}
&amp;\frac{d(L(D^{\ast}))}{ dD^{\ast}} = \frac{p_{r}}{D(x^{\ast})} + \frac{p_{g}}{1 - D(x^{\ast})} = 0\\
&amp;\Rightarrow D^{\ast}(x) = \frac{p_{r}}{p_{r} + p_{g}}\\
&amp;\Rightarrow D^{\ast}(x) = \frac{1}{2}
\end{align*}
$$</p>
<p>That is to say, when $G$ is trained to optimal, $D$ will output probability as 0.5</p>
<h4 id="generator">Generator</h4>
<p>$$
\mathcal{L}(G) = \mathcal{L}( D = D_{0}, G) = 2D_{JS}(p_{r}||p_{g}) - 2 \log 2
$$
So the target of training $G$ can be interpreted as increasing the distance between $p_{r}$ and generated distribution $p_{g}$:</p>

$$
\mathcal{L}( D, f^{\ast}) = \mathbb{E}_{x \sim p_{r}}[f(x) ] - \mathbb{E}_{z \sim p_{z}}[f(G(z))]
$$

<blockquote>
<p>[!TIP]
Almost in all GANs, for $G$, the objective can always be interpreted as (some kind of) difference between $p_{r}$ and $p_{g}$</p>
</blockquote>
<h3 id="problems-of-gan">Problems of GAN</h3>
<h4 id="hard-to-achieve-nash-equilibrium">Hard to achieve Nash Equilibrium</h4>
<p>Ideally, two models are trained simultaneously(or iteratively) to find a <a href="https://en.wikipedia.org/wiki/Nash_equilibrium"><strong>Nash Equilibrium</strong></a> to a two-player non-cooperative game. However, they update independently without coordination, so the convergence is not guaranteed.</p>
<p>















  
  
      
      
  <figure align=center class="figure d-block text-center">
  <picture align=center >
  <img class="figure-img img-fluid" src="https://mickjagger19.github.io/Attachments/AI/Models/The%20GAN%20Family/IMG-20240120210126755.png?v=94562a842ccb99020a904374509ca22e#center" alt="A simulation of $D$ and $G$ to update regardless of each other" loading="lazy" height="550px" width="1305px" />
</picture>

  <figcaption class="figure-caption"><p>A simulation of $D$ and $G$ to update regardless of each other</p></figcaption>
</figure>
</p>
<blockquote>
<p>[!NOTE]
Maybe the idea of <strong>advesarial</strong> and <strong>cooperative</strong>(for a better overall result) are fundamentally contradictory</p>
</blockquote>
<h4 id="vanishing-gradient">Vanishing gradient</h4>
<p>When $D$ does a great job but $G$ does not, $D(x) \to 1, x \in p_{g}$, which is a constant value, this will lead to objective function being a fixed constants, thus the $\ell \to 0$.</p>
<p>















  
  
      
      
  <figure align=center class="figure d-block text-center">
  <picture align=center >
  <img class="figure-img img-fluid" src="https://mickjagger19.github.io/Attachments/AI/Models/The%20GAN%20Family/IMG-20240120210126802.png?v=94562a842ccb99020a904374509ca22e" alt="The log_gradients of the generator decays quickly. Also, the better the $D$, the smaller the gradient" loading="lazy" height="666px" width="840px" />
</picture>

  <figcaption class="figure-caption"><p>The log_gradients of the generator decays quickly. Also, the better the $D$, the smaller the gradient</p></figcaption>
</figure>
</p>
<p>Hence, the ability of $D$ is faced with a dilemma: it can be neither too good or too gad.</p>
<h4 id="mode-collapse">Mode Collapse</h4>
<p>During training, the generator may collapse to a setting where it always produces same outputs. This is a common failure case for GANs, commonly referred to as <strong>Mode Collapse</strong>(or Mode Dropping).</p>
<blockquote>
<p>[!TIP]
The reason of mode collapse can be interpreted as: the generator finds a local optima where the loss-map is really sharp, leading to lack in diversity, due to the nature of KL-divergence</p>
</blockquote>
<h4 id="lack-of-a-reliable-evaluation-metric">Lack of a reliable evaluation metric</h4>
<p>The score given by $D$ does not provide an objective metric of the quality of image.</p>
<h2 id="sgan">SGAN</h2>
<table>
<thead>
<tr>
<th style="text-align:left">Notations</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">$\delta$ distribution: Dirac delta function</td>
<td style="text-align:left">A function, whose value is zero everywhere except at zero, and $\int \delta  = 1$</td>
</tr>
<tr>
<td style="text-align:left">$T$</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
</tr>
</tbody>
</table>
<p><strong>SGAN</strong> designs the Discriminator loss as:</p>
<p>$$
\begin{equation}\mathcal{D}<em>{SGAN}[p(x),q(x)]  =
\max</em>{T}, \frac{1}{2}\mathbb{E}<em>{x\sim p(x)}[\log \sigma(T(x))] + \frac{1}{2}\mathbb{E}</em>{x\sim q(x)}[\log (1 - \sigma(T(x)))] + \log 2</p>
<p>\end{equation}
$$
which is basically a <strong>dual form</strong> of JS-Divergence.</p>
<p>Consider a case, where $p(x)$ and $q(x)$ has no intervention:
$$
\begin{equation}p(x)=\delta(x-\alpha),q(x)=\delta(x-\beta)\end{equation}
$$
This changes loss function into:
$$
\begin{equation}\mathcal{D}[p(x),q(x)] = \max_T, \frac{1}{2}[\log \sigma(T(\alpha))] + \frac{1}{2}[\log (1 - \sigma(T(\beta)))] + \log 2\end{equation}
$$</p>
<p>And while training, $T(\alpha) \to +\infty, T(\beta) \to -\infty$</p>
<h2 id="wgan">WGAN</h2>
<table>
<thead>
<tr>
<th>Notation</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://en.wikipedia.org/wiki/Wasserstein_metric">Wasserstein Distance</a>(a.k.a. Earth Mover’s distance, short for EM distance) $\mathcal{W}$</td>
<td>A measure of the distance between two PD, as the minimum energy cost of moving and transforming two PDs for them to be the same, a.k.a. the <strong>optimal transport cost</strong>. <br>$$\mathcal{W}(p,q) = \inf_{\gamma \sim \prod(p,q)}{\mathbb{E}_{(x,y) \sim \gamma}[c(x,y)]}$$</td>
</tr>
<tr>
<td><a href="https://en.wikipedia.org/wiki/Lipschitz_continuity">Lipschitz continuity</a></td>
<td>A strong form of uniform continuity for functions. If $\vert f(x_{1} - f(x_{2})\vert \le K\vert x_{1} - x_{2}\vert$, f is called $K$-Lipschitz, formally $\vert f\vert _{L} \le K$</td>
</tr>
<tr>
<td>infimum $\inf$</td>
<td>the greast lower  bound, in most cases the minimum</td>
</tr>
<tr>
<td>supremum $\sup$</td>
<td>the least upper bound, in most cases the maximum</td>
</tr>
<tr>
<td>critic $f(.)$</td>
<td>the form of $D$ in WGAN. No longer outputting the realism (a probability) of sample, it learns a specific distance (or a score, of distributions) from samples. $f(x) = D(x)$</td>
</tr>
</tbody>
</table>
<blockquote>
<p>[!TIP]
W-Distance is better than JS and KL for providing a smooth measure of distributions, even when they are disjoint</p>
</blockquote>
<p><strong>WGAN(Wasserstein Generative Adversarial Nets)</strong> is proposed to replace JS-divergence with W-distance, due its smooth-measurement nature.</p>
<p>Starting from the W-Distance of $p_{r}, p_{g}$, the objective function of WGAN is derived into the form:</p>

$$
\mathcal{L} = \mathcal{W}_{K}(p_{r}, p_{g}) = \sup_{f, |f|_{L} \le K}{\mathbb{E}_{x \sim p_{r}}[f(x) ] - \mathbb{E}_{x \sim p_{g}}[f(x)]}
$$

<p>WGAN consists of :</p>
<ul>
<li>$f(.)$ : Be used to approximate the W-Distance between two distributions to whom inputs belongs. It is trained by maximizing the objective function, formally, the subtraction of two scores. It is meant to find the maximum from all possible $f$</li>
<li>$G$: Generates samples. It is trained to minimize the objective function</li>
</ul>
<blockquote>
<p>[!TIP]</p>
<ul>
<li>In order to let critic $f$ subject to <strong>Lipschitz continuity</strong>, <strong>weight clipping</strong> is applied to <strong>indirectly</strong> achieve that.</li>
<li>Unlike original GAN, critic can be trained as good as possible but not leading to problems like <strong>gradient vanishing</strong> and <strong>mode collapse</strong></li>
</ul>
</blockquote>
<h2 id="wgan-gp">WGAN-GP</h2>
<p>Although improved, WGAN sometimes suffer from problems like <a href="/posts/ai/models/the-gan-family/#hard-to-achieve-nash-equilibrium">Hard to achieve Nash Equilibrium</a>, still.</p>
<p>Identifying and proving <strong>weight clipping</strong> as a reason of that, authors of <strong>WGAN-GP</strong> replace it with <strong>gradient penalty(GP)</strong>:</p>

$$
\begin{align}
\mathcal{L} = \underbrace{\mathbb{E}_{\tilde x \sim p_{g}}[D(\tilde x)] - \mathbb{E}_{\tilde x \sim p_{r}}[D(x)]}_{\text{Original critic loss}} + \underbrace{\lambda\mathbb{E}_{\tilde x \sim p_{\tilde x }}[(||\nabla_{\hat x}(D(\hat x))||_{2}- 1)^2]}_{\text{gradient penalty}}
\end{align}
$$

<h2 id="gan-qp">GAN-QP</h2>
<p>Consider the following divergence: QP-div, quadratic potential divergence:
$$
\begin{equation}\begin{aligned}&amp;\mathcal{D}[p(x),q(x)] \ =&amp; \max_{T}, \mathbb{E}_{(x_r,x_f)\sim p(x_r)q(x_f)}\left[T(x_r,x_f)-T(x_f,x_r) - \frac{(T(x_r,x_f)-T(x_f,x_r))^2}{2\lambda d(x_r,x_f)}\right]\end{aligned}\end{equation}
$$
where:</p>
<ul>
<li>$\lambda &gt; 0$</li>
<li>$d$ is any distance function</li>
</ul>
<p>It&rsquo;s natural to build Loss Function for $D$ and $G$ with the Divergence, but for $G$, the quadratic term is neglected, since minimizing the denominator $d$  is not feasible.
$$
\begin{equation}\begin{aligned}&amp;T= \mathop{\arg\max}<em>T, \mathbb{E}</em>{(x_r,x_f)\sim p(x_r)q(x_f)}\left[T(x_r,x_f)-T(x_f,x_r) - \frac{(T(x_r,x_f)-T(x_f,x_r))^2}{2\lambda d(x_r,x_f)}\right] \ &amp;G = \mathop{\arg\min}<em>G,\mathbb{E}</em>{(x_r,x_f)\sim p(x_r)q(x_f)}\left[T(x_r,x_f)-T(x_f,x_r)\right]\end{aligned}\end{equation}
$$</p>
<h2 id="f-gan">f-GAN</h2>
<table>
<thead>
<tr>
<th>Notation</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://en.wikipedia.org/wiki/F-divergence">f-divergence</a></td>
<td>A generalized version of <strong>KL-divergence</strong> and <strong>JS-divergence</strong>: $$\mathcal{D}_{f}(P|Q) = \int{q(x)f(\frac{p(x)}{q(x)})dx}$$, where $f$ is a convex function with measures the difference between PDs</td>
</tr>
<tr>
<td>$T$</td>
<td>The gradient of $f = \frac{p(x)}{q(x)}$</td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>In a nutshell, the $f$-GAN:</p>
<ul>
<li>Decide an $f$</li>
<li>Learn a model $\mathcal{D}$ to score the input $x \sim p_{r}, y \sim p_{g}$, whose results can be used to approximate the $f$-divergence between $p_{r}$ and $p_{g}$</li>
<li>Formulate the objective function as:
$$\mathcal{L} = \mathbb{E}_{x \sim P_{\text{data}}} [T^*(D(x))] - \mathbb{E}_{z \sim P_z} [f^*(T^*(D(G(z))))]$$</li>
</ul>
<h2 id="bigan">BiGAN</h2>
<p>Changes the task of $D$, $D(x, z)$ outputs the possibility of $x$ coming from real image.</p>
<p>Besides, the divergence of $p(x|z)$ and prior $\mathcal{N}(0, I)$ is no longer included in the loss, the target is achieved by optimizing $D$</p>
<p>$$
\mathcal{L} = (0 - D_{x\sim p_{g}, z\sim \mathcal{N}(0, I)}(x,z)))^{2} + (1 - D_{x\sim p_{r}, z\sim p(z|x)}(x,z)))^{2}
$$</p>
<h2 id="sagan">SAGAN</h2>
<p><strong>GAN</strong> fails to capture the invariant geometric pattern within some categories, e.g. the fur of dogs. This is not brought by GAN itself, since it doesn&rsquo;t imply any implementation, but brought by relying on <strong>Convolution</strong> to capture dependency between different regions.</p>
<p><strong>SAGAN</strong>(Self-Attention GAN) proproses two changes:</p>
<ul>
<li>Introduce <strong>self-attention</strong> into GAN: promotes the long-distance dependency and description of geometric features of image generation</li>
<li>Introduce <strong>spectral normalization</strong> into experiments, achieving better results</li>
<li>Set different learning rates for $G$ and $D$: $l_{r_{D}}$ = 4e-4, $l_{r_{G}}$ = 1e-4</li>
<li><strong>hinge adversarial loss</strong>: $L_{D} = -\mathbb{E}<em>{(x,y)\sim p</em>{data}}[min(0, -1 + D(x,y))]  -\mathbb{E}<em>{z\sim p</em>{z,}y \sim p_{data}}[min(0, -1 - D(G(z),y))]$, restricting ability of $D$</li>
</ul>
<h2 id="biggan">BigGAN</h2>
<h3 id="notations-1">Notations</h3>
<table>
<thead>
<tr>
<th style="text-align:left">Notations</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Orthogonal matrix</td>
<td style="text-align:left">A matrix, which row/column vectors are unit vector, and co-orthogonal. The reverse matrix of an orthogonal matrix is equal to its transpose matrix, $A^{T}A = AA^{T} = I$. The transpose of a real-valued orthogonal matrix is orthogonal.</td>
</tr>
<tr>
<td style="text-align:left">Conjugate transpose</td>
<td style="text-align:left">A matrix as a result of taking the conjugate of each elements, then transpose the original matrix. For real-valued matrixs, conjugate transpose is equal to transpose, since the conjugate of a real value is itself</td>
</tr>
<tr>
<td style="text-align:left">Singular Value Decomposition(SVD)</td>
<td style="text-align:left">An algorithm for decomposing matrix as <strong>eigenvector</strong> and <strong>eigenvalue</strong>, representing $A$ as: $A = U\Sigma V^{\ast}$, where $\Sigma$ is eigenvalue, $U$ is left eigenvector, $V$ is right eigenvector</td>
</tr>
<tr>
<td style="text-align:left">Eigenvalue</td>
<td style="text-align:left">Special value of a matrix, indicating the <strong>energy</strong> of a matrix in according <strong>eigenvector</strong> direction. In the direction of a eigenvector with bigger eigenvalue, data varies faster(e.g. the variation of the projected-matrix is bigger in the direction), so keeping the biggest eigenvalues is ideal in data compresion/dimension reduction</td>
</tr>
<tr>
<td style="text-align:left">Singular Value</td>
<td style="text-align:left">eigenvalue of the matrix</td>
</tr>
<tr>
<td style="text-align:left">Spectral Norm (a.k.a. Induced 2-norm)</td>
<td style="text-align:left">Maximum Singular value of the matrix:$$|A|<em>{2} = \max</em>{|x|<em>{2}\ne0} \frac{|Ax|</em>{2}}{|x|_2}$$By restricting spectral norm, Lipschitz can also be controlled.</td>
</tr>
</tbody>
</table>
<p><strong>BigGAN</strong> bases on <a href="/posts/ai/models/the-gan-family/#sagan">SAGAN</a>, introduce several improvements:</p>
<ul>
<li>Send $z$ to multiple layers of $G$ with skip-net instead of input layer only</li>
<li><strong>Truncation trick</strong>: Truncate $z$ by setting threshold, exceeded sample is resampled until falling into the range</li>
<li>Hinge adversarial loss:</li>
<li>Spectral Norm</li>
</ul>
<h2 id="vae-gan">VAE-GAN</h2>
<p>The original output of VAE is blurry. By adding a discriminator, we hope it will recognize the blurs in the image.</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Reinforcement Learning</title>
      <link>https://mickjagger19.github.io/posts/ai/rl/reinforcement-learning/</link>
      <pubDate>Tue, 16 Jan 2024 21:31:43 +0800</pubDate>
      
      <guid>https://mickjagger19.github.io/posts/ai/rl/reinforcement-learning/</guid>
      <description>Personal takeaways of RL/RLHF/DPO</description>
      <content:encoded><![CDATA[<h2 id="terminologies">Terminologies</h2>
<p>General:</p>
<table>
<thead>
<tr>
<th style="text-align:left">Term</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Reinforcement Learning</td>
<td style="text-align:left">A branch/paradigm of machine learning, concerned with how an intelligent agent behaves in a dynamic environment.</td>
</tr>
<tr>
<td style="text-align:left"><strong>BLUE</strong>(bilingual evaluation understudy)</td>
<td style="text-align:left">An algorithm for evaluating the quality of text which has been machine-translated from one natural language to another</td>
</tr>
<tr>
<td style="text-align:left">Reward Model(Actor model)</td>
<td style="text-align:left">A model aligned with human feedback, predicting the reward of given actions</td>
</tr>
<tr>
<td style="text-align:left">$G_{t}$</td>
<td style="text-align:left">Return(aka the future reward), total sum of <strong>discounted</strong> rewards after time $t$:  $G_{t} = {\sum}^{\infty}_{k = 0}\gamma^{k}R_{t + k + 1}$</td>
</tr>
<tr>
<td style="text-align:left">$V_{\pi}(s)$</td>
<td style="text-align:left">State-value function, measures the expected return of state $s$: $V(s) = \mathbb{E}_{\pi}[G_{t}\vert S_{t} = s]$ under $\pi$</td>
</tr>
<tr>
<td style="text-align:left">$Q_{\pi}(s,a)$</td>
<td style="text-align:left">Action-value function, measures the expected return of action $a$ under state $s$: $Q_{\pi}(s, a) = \mathbb{E}_{\pi}[G_{t} \vert S_{t} = s, A_{t} = a]$ under $\pi$</td>
</tr>
<tr>
<td style="text-align:left">Bellman Equations</td>
<td style="text-align:left">A set of equations that decompose the value function into <strong>immediate reward</strong> + <strong>discounted future values</strong></td>
</tr>
<tr>
<td style="text-align:left">$A_{q}$</td>
<td style="text-align:left">the action to update $Q$</td>
</tr>
<tr>
<td style="text-align:left">$A_{t+1}$</td>
<td style="text-align:left">the actual taken action</td>
</tr>
</tbody>
</table>
<p>In RL Algorithms (mostly adjectives):</p>
<table>
<thead>
<tr>
<th style="text-align:left">Term</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Model-based</td>
<td style="text-align:left">Algorithms of RL relying on a (environment dynamic) model, which defines $P(s&rsquo;\vert s,a), R(s,a)$</td>
</tr>
<tr>
<td style="text-align:left">Model-free</td>
<td style="text-align:left">Algorithms of RL learning by the interaction of the model with environment</td>
</tr>
<tr>
<td style="text-align:left">Policy-Based(Policy Gradient) Methods</td>
<td style="text-align:left">A branch of RL: quantize each action as <strong>PDF</strong></td>
</tr>
<tr>
<td style="text-align:left">Value-Based Methods</td>
<td style="text-align:left">A branch of RL: quantize each action as value(PMF ? )</td>
</tr>
<tr>
<td style="text-align:left">Current policy</td>
<td style="text-align:left">The policy(actions) actually taken by an agent in an episode</td>
</tr>
<tr>
<td style="text-align:left">On-policy</td>
<td style="text-align:left">Using the action in current(actually exploited/taken) policy to update $V$</td>
</tr>
<tr>
<td style="text-align:left">Off-policy</td>
<td style="text-align:left">Using an action not from current policy to update $V$</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
</tr>
</tbody>
</table>
<h2 id="introduction">Introduction</h2>
<h2 id="value-based">Value-based</h2>
<h3 id="dynamic-programming">Dynamic Programming</h3>
<p>We can use Dynamic Programming to iteratively update and query value functions ($V_{\pi}$), with the help of Bellman
equations, <strong>when the model is fully known</strong>.</p>
<h3 id="monte-carlo">Monte-Carlo</h3>
<p>#model_free</p>
<p>Instead of modeling the environment, <strong>MC methods</strong> learns from <strong>episodes of raw experience</strong>, approximating the
observed mean return as expected return.</p>
<p>To optimally learn in <strong>MC</strong>, we take following steps:</p>
<ol>
<li>Improve the policy greedily: $\pi(s) = \underset{a}{argmax}Q(s, a)$</li>
<li>Generate a new episode with the combination of the new policy $\pi$ and randomness(e.g. $\epsilon$-greedy), balancing
between exploitation and exploration</li>
<li>Estimate $Q$ with the generated episode $\pi$</li>
</ol>
<h3 id="temporal-difference-methods">Temporal Difference methods</h3>
<p>#model-free</p>
<blockquote>
<p>[!NOTE]
TD learning can learn from <strong>incomplete</strong> episodes</p>
</blockquote>
<h4 id="bootstrapping">Bootstrapping</h4>
<p><strong>Estimate</strong> the rewards, rather than exclusively carrying out the episode.</p>
<h4 id="value-estimation">Value Estimation</h4>
<p>The estimated Value funciont $V$ is updated towards an estimated return $R_{t+1} + \gamma V(S_{t+1})$</p>
<h4 id="sarsa">SARSA</h4>
<p>#on-policy</p>
<blockquote>
<p>[!TIP]
Define $A_{q}$ as the action to update $Q$</p>
</blockquote>
<p>State-Action-Reward-State-Action
In each $t$:</p>
<ol>
<li>Choose $A_{t} = \underset{a \in A}{argmax}{Q(S_{t}, a)}$ with $\epsilon$-greedy</li>
<li>Obtain $R_{t + 1}$</li>
<li>Set $A_{t+1} \sim \pi(\cdot|s) = A_{q}$, under <strong>current policy</strong></li>
<li>Update $Q$ with the <strong>advantage of actual $A_{t+1}$ over expected reward</strong>:</li>
</ol>


$$
Q(S_{t}, A_{t}) \leftarrow Q(S_{t}, A_{t}) + \alpha(\underbrace{R_{t+1} + \gamma Q(S_{t + 1}, A_{t + 1})}_{\text{value  of current  policy, on-policy}} - \underbrace{Q(S_{t},A_{t})}_{\text{expected  value}})
$$


<ol start="5">
<li>$t = t + 1$</li>
</ol>
<blockquote>
<p>[!NOTICE]
$A_{q} == A_{t + 1}$, making it on-policy</p>
</blockquote>
<h4 id="q-learning">Q-Learning</h4>
<p>#off-policy</p>
<p>Q-learning is an off-policy method, with the steps in one episodes ($t, S_{t}$) being:</p>
<ol>
<li>Choose $A_{t} = \underset{a \in A}{argmax}Q(S_{t}, a)$ with $\epsilon$-greedy</li>
<li>Obtain $R_{t + 1}$</li>
<li>Set $A_{t+1} \sim \pi(\cdot|s)$, $A_{q} = \underset{a \in A}{\max} Q(S_{t + 1}, a)$</li>
<li>Update $Q$ with the <strong>advantage of optimal $A_{t + 1}$ over expected reward</strong>:<br>


   $$Q(S_{t}, A_{t}) \leftarrow Q(S_{t}, A_{t}) + \alpha(\underbrace{R_{t+1} + \gamma \underset{a \in A}{\max} Q(S_{t + 1}, a)}_{\text{best  value  after $A_{t}$, off-policy}} - \underbrace{Q(S_{t}, A_{t})}_{\text{expected  value}})
   $$

   </li>
<li>$t = t + 1$</li>
</ol>
<blockquote>
<p>[!NOTICE]
$A_{q} = \underset{a \in A}{\max} Q(S_{t + 1}, a) \ne A_{t + 1}$, making it off-policy</p>
</blockquote>
<h4 id="dqn">DQN</h4>
<p>#off-policy</p>
<p>Deep Q-Network, An improvement of <strong>Q-Learning</strong>:</p>
<ul>
<li><strong>Experience Replay</strong>: All the episode steps $e_{t} = (S_{t}, A_{t}, R_{t}, S_{t+1})$ are stored in one replay memory
$D_{t} = {e_{1}, &hellip;, e_{t}}$. During Q-learning updates, samples are drawn at random from the replay memory and thus
one sample could be used multiple times.</li>
<li><strong>Periodically Updated Target</strong>: Q is optimized towards target values that are only <strong>periodically</strong> updated(not
updated in each iteration anymore). The Q network is cloned and kept frozen as the optimization target every <strong>C</strong>
steps (C is a hyperparameter).</li>
</ul>
<blockquote>
<p>[!WARNING]
Known for overestimating value function $Q$</p>
</blockquote>
<h2 id="policy-gradient">Policy Gradient</h2>


$$
\begin{align*}
J(\theta) = \underset{s \in S}{\sum\limits} d^{\pi}(s)V^{\pi}(s) = \underset{s \in S}{\sum\limits} d^{\pi} \underset{a
\in A}{\sum\limits} \pi_{\theta}(a|s)Q^{\pi}(s,a)
\end{align*}
$$


<h3 id="actor-critic">Actor-Critic</h3>
<p>Actor-Critic learns the <strong>value function</strong> in addition to the policy, assisting the policy update.</p>
<p>It consists of two models:</p>
<ul>
<li><strong>Actor</strong> updates the policy $\theta$ of $\pi_\theta(a|s)$, suggested by critic</li>
<li><strong>Critic</strong> updates the value estimation function $Q(a|s) | V_{w}(s)$</li>
</ul>
<p>The main process being, for $t \in (1, T)$:</p>
<ol>
<li>Sample $a \sim \pi_{\theta}(a|s), r_{t} \sim R(s,a), s&rsquo; \sim P(s&rsquo;|s,a)$, next action $a&rsquo; \sim \pi_{\theta}(a&rsquo;|s&rsquo;)$</li>
<li>Update <strong>Actor</strong> $\theta$:
$$
\theta \leftarrow \theta + \alpha_{\theta} Q_{w}(s,a)\nabla_{\theta} ln \pi_{\theta}(
a|s)</li>
</ol>
<p>$$</p>
<p>to maximize the reward
5. Compute the correction (TD error, measures the quality of current policy $a&rsquo;$):


$$
\delta_{t} = \underbrace{r_{t} + \gamma Q_{w}(s', a')}_{\text{Action-Value of a'}} - \underbrace{Q_{w}(s,a)}_{\text{actual reward}}
$$


6. Update <strong>Critic</strong>: $w \leftarrow w + \alpha_{w}\delta_{t}\nabla_{w}Q_{w}(s,a)$ to reduce estimate error (ideally,
$\delta_{t} \leftarrow 0$, as $a&rsquo; \sim \pi_{\theta}(a&rsquo;|s&rsquo;)$)
7. Update $a \leftarrow a&rsquo;, s \leftarrow s'$</p>
<blockquote>
<p>[!TIP]
Adversarial training, resembles GAN: (generator, discriminator)</p>
</blockquote>
<h3 id="a2c">A2C</h3>
<table>
<thead>
<tr>
<th style="text-align:left">Model</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Actor $\pi_{\theta}$</td>
<td style="text-align:left">The target model</td>
</tr>
<tr>
<td style="text-align:left">Critic</td>
<td style="text-align:left">Estimate $V$</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
</tr>
</tbody>
</table>
<p>$$
L(\theta) = -\log \pi_{\theta}(a_{t}|s_{t})\hat A_{t}
$$</p>
<p>where:</p>
<ul>
<li>$\hat A$: advantage function, the advantage of $a_{t}$ compared with average, normally $V$</li>
</ul>
<blockquote>
<p>[!WARNING]
This objective function can lead to massive change to policy</p>
</blockquote>
<h3 id="a3c">A3C</h3>
<p><strong>Asynchronous Advantage Actor-Critic</strong> focuses on parallel training. Multiple actors are trained in parallel and get
synced with global parameters.</p>
<h3 id="dpg">DPG</h3>
<p>#model-free #off-policy</p>
<p><strong>Deterministic Policy Gradient</strong> models the policy as deterministic function $a = \mu(s)$.</p>
<p>It is trained by maximizing the objective function: the expected discounted reward:</p>
<p>$$
J(\theta) = \int_{S}\rho^{\mu}(s)Q(s, \mu_{\theta}(s))ds
$$</p>
<p>where:</p>
<ul>
<li>$\rho^{\mu}(s&rsquo;)$: discounted state distribution</li>
<li>$\mu$: the deterministic action predictor</li>
</ul>
<h3 id="ddpg">DDPG</h3>
<p>#model-free #off-policy</p>
<p><strong>Deep Deterministic Policy Gradient</strong></p>
<p>Combining <a href="/posts/ai/rl/reinforcement-learning/#dqn">DQN</a> (experience replay, freezing target model) and <a href="/posts/ai/rl/reinforcement-learning/#dpg">DPG</a></p>
<p>Key design:</p>
<ul>
<li>Better exploration: $\mu’(s) = \mu_{\theta}(s) + \mathcal{N}$, adding noise $\mathcal{N}$ to policy</li>
<li>Soft updates: Moving average of parameter $\theta$</li>
</ul>
<h3 id="td3">TD3</h3>
<p><strong>Twin Delayed Deep Deterministic</strong> applied tricks on <a href="/posts/ai/rl/reinforcement-learning/#ddpg">DDPG</a> to prevent overestimating value function:</p>
<ol>
<li>Clipped Double Q-learning: Action selection and Q-value estimation are made by two networks separately.</li>
<li>Delayed update of target the policy network: Instead of updating actor and critic in one iteration, <strong>TD3</strong> updates
the <strong>actor</strong> at a lower frequency than <strong>critic</strong>, waiting for it to become stable. It helps reducing the variance.</li>
<li>Target policy smoothing: Introduce a smoothing regularization strategy by adding $\epsilon \sim clip(\mathcal{N}(0,
\sigma), -c , +c)$ to the value function $Q_{w}(s&rsquo;, \mu_{\theta}(s&rsquo;) + \epsilon))$. It mitigates the risk of
deterministic policies overfitting the value function.</li>
</ol>
<h3 id="sac">SAC</h3>
<p><strong>Soft Actor-Critic</strong> learns a more random policy by incorporating the entropy of the policy $H(\pi)$ into the reward.</p>
<p>Three key components:</p>
<ul>
<li>An actor-critic architecture</li>
<li>An off-policy approach</li>
<li>Entropy Maximization to encourage exploration</li>
</ul>
<p>The policy is trained by maximizing the objective function: expected return + the entropy


$$
J(\theta) = \sum\limits_{t = 1}^athbb{E}_{s_{t},a_{t} \sim \rho_{\pi_
{\theta}}} [r(s_{t},a_{t}) + \alpha \mathcal{H}(\pi_{\theta}(* | s_{t}))]
$$

</p>
<h3 id="ppo-proximal-policy-optimization">PPO (Proximal Policy Optimization)</h3>
<p>#on-policy</p>
<blockquote>
<p>[!TIP]</p>
<ul>
<li>clipped objective</li>
<li><strong>Proximal</strong> stands for <strong>Reward Model</strong></li>
</ul>
</blockquote>
<p>As a successor of <a href="/posts/ai/rl/reinforcement-learning/#a2c">A2C</a>, PPO defines 2 more models:</p>
<table>
<thead>
<tr>
<th style="text-align:left">Model</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Reward $r_{\theta}$</td>
<td style="text-align:left">Calculate $R$</td>
</tr>
<tr>
<td style="text-align:left">Reference $\pi_{ref}$</td>
<td style="text-align:left">Apply constraint and guidance to <em>Actor</em></td>
</tr>
<tr>
<td style="text-align:left">$r^{\ast}$</td>
<td style="text-align:left">Ground-truth reward function</td>
</tr>
<tr>
<td style="text-align:left">$r_\phi$</td>
<td style="text-align:left">MLE of $r^{\ast}$</td>
</tr>
</tbody>
</table>
<p>$$
L(\theta) = \underbrace{-\hat A_{t} \cdot min(r_{t}(\theta), clip(r_{t}(\theta), 1 - \epsilon, 1 + \epsilon))}<em>{\text{A2C loss, $\le 1$ + $\epsilon$}}  -  \underbrace{\beta D</em>{KL}(\pi_{\theta}(y|x)||\pi_{ref}(y|x))}_{\text{penalty of being too distant to normal response}}
$$</p>
<p>where:</p>
<ul>
<li>$r_{t}(\theta) = \frac{\pi_{\theta}(a_{t}|s_{t})}{\pi_{\theta_{old}}(a_{t}|s_{t})}$: the ratio of new policy to old policy</li>
<li>$\epsilon$: normally 0.1 or 0.2</li>
</ul>
<ul>
<li>Generate two outputs from same input $x$: $y_{1}, y_{2}$
<ul>
<li>Objective: $\mathcal{L} = \underset{\pi_{\theta}}{\max}\mathbb{E}[r_{\theta}(x,y_{2})]$
<ul>
<li>Update:
<ul>
<li>Optimize with the reward of current batch</li>
<li>TRO(<strong>Trust Region Optimization</strong>): using <strong>gradient constraint</strong> to make sure the update process doesn&rsquo;t sabotage the stability of learning process.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<blockquote>
<p>[!TIP]</p>
<ul>
<li>$r$ and $\pi$ can be optimized iteratively.</li>
<li>RLHF and PPO is difficult to train.</li>
</ul>
</blockquote>
<h3 id="dpodirect-preference-optimization">DPO(Direct Preference Optimization)</h3>
<blockquote>
<p>[!NOTE] The major difference
<strong>Direct</strong>: directly optimize with reward, rather than $V | Q$: <strong>expected</strong> rewards from a reward model</p>
</blockquote>
<p>Rewrite objective:</p>
<p>$$
\begin{align*}
\pi
&amp;= \underset{\pi}{\max}(r_{\phi}(x,y) - \beta D_{KL}(\pi_{\theta}(y|x)||\pi_{ref}(y|x)))\\
&amp;= \underset{\pi}{\max}(r_{\phi}(x,y) - \beta \log \frac{\pi_{\theta}(y|x)}{\pi_{ref}(y|x)})\\
&amp;= \underset{\pi}{\min}( \log \frac{\pi_{\theta}(y|x)}{\pi_{ref}(y|x)} - r_{\phi}(x,y)/\beta)\\
&amp;= \underset{\pi}{\min}( \log \frac{\pi_{\theta}(y|x)}{\pi_{ref}(y|x) e^{r_{\phi}(x,y)/\beta} })
\end{align*}
$$</p>
<p>^0639d4</p>
<p>Define partition function: $Z = \Sigma_{y}{\pi_{ref}(y|x) e^{r_{\theta}(x,y)/\beta}}$, which relates to the reward of $\theta$ over $ref$</p>
<p>We can get the optimal strategy $\pi^{\ast}$ under $r_{\phi}$(irrelevant of $\theta$):</p>
<p>$$
\pi^{*}(y|x)  = \pi_{ref}(y|x)e^{\frac{r_{\phi} (x,y)}{\beta}} \frac{1}{Z(x)}
$$</p>
<p>^5ee375</p>
<p>Then Eq [[#^0639d4]] became:</p>
<p>$$
\begin{align*}
\pi
&amp;= \underset{\pi}{\min}\left( \log \frac{\pi_{\theta}(y|x)}{\pi_{ref}(y|x) e^{r_{\phi}(x,y)}{\beta}}\right)\\
&amp;= \underset{\pi}{\min}\left( \log \frac{\pi_{\theta}(y|x)}{\pi^{\ast}(y|x) Z(x)}\right)\\
&amp;= \underset{\pi}{\min}\left( \log \frac{\pi_{\theta}(y|x)}{\pi^{\ast}(y|x)}\right)\\
&amp;= \underset{\pi}{\min}\left( D_{KL}(\pi_{\theta}(y|x) || \pi^{\ast}(y|x))\right)
\end{align*}
$$</p>
<p>Apparently, the optimal $\pi$ is: $\pi_{\theta} \to \pi^{*}$.</p>
<p>Noticing that the reward function of E.Q. [[#^5ee375]] can be rewritten(reparameterized) as(where $\pi_{ref}$ is the human-preference data as ground-truth):</p>
<p>$$
r_{\phi} (x,y) = \beta \log \frac{\pi^{\ast}(y|x)}{\pi_{ref}(y|x)} + \beta \log Z(x)
$$</p>
<blockquote>
<p>[!TIP] the reward function can be represted with best policy trained under it</p>
</blockquote>
<p>By replacing $r_{\phi} (x,y)$ in the objective of RLHF as $\pi^{*}$, we get an objective function without the <strong>reward function</strong>:</p>


$$
\begin{align}
\mathcal{L}_{\text{DPO}}(\pi_{\theta}; \pi_{ref}) = -{{\mathbb{E}_{(x, y_{w}, y_{l}) \sim D}[\log \sigma{({\beta \frac{\pi_{\theta}(y_{w}|x)}{\pi_{ref}(y_{w}|x)} - \beta\frac{\pi_{\theta}(y_{l}|x)}{\pi_{ref}(y_{l}|x)} }})]}}
\end{align}
$$


<p>From this equation, we found that: <strong>Training the reward model in RLHF is equivalent to training $\pi_{\theta}$ with the derived objective function</strong>.</p>
<p>That is to say, no need of 4 models, we can achieve the same target of RLHF with directly training $\pi_{\theta}$.</p>
<h2 id="methods">Methods</h2>
<h3 id="rlhf">RLHF</h3>
<p><strong>RLHF(Reinforcement learning from human feedback)</strong> is a technique that trains a <strong>reward model</strong>.</p>
<p>It has following key concepts:</p>
<ul>
<li><strong>Reward Model</strong>: trained in advance directly from human feedback</li>
<li>human feedback: data collected by asking humans to <strong>rank</strong> instances of the agent&rsquo;s behavior</li>
</ul>
<p>The procedure is given by 3 steps</p>
<h4 id="1-sft">1. SFT</h4>
<p>Pre-train a (target) model: $\pi^{SFT}$</p>
<h4 id="2-reward-modeling-phase">2. Reward Modeling Phase</h4>
<p>Train a reward model: $r_{\phi}(x,y) = r, r \in (0, + \infty)$, where $r$ is the reward of the given input.</p>
<ul>
<li>
<p>Initialization: Often initialized from Pretrained Models</p>
</li>
<li>
<p>Data:</p>
<ul>
<li>$D$:  $Prompt: x \to (Generation: y, Reward: r)$, generated by human or models</li>
<li>Human Feedback: <strong>Ranking</strong> the outputs of different models under same prompt with $r$
<ul>
<li>effective ways of ranking: Comparing two/ ELO</li>
</ul>
</li>
<li>$(y_{win}, y_{loss})$ : sampled from generation</li>
</ul>
</li>
<li>
<p>Train the RM with Data
The Objective is (negative log-likelihood loss):

  
  $$
  \begin{align*}
  \mathcal{L}_{R}(r_{\phi}, D) = -{{\mathbb{E}_{(x, y_{w}, y_{l}) \sim
  D}[\log{\sigma({r_{\phi}(x, y_{w}) - r_{\phi}(x, y_{l})}})]}}
  \end{align*}
  $$

  </p>
<p>maximize the gap of rewards between better/worse response</p>
</li>
</ul>
<h4 id="3-rl-fine-tuning-phase-pi_thetax--py">3. RL Fine-Tuning Phase: $\pi_{\theta}(x) = p(y)$</h4>
<ul>
<li>In the past, training LM with RL was considered impossible.</li>
<li>One of the proposed feasible plan is PGR(<strong>Policy Gradient RL</strong>)/PPO(<strong>Proximal Policy Optimization</strong>)</li>
</ul>
<h2 id="conclusion">Conclusion</h2>
]]></content:encoded>
    </item>
    
    <item>
      <title>2D to 3D Assets - A survey</title>
      <link>https://mickjagger19.github.io/posts/misc/2d-to-3d-assets---a-survey/</link>
      <pubDate>Mon, 15 Jan 2024 21:21:07 +0800</pubDate>
      
      <guid>https://mickjagger19.github.io/posts/misc/2d-to-3d-assets---a-survey/</guid>
      <description>A survey of techniques to transform 2D image to 3D assets</description>
      <content:encoded><![CDATA[<hr>
<p>title: 2D to 3D Assets - A survey
summary: A survey of techniques to transform 2D image to 3D assets
tags:</p>
<ul>
<li>Diffusion</li>
<li>3D
author: Mick
draft: false
date: 2024-01-15T21:21:07+0800
math: true</li>
</ul>
<hr>
<h2 id="terminologies">Terminologies</h2>
<table>
<thead>
<tr>
<th style="text-align:left">Term</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">3D representation</td>
<td style="text-align:left">The representation of an object in 3D. There are different forms, including:<br>- Radiance Field</td>
</tr>
<tr>
<td style="text-align:left"><a href="https://en.wikipedia.org/wiki/Volume_rendering">Volume rendering</a></td>
<td style="text-align:left">A set of techniques used to display a 2D projection of a 3D discretely sampled data set</td>
</tr>
<tr>
<td style="text-align:left">Triangle mesh</td>
<td style="text-align:left">An important basic concept in digital modeling. A 3D model consists of triangles, the peripheral triangles form a triangle mesh</td>
</tr>
<tr>
<td style="text-align:left"><a href="https://en.wikipedia.org/wiki/Point_cloud">Point cloud</a></td>
<td style="text-align:left">A discrete set of data points in space</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
</tr>
</tbody>
</table>
<h2 id="notations">Notations</h2>
<table>
<thead>
<tr>
<th style="text-align:left">Notation</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">$N$</td>
<td style="text-align:left">the $NeRF$ instance</td>
</tr>
<tr>
<td style="text-align:left">$\sigma$</td>
<td style="text-align:left">The volumemetric density of a point in radiance</td>
</tr>
<tr>
<td style="text-align:left">$\theta$</td>
<td style="text-align:left">the viewing direction of a point between $x,y$</td>
</tr>
<tr>
<td style="text-align:left">$\phi$</td>
<td style="text-align:left">the viewing direction of a point between $(xy), z$</td>
</tr>
<tr>
<td style="text-align:left">$c$</td>
<td style="text-align:left">color</td>
</tr>
</tbody>
</table>
<h2 id="background">Background</h2>
<p>All 123 methods involves representing geometric latents.</p>
<h2 id="3d-representations">3D Representations</h2>
<h3 id="mesh">Mesh</h3>
<p>Set of $vertice, edge, face$</p>
<h3 id="point-cloud">Point Cloud</h3>
<p>Set of points described with $x, y, x$</p>
<h2 id="models">Models</h2>
<h3 id="nerf">NeRF</h3>
<p>NeRF(Neural Radiance Field) aims to synthesize novel view of complex scenes. Given several images from different views, output 3D-representatins of this view.</p>
<h4 id="radiance-field">Radiance Field</h4>
<p>Radiance Field describes a function $F$:
$$
F : (\underbrace{x,y,z}<em>{position},\underbrace{\theta,\phi}</em>{direction}) \to (R,G,B,\sigma)
$$
which outputs the color and density of a point (x,y,z), in the direction described by $\theta , \phi$</p>
<p>The color of a point can be calculated by:</p>
<p>$$
C(r) = \int_{t_{n}}^{t_{f}}{T(t)\sigma(r(t))}c(r(t), d)dt
$$</p>
<p>where:</p>
<ul>
<li>Cumulative light transmittance:  indicating the remained light density at a point in a radiance
$$T(t) = exp\left(-\int_{t_{n}}^{t}{\sigma(r(s))ds)}\right)$$</li>
<li>The radiance from focus to a pixel $r(t) = o + td$: $o$ for zero point, $t$ for distance</li>
</ul>
<p>Considering the calculation amount of this integration, the actual coloring involves calculating the weighted average of N points sampled from a radiance</p>
<p>With the help of this F, we can generate the image of the object viewing from every direction.Ne</p>
<p>NeRF models the $F$ with a nerual network $MLP$</p>
<blockquote>
<p>[!NOTE]
The input is 5D</p>
</blockquote>
<h4 id="volume-rendering">Volume Rendering</h4>
<p>Calculating the color of a certain pixel with integration would include massive Tensor, representing radiance field.</p>
<p>With the help of $F$, the color can be regressed with limited tensor size</p>
<h4 id="training">Training</h4>
<p>The objective function:
$$
\mathcal{L} = MSE( F_{\Theta} - \text{g.t.})
$$</p>
<p>The training data would be: images of an object from different views, with the color of a certain radiance extracted from the image.</p>
<h4 id="improvements">Improvements</h4>
<h5 id="positional-encoding">Positional encoding</h5>
<p>$$
\gamma(pos, p) = sin(2^{pos}\pi p)
$$</p>
<h5 id="hierarchical-volume-sampling">Hierarchical volume sampling</h5>
<p>Based on the fact that the density distribution is not uniform, random sample is not a good idea: some high-density region can sometimes be not sampled. Thus, we can use a $w_{i}$ for the possibility of a region being sampled, to increase the sample efficiency.
$$
\hat w_{i} = \frac{w_{i}}{\sum\limits_{j = 1}^{N_{c}}w_{j}}
$$</p>
<h4 id="modeling-3d-assets">Modeling 3D assets</h4>
<p>Luma AI</p>
<h4 id="e-commerce">E-commerce</h4>
<p>Modeling merchandises</p>
<h4 id="aigc">AIGC</h4>
<h4 id="virtual-human">Virtual human</h4>
<p>NeRF/3DGS</p>
<h4 id="gamevr">Game/VR</h4>
<h3 id="gaussian-splatting">Gaussian Splatting</h3>
<h3 id="dreamfusion">DreamFusion</h3>
<table>
<thead>
<tr>
<th style="text-align:left">Notation</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Imagen</td>
<td style="text-align:left">A predictor, predicting the noise of the input image conditioned on text prompt</td>
</tr>
</tbody>
</table>
<p>Text-To-3D with 2D Diffusion.
DreamFusion models the 3D object as $N$ in $NeRF$ format.</p>
<h4 id="trainingforward-process">Training(Forward Process)</h4>
<p>$$
\begin{align}
img_{0} &amp;= N(P(camera))&amp; \text{Camera is randomly generated}\
img_{1} &amp;= z_{t} + \epsilon, \epsilon \sim \mathcal{N}(0, I)\
\hat \epsilon_{\phi} &amp;= imagen(z_{t}, y)\
\epsilon_{\theta} &amp;= \hat \epsilon_{\phi} - \epsilon \
\end{align}
$$</p>
<blockquote>
<p>[!NOTE]</p>
<ul>
<li>The $\epsilon_{\theta}$ indicates the quality of the current NeRF MLP</li>
<li>The $\theta$ is not known by imagen, since it is only used to predict noise, which doesn&rsquo;t care about the position and angle of the camera</li>
</ul>
</blockquote>
<p>As for the loss function, since they observed that, with the loss as:</p>
<p>$$
\begin{align}
\mathcal{L}<em>{\text{Diff}}(\phi, x = g(\theta)) &amp;= \mathbb{E}</em>{t, \epsilon}\left[w(t)||\hat \epsilon_{\phi}(\underbrace{\alpha_{t}x + \sigma_{t}\epsilon}<em>{z</em>{t}}; t) - \epsilon||<em>{2}^{2}\right]\
\nabla</em>{\theta}\mathcal{L}<em>{\text{Diff}}(\phi, x = g(\theta)) &amp;= \mathbb{E}</em>{t, \epsilon}\left[w(t)\underbrace{(\hat \epsilon_{\phi}(z_{t};y, t) - \epsilon)}<em>{\text{Noise Residual}}\underbrace{\frac{\partial \hat \epsilon</em>{\phi}(z_{t};y, t) }{z_t}}<em>{\text{U-Net jacobian}}\underbrace{\frac{\partial x}{\partial \theta }}</em>{\text{Generator Jacobian}}\right]
\end{align}
$$
is <strong>expensive</strong> because of the U-Net Jacobian term(since U-net accepts $z_t$), they modify it by removing it:</p>
<p>$$
\nabla_{\theta}\mathcal{L}<em>{\text{SDS}}(\phi, x = g(\theta)) \triangleq \mathbb{E}</em>{t, \epsilon}\left[w(t)\underbrace{(\hat \epsilon_{\phi}(z_{t};y, t) - \epsilon)}<em>{\text{Noise Residual}}\underbrace{\frac{\partial x}{\partial \theta }}</em>{\text{Generator Jacobian}}\right]
$$
Intuitively, this loss perturbs x with a random amount of noise corresponding to the timestep t, and estimates an update direction that follows the score function of the diffusion model to move to a higher density region.</p>
<p>$$
\nabla_{\theta}L_{Diff} = \mathbb{E}<em>{t, \epsilon}\left[ \nabla</em>{x} \left( w(t) \left| e_{\phi}(\alpha_t x + \sigma_t \epsilon; t) - \ell \right|^2 \right) \nabla_{\theta}x \right] $$</p>
<h3 id="3d-gaussian-splatting">3D Gaussian Splatting</h3>
<table>
<thead>
<tr>
<th style="text-align:left">Notation</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">$x = [a,b,c]^{\top}$</td>
<td style="text-align:left">3D coordinates in a column vector</td>
</tr>
<tr>
<td style="text-align:left">$\Sigma$</td>
<td style="text-align:left">Covariance matrix</td>
</tr>
<tr>
<td style="text-align:left">Ellipsoid</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">Positive-Definite</td>
<td style="text-align:left">A property of a <strong>symmetric</strong> matrix $M$, where for every nonzero real column vector $z$, $z^{\top}Mz \gt 0$<br>$A^{\top}A$ is always a positive-definite matrix</td>
</tr>
<tr>
<td style="text-align:left">Positive Semi-Definite</td>
<td style="text-align:left">A similar concept to <strong>Positive-Definite</strong>, except that $z^{\top}Mz \ge 0$<br>The covariance is always positive semi-definite, and vice versa</td>
</tr>
<tr>
<td style="text-align:left">rasterization</td>
<td style="text-align:left">A task in image rendering, major task being mapping 3D triangles to the projection plane and pixelate them</td>
</tr>
<tr>
<td style="text-align:left">Like any other modeling methods, 3GS attempts to describe diverse geometric structure with basic elements.</td>
<td></td>
</tr>
</tbody>
</table>
<p>The standard form of (3D Gaussian) <strong>Ellipsoid</strong> is:</p>
<p>$$
G_{s}(x) = (\frac{1}{\sqrt{2\pi}^{3}det(\Sigma)})e^{-\frac{1}{2}(x-\mu)^{\top}\Sigma^{-1}(x-\mu)}
$$
where:</p>
<ul>
<li>$\Sigma$ controls the scale and rotation of the ellipsoid in 3 axis</li>
</ul>
<blockquote>
<p>[!NOTE]
The eigenvector of the $\Sigma$ is the symmetric axis of the ellipsoid</p>
</blockquote>
<p>In the paper, they define ellipsoid as:
$$
G(x) = e^{- \frac{1}{2}(x)^{\top}\Sigma^{-1}(x)}
$$
where the factor is removed</p>
<h4 id="initialize-the-ellipsoid">Initialize the ellipsoid</h4>
<p>It&rsquo;s not feasible to initialzie a 3D ellisoid from scratch, since though each 3D Gaussian Ellisoid has a corresponding $\Sigma$, it&rsquo;s not vice versa, which requires it to be <strong>positive semi-definite</strong>(a equivalence of a covariance matrix)</p>
<p>The paper mentioned that 3Ｄ Gaussian ellisoid is <del>similar to</del> an ellisoid. Inspired by that fact that ellisoid can be transformed from a <strong>ball</strong>, for ellisoid, we have a derivation of $\Sigma$ as:
$$
\Sigma = RSS^{\top}R^{\top}
$$
where:</p>
<ul>
<li>$S \in \mathbb{R}^{3}$ is a scaling transform</li>
<li>$R \in \mathbb{R}^{4}$ is a rotation transform</li>
</ul>
<p>And both transformation can be learned with nerual network</p>
<h4 id="splatting">Splatting</h4>
<p>The rasterization of ellisoid needs to be handled as well.</p>
<p>The 2D graphic we get from projecting the ellisoid to a plane is called <strong>Splat</strong>, which is defined as:
$$
\Sigma^{&rsquo;} = JW\Sigma W^{\top}J^{\top}
$$
where:</p>
<ul>
<li>$W$ for <strong>view</strong> transformation</li>
</ul>
<ul>
<li>$J$ for <strong>project</strong> transformation.It is get from taking the partial derivatives for the projection matrix from current point to the camera point</li>
</ul>
<h4 id="322">322</h4>
<p>It is proven that, the intergration of a 3D Gaussian along a certain axis is a 2D Gaussian, so the intergration can be replaced by a 2D Gaussian intergration</p>
<h2 id="projects">Projects</h2>
<h3 id="streamdiffusion">StreamDiffusion</h3>
]]></content:encoded>
    </item>
    
    <item>
      <title>Denoising Diffusion Models</title>
      <link>https://mickjagger19.github.io/posts/ai/models/denoising-diffusion-models/</link>
      <pubDate>Sun, 14 Jan 2024 14:31:43 +0800</pubDate>
      
      <guid>https://mickjagger19.github.io/posts/ai/models/denoising-diffusion-models/</guid>
      <description>Personal takeaways of DDIM/DDPM</description>
      <content:encoded><![CDATA[<h2 id="terminologies">Terminologies</h2>
<table>
<thead>
<tr>
<th style="text-align:left">Term</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Diffusion Models</td>
<td style="text-align:left">models that can sample from a highly complex probability distribution(e.g. images of cars)</td>
</tr>
<tr>
<td style="text-align:left">Non-equilibrium thermodynamics</td>
<td style="text-align:left">a branch of <a href="https://en.wikipedia.org/wiki/Thermodynamics" title="Thermodynamics">thermodynamics</a> that deals with physical systems that are not in <a href="https://en.wikipedia.org/wiki/Thermodynamic_equilibrium" title="Thermodynamic equilibrium">thermodynamic equilibrium</a>, where &ldquo;there are no net <a href="https://en.wikipedia.org/wiki/Macroscopic" title="Macroscopic">macroscopic</a> <a href="https://en.wikipedia.org/wiki/Flow_(mathematics)" title="Flow (mathematics)">flows</a> of <a href="https://en.wikipedia.org/wiki/Matter" title="Matter">matter</a> nor of energy within a system or between systems&rdquo;. <br>It is often used by diffusion models as a technique to sample from distribution.</td>
</tr>
<tr>
<td style="text-align:left">Diffusion</td>
<td style="text-align:left">the <strong>net movement</strong> of anything, generally from a region of higher <a href="https://en.wikipedia.org/wiki/Concentration" title="Concentration">concentration</a> to a region of lower concentration. <br>Also a technique of <strong>Non-equilibrium thermodynamics</strong></td>
</tr>
<tr>
<td style="text-align:left">DDPM</td>
<td style="text-align:left">model that improves the performance of <strong>diffusion models</strong> by <strong>variational inference</strong></td>
</tr>
<tr>
<td style="text-align:left">DDIM</td>
<td style="text-align:left">a generalized version of DDPM, with better performance and less diversity and quality</td>
</tr>
<tr>
<td style="text-align:left"><strong>Jensen</strong> inequality</td>
<td style="text-align:left">$f(\sum\limits a_{i}x_{i}) \le \sum\limits a_{i}f(x_{i})$, where $a \ge 0, \sum\limits a_{i} = 1$<br>In other words, the Expected Value of a convex function $\ge$ the value of the function at the Expected Input <br></td>
</tr>
<tr>
<td style="text-align:left"><a href="https://en.wikipedia.org/wiki/Evidence_lower_bound">Variational Lower Bound</a>(short for <strong>VLB</strong>, a.k.a. Evidence Lower BOund, short for <strong>ELBO</strong>)</td>
<td style="text-align:left">A easy-to-train lower bound of Log-Likelihood, derived by using a prior $p(z)$ to approximate (implies <em>variational</em>) an <del>intractable</del> posterior $q$.</td>
</tr>
<tr>
<td style="text-align:left"><a href="https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant"><strong>Jacobian matrix</strong></a></td>
<td style="text-align:left">A matrix derived from a vector of function of several variables, with all its first-order partial derivatives. Suppose $f: R^{n} \to R^{m}$:<br>$$<br>J = [\frac{\partial{f}}{\partial{x_{1}}}&hellip;\frac{\partial{f}}{\partial{x_{n}}}]<br>$$</td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
</tr>
</tbody>
</table>
<h2 id="notations">Notations</h2>
<table>
<thead>
<tr>
<th style="text-align:left">Notation</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">$x_{0}$</td>
<td style="text-align:left">the data point, where $t$ is the total count of timestamp</td>
</tr>
<tr>
<td style="text-align:left">$x_{t}$</td>
<td style="text-align:left">the data after applying $t$ times of forward iteration</td>
</tr>
<tr>
<td style="text-align:left">$\epsilon_t$</td>
<td style="text-align:left">the (standard gaussian) noise</td>
</tr>
<tr>
<td style="text-align:left">$\epsilon_{\theta}(x_t,t)$</td>
<td style="text-align:left">our model to predict the noise at each timestamp</td>
</tr>
<tr>
<td style="text-align:left">$\mu_{\theta}(x_t, t)$</td>
<td style="text-align:left">parameterized model to predict $x_{t-1}$ at time $t$</td>
</tr>
<tr>
<td style="text-align:left">$p(x_{0:T})$</td>
<td style="text-align:left">the joint distribution of $x_{0}, x_{1} &hellip; x_{T}$</td>
</tr>
</tbody>
</table>
<h2 id="introduction">Introduction</h2>
<p>This article will introduce the definitions of <strong>DDPM</strong> and <strong>DDIM</strong>.</p>
<p>As stated earlier, the work of <strong>DDIM</strong> is based on <strong>DDPM</strong>.</p>
<h2 id="ddpm">DDPM</h2>
<p><strong>Diffusion Models</strong> often involves modeling two processes:</p>
<ul>
<li><strong>forward process</strong>: noise data($x_{0}$) to data point($x_{t}$)</li>
<li><strong>reverse process</strong>: data point to noise data, the reversion of <em>forward process</em></li>
</ul>
<h3 id="forward-process">Forward Process</h3>
<p>As a improvement of Diffusion Models, <strong>DDPM</strong> models the forward process as:</p>
<p>$$
\begin{equation}
x_{t} = \alpha_{t} x_{t-1} + \beta_{t}\epsilon_{t}, \epsilon_{t} \sim \mathcal{N}(0,1), 0 \le t \le T, t \in \mathbb{Z}
\end{equation}
$$</p>
<p>where $\alpha, \beta &gt; 0, \alpha_{t}^{2}+ \beta_{t}^{2} = 1$. This can be viewed as:</p>
<ul>
<li>the remains from the previous data: $\alpha_{t} x_{t-1}$</li>
<li>the destruction by introducing noise $\epsilon_{t}$</li>
</ul>
<p>Accordingly, the conditional probability of $x_t$ would be:
$$
p(x_{t}| x_{t-1}) = \mathcal{N}(x_{t};\alpha_{t}x_{t-1}, \beta_{t}^{2}I)
$$</p>
<blockquote>
<p>[!NOTE]</p>
<ul>
<li>All $\alpha, \beta, T$ are constants</li>
<li>Apparently, this is a <strong>Markovian process</strong></li>
</ul>
</blockquote>
<h3 id="reverse-process">Reverse Process</h3>
<p>By applying the forward process for $T$ times, we have $t$ pairs of $(x_{t-1}, x_{t})$. This is our training data.</p>
<p>Reversing the forward process, the task of the reverse process should be:
learn how to get  $x_0$ from $x_{t}$, formally $x_0 \to x_{t}$.</p>
<p><strong>DDPM</strong> splits this process into $t$ steps of  $x_t \to x_{t-1}$.</p>
<blockquote>
<p>[!TIP]  The Methodology of DDPM
<strong>DDPM</strong> is a Likelihood-based Model.</p>
</blockquote>
<p>In the paper, they model each single step as a <strong>gaussian transition</strong>:
$$
p_{\theta}(x_{t-1}|x_{t}) = \mathcal{N}(x_{t-1};\mu_{\theta}(x_{t}, t), \Sigma_{\theta}(x_{t},t))
$$
where:</p>
<ul>
<li>$\mu_{\theta}(x_{t}, t)$: the mean value</li>
<li>$\Sigma_{\theta}(x_{t},t)$: the variance predictor (of reverse process).</li>
</ul>
<blockquote>
<p>[!NOTE]</p>
<ul>
<li>The noise is not gaussian noise multiplied by a factor, but predicted directly.</li>
<li>Not to confuse:
* $\Sigma_{\theta}(x_{t},t)$: the noise in reverse process. It has been tested positive to the reverse process
* $\epsilon_\theta(x_{t},t) \to \epsilon_{t}$ : the noise predictor (<strong>of forward process</strong>)</li>
</ul>
</blockquote>
<h4 id="mean-value-predictor-mu_thetax_t-t">Mean Value Predictor: $\mu_{\theta}(x_{t}, t)$</h4>
<p>In order to model the $\mu_{\theta}(x_{t}, t)$, from Bayes&rsquo; Theorem we have:
$$
p(x_{t-1}| x_{t},x_0)= \frac{p(x_t|x_{t-1}) p(x_{t-1} | x_0)}{p(x_{t}|x_{0}) }
$$</p>
<p>The process of induction would be:</p>
<ol>
<li>Predict $x_t$, $x_{t-1}$ from $x_0$</li>
<li>Replace all the variables($x_{0}$) with $x_{t}$ in Equation 4</li>
</ol>
<h5 id="predict-x_0-with-x_t">Predict $x_{0}$ with $x_{t}$</h5>
<p>Applying forward process $p(x_{t}|x_{t-1})$ for $t$ times, we can rewrite $x_{t}$ as:</p>
<p>$$
x_{t} = \bar \alpha_{t} x_{0} + \bar{\beta_{t}} ^ {2}\epsilon_{t}, \bar \alpha_{t} = \prod \alpha_{i}, \bar \beta_{t} = \sqrt{1-\bar \alpha_{t}^{2}}
$$
And the probability version:
$$
p(x_t|x_{0}) = \mathcal{N}(x_{t}; \bar \alpha_{t} x_{0},  \bar \beta_{t} ^ {2}I)
$$</p>
<p>Now that we have $x_t$ from $x_0$, update Eq 4 (since $\mathcal{N}$ can be represented as probabilities, the result is conformed to $\mathcal{N}$ as well):</p>

$$
p(x_{t-1}| x_{t},x_{0}) = \mathcal{N}\left(x_{t-1}; \underbrace{\frac{\alpha_{t}\bar \beta_{t-1}^{2}}{\bar \beta_{t}^{2}}x_{t} + \frac{\bar \alpha_{t-1}\beta_{t}^{2}}{\bar \beta_{t}^{2}}x_{0}}_{\text{$\tilde \mu_t(x_{t}, x_{0})$}},\frac{\bar \beta_{t-1}^{2}\beta_{t}^{2}}{\bar \beta_{t}^{2}}I\right)
$$

<p>Let&rsquo;s define the predicted mean value of $x_{t-1}$ as $\tilde \mu_t(x_{t}, x_{0}) = \frac{\alpha_{t}\bar \beta_{t-1}^{2}}{\bar \beta_{t}^{2}}x_{t} + \frac{\bar \alpha_{t-1}\beta_{t}^{2}}{\bar \beta_{t}^{2}}x_{0}$.</p>
<p>Notice the meaning of it: <strong>With Bayes&rsquo; Theorem, using $x_{t}$ and $x_{0}$, we can derive the mean value of $x_{t-1}$.</strong></p>
<p>So naturally, we can make our $\mu_{\theta}(x_{t}, t)$, who have the same estimated output as  $\tilde \mu_t(x_{t}, x_{0})$, learn the distribution of it:
$$
\mu_{\theta}(x_{t}, t) = \tilde \mu_t(x_{t}, x_{0})
$$</p>
<blockquote>
<p>[!NOTE] Different ways of modeling $\mu_\theta$ is also acceptable, it&rsquo;s just that this is a better way (or not)</p>
</blockquote>
<p>However, we don&rsquo;t have $x_0$ to pass to $\tilde \mu_t(x_{t}, x_{0})$.  Luckily, we can <strong>predict</strong> $x_0$ from rewriting Equation 7:
$$
x_{0}= \frac{x_{t} - \sqrt{1- \bar \alpha_{t}}}{\sqrt{\bar \alpha_{t}}}\epsilon_{t}
$$</p>
<blockquote>
<p>[!TIP]
This is actually an embodiment of the <a href="https://en.wikipedia.org/wiki/Predictor%E2%80%93corrector_method"><strong>predictor–corrector</strong></a> method</p>
</blockquote>
<p>Since we don&rsquo;t have $\epsilon$ in the reverse process, we can make a neural work to learn it: $\epsilon_\theta(x_t,t) \to \epsilon_t$ :
$$
x_{0}= \frac{x_{t} - \sqrt{1- \bar \alpha_{t}}}{\sqrt{\bar \alpha_{t}}}\epsilon_{\theta}(x_{t}, t)
$$</p>
<p>Update the Eq 10:
$$
\mu_{\theta}(x_{t}, t) = \tilde \mu_t(x_{t}, x_{0}) = \tilde \mu_t\left(x_{t}, \frac{x_{t} - \sqrt{1- \bar \alpha_{t}}}{\sqrt{\bar \alpha_{t}}}\epsilon_{\theta}(x_{t}, t)\right)= \frac{1}{\alpha_{t}}\left(x_{t} - \frac{\beta_{t}^2}{\bar\beta_{t}}\epsilon_{\theta}(x_{t},y, t)\right)
$$</p>
<h4 id="reverse-noise-predictor-sigma_thetax_tt">Reverse Noise Predictor: $\Sigma_{\theta}(x_{t},t)$</h4>
<p>It still remains to design $\Sigma_{\theta}(x_{t},t)$, since it encourages diversity.</p>
<p>The DDPM paper suggested not learning it, since it:</p>
<blockquote>
<p>resulted in unstable training and poorer sample quality</p>
</blockquote>
<p>By fixing it at some value $\Sigma_{\theta}(x_{t},t) = \sigma_{t}^{2}I$ , where either $\sigma_{t}^{2} = \beta_{t}$ or $\tilde{\beta_t}$ yielded similar performance.</p>
<h3 id="training--defining-loss">Training &amp; Defining Loss</h3>
<p>Conclusively, we have only defined one trainable model: $\epsilon_{\theta}(x_t, t)$</p>
<h4 id="to-reconstruct-x_0">To reconstruct $x_{0}$</h4>
<p>The training target can be <strong>MLE</strong>, the objective function being log-likelihood of reconstructing $x_{0}$:</p>

$$
\begin{align*}
\ln p(x_{0}) &= \int{\ln p(x_{0:T})dx_{1:T}} & \text{marginalization of marginal distribution}\\\\
&=  \ln \int{p(x_{0:T})dx_{1:T}}\\\\\\
&=  \ln \mathbb{E}_{q(x_{1:T}|x_{0})}[\int{\frac{p(x_{0:T})}{q(x_{1:T}|x_{0})}}]\\\\
&\ge \mathbb{E}_{q(x_{1:T}|x_{0})}\left[\ln \frac{p(x_{0:T})}{q(x_{1:T}|x_{0})}\right] & \text{Jensen Inequality of $\log$}\\\\
&= \underbrace{\mathbb{E}_{q(x_{1}|x_{0})}\left[\ln p_{\theta}({x_{0}|x_{1}})\right]}_{\text{reconstruction term}} - \sum_{t = 2}^{T}\mathbb{E}_{q(x_{t}|x_{0})}\left[D_{KL}(q(x_{t-1}|x_{t},x_{0})||p_{\theta}(x_{t-1}|x_{t}))\right]\\\\
&= \sum\limits_{t=1}^{T}  \gamma \mathbb{E}_{q(x_{t}|x_{0})}[||\epsilon_{t} - \epsilon_{\theta}(x_{t}, t) || ^{2}] & \text{$\gamma$ being some constants}
\end{align*}
$$

<h4 id="to-optimize-the-pixels">To optimize the pixels</h4>
<p>We design the loss function of $\theta$ as the <strong>Euclidean distance</strong> of the true and predicted mean of  $x_{t-1}$:
$$
\begin{align*}
\ell  &amp;= ||x_{t-1} - \hat x_{t-1}|| ^ 2 \newline
&amp;= ||x_{t-1} - \mu_\theta(x_{t},t)|| ^ 2 \newline
&amp;=|| (\frac{1}{\alpha_{t}}(x_{t} - \beta_{t}\epsilon_{t}) ) ^ {2} - \frac{1}{\alpha_{t}}(x_{t} - \beta_{t}\epsilon_{\theta}(x_{t}, t)) ^ {2}||\newline
&amp;= \frac{\beta_{t}^{2}}{\alpha_{t}^{2}} ||\epsilon_{t} - \epsilon_{\theta}(x_{t}, t) || ^2
\end{align*}
$$</p>
<h2 id="ddim">DDIM</h2>
<p>While the original <strong>DDPM</strong> is capable to generate satisfying images, it is known for poor performance: since the denoising(reverse) diffusion process usually take $T \sim 1000$ times of noise-prediction.</p>
<p><strong>DDIM</strong> is proposed to boost the reverse process as <strong>Non-Markovian Process</strong>, by directly taking any model trained on <strong>DDPM</strong> and sampling only  $T_{ddim}, T_{ddim} \le T$ timestamps, with some timestamps skipped. As a side-effect, the quality is compromised a little.</p>
<h3 id="reverse-process-1">Reverse Process</h3>
<p>It still takes the same approach as DDPM: predict $x_0$ from $x_t$ first.</p>
<p>From Eq 4, we can see that the sampling/training do involves $x_t$, but doesn&rsquo;t actually involves $p(x_{t-1}|x_{t})$ (which is defined in our reverse model). Instead, it defines:</p>
<p>$$
q_\sigma(x_{t-1}|x_{t}, x_0) = \mathcal{N}(x_{t-1};\sqrt{\bar \alpha_{t-1}}x_0 + \sqrt{1 - \bar \alpha_{t-1} - \sigma_t^2}\frac{x_t - \sqrt{\bar \alpha_t}x_0}{\sqrt{1 - \bar \alpha_t}}, \sigma_t^2I)
$$</p>
<p>Hence, the relation between $x_{t-1}$ and $x_t$ is:</p>
<p>
$$
x_{t-1} = \sqrt{\alpha_{t-1}} \underbrace{\left( \frac{x_t - \sqrt{1 - \alpha_{t}}\epsilon_{\theta}(x_{t},t)} {\sqrt{\bar \alpha_{t}}} \right)}_{\text{predicted $x_0$}} + 
\underbrace{\sqrt{1 - \bar \alpha_{t-1} - \sigma_{t}^{2}\epsilon_{t}(x_{t},t)}}_{\text{predicted  noise}} + \underbrace{\sigma_{t} \epsilon_{t}}_{\text{random noise}}
$$


where:
$$
\sigma_t = \eta \sqrt{(\frac{1 - \bar \alpha_t}{1 - \bar \alpha_{t-1}}) \left(1 - \frac{\bar \alpha_t}{\bar \alpha_{t-1}}\right)}
$$
where $\eta \in (0,1)$ is a constant, indicating the level of random noise:</p>
<ul>
<li>$\eta = 1$: The random noise is maximized, which is <strong>DDPM</strong>.</li>
<li>$\eta = 0$: The random noise is totally removed, making it a deterministic process/Implicit model, which is <strong>DDIM</strong>. It relies entirely on the predicted noise, while sacrificing some diversity with lowering random noise level.</li>
</ul>
<p>As for the timestamps chosen, they are determined empirically.</p>
<style type="text/css">.notice{--root-color:#444;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#c33;--warning-content:#fee;--info-title:#fb7;--info-content:#fec;--note-title:#6be;--note-content:#e7f2fa;--tip-title:#5a5;--tip-content:#efe}@media (prefers-color-scheme:dark){.notice{--root-color:#ddd;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#800;--warning-content:#400;--info-title:#a50;--info-content:#420;--note-title:#069;--note-content:#023;--tip-title:#363;--tip-content:#121}}body.dark .notice{--root-color:#ddd;--root-background:#eff;--title-color:#fff;--title-background:#7bd;--warning-title:#800;--warning-content:#400;--info-title:#a50;--info-content:#420;--note-title:#069;--note-content:#023;--tip-title:#363;--tip-content:#121}.notice{padding:18px;line-height:24px;margin-bottom:24px;border-radius:4px;color:var(--root-color);background:var(--root-background)}.notice p:last-child{margin-bottom:0}.notice-title{margin:-18px -18px 12px;padding:4px 18px;border-radius:4px 4px 0 0;font-weight:700;color:var(--title-color);background:var(--title-background)}.notice.warning .notice-title{background:var(--warning-title)}.notice.warning{background:var(--warning-content)}.notice.info .notice-title{background:var(--info-title)}.notice.info{background:var(--info-content)}.notice.note .notice-title{background:var(--note-title)}.notice.note{background:var(--note-content)}.notice.tip .notice-title{background:var(--tip-title)}.notice.tip{background:var(--tip-content)}.icon-notice{display:inline-flex;align-self:center;margin-right:8px}.icon-notice img,.icon-notice svg{height:1em;width:1em;fill:currentColor}.icon-notice img,.icon-notice.baseline svg{top:.125em;position:relative}</style>
<div><svg width="0" height="0" display="none" xmlns="http://www.w3.org/2000/svg"><symbol id="tip-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet"><path d="M504 256c0 136.967-111.033 248-248 248S8 392.967 8 256 119.033 8 256 8s248 111.033 248 248zM227.314 387.314l184-184c6.248-6.248 6.248-16.379 0-22.627l-22.627-22.627c-6.248-6.249-16.379-6.249-22.628 0L216 308.118l-70.059-70.059c-6.248-6.248-16.379-6.248-22.628 0l-22.627 22.627c-6.248 6.248-6.248 16.379 0 22.627l104 104c6.249 6.249 16.379 6.249 22.628.001z"/></symbol><symbol id="note-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet"><path d="M504 256c0 136.997-111.043 248-248 248S8 392.997 8 256C8 119.083 119.043 8 256 8s248 111.083 248 248zm-248 50c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"/></symbol><symbol id="warning-notice" viewBox="0 0 576 512" preserveAspectRatio="xMidYMid meet"><path d="M569.517 440.013C587.975 472.007 564.806 512 527.94 512H48.054c-36.937 0-59.999-40.055-41.577-71.987L246.423 23.985c18.467-32.009 64.72-31.951 83.154 0l239.94 416.028zM288 354c-25.405 0-46 20.595-46 46s20.595 46 46 46 46-20.595 46-46-20.595-46-46-46zm-43.673-165.346l7.418 136c.347 6.364 5.609 11.346 11.982 11.346h48.546c6.373 0 11.635-4.982 11.982-11.346l7.418-136c.375-6.874-5.098-12.654-11.982-12.654h-63.383c-6.884 0-12.356 5.78-11.981 12.654z"/></symbol><symbol id="info-notice" viewBox="0 0 512 512" preserveAspectRatio="xMidYMid meet"><path d="M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196 0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627 0 12 5.373 12 12v100h12c6.627 0 12 5.373 12 12v24z"/></symbol></svg></div><div class="notice tip" >
<p class="first notice-title"><span class="icon-notice baseline"><svg><use href="#tip-notice"></use></svg></span>Tip</p><p>In fact, $\eta$ represents the degree of moving some of the noise from predicted noise $\epsilon_t$ to sampled noise $\epsilon$: the bigger the $\eta$, the less deterministic, the larger random noise will be introduced to the reverse process.</p></div>
<h2 id="short-summary">Short Summary</h2>
<p>Conclusively, both models apply the same forward process, and have the same target: $x_{t} \to x_{0}$, though they have differences in the reverse process:</p>
<ul>
<li><strong>DDPM</strong> maximize the random noise, and in order to mitigate the negative effects it has,  takes more timestamps in the reverse process</li>
<li><strong>DDIM</strong> boost the performance by only selecting some of the timestamps, and reduce the random noise level</li>
</ul>
<h2 id="conditioned-generation">Conditioned Generation</h2>
<p>While being able to generate high quality images with reasonable speed with the models mentioned above, it is a common feature to generated <strong>conditioned output</strong>.</p>
<p>Given condition $y$, our goal is to derive $p(x_{t-1}|x_{t},y)$</p>
<h3 id="classifier-guided-diffusion">Classifier Guided Diffusion</h3>
<p>Using bayes&rsquo; rule, we have:
$$
p(x_{t-1}|x_{t},y) = \frac{p(x_{t-1}|x_{t})p(y|x_{t-1},x_{t})}{p(y|x_{t})}
$$</p>
<p>Using the notations in <a href="/posts/ai/models/denoising-diffusion-models/#reverse-process">Reverse Process</a>:
$$
p(x_{t-1}|x_{t},y) \propto \exp(-||x_{t-1}-\mu(x_{t})-\Sigma_{t}^{2}\underbrace{\nabla_{x_{t}}\log p(y|x_{t})}<em>{\text{classifier}}||^{2}/2\Sigma</em>{t}^{2})
$$</p>
<p>So $\mu_{\theta}(x_{t}, t,y) = \mu(x_{t})+\Sigma_{t}^{2}\nabla_{x_{t}}\log p(y|x_{t})), \Sigma_{t} = \sigma_{t}$</p>
<blockquote>
<p>[!NOTE]</p>
<ul>
<li>The gradient of the prob is easy to get with <em>autograd</em>, if the classifier can output the prob</li>
<li>The classifier guides the model only when inferencing</li>
</ul>
</blockquote>
<h3 id="classifier-free-diffusion">Classifier-Free Diffusion</h3>
<p>To infer without a classifier, we need to blend the condition $y$ into training process.</p>
<p>By directly modeling the conditioned reverse process as  $p(x_{t-1}|x_{t},y) = \mathcal{N}(x_{t-1};\mu(x_{t},y), \sigma_{t}^{2}I)$, following the modeling of Eq. 11, we have:
$$
\mu(x_{t}, y) = \frac{1}{\alpha_{t}}\left(x_{t} - \frac{\beta_{t}^2}{\bar\beta_{t}}\epsilon_{\theta}(x_{t},y, t)\right)
$$</p>
<p>The $\epsilon_{\theta}(x_{t},y, t)$ can be trained to predict the noise under condition.</p>
<blockquote>
<p>[!WARNING]
The conditioned noise predictor depends on $y$, so retraining is required if the $y$ is changed</p>
</blockquote>
<h2 id="score-based-generative-models">Score-based generative models</h2>
<table>
<thead>
<tr>
<th style="text-align:left">Notation</th>
<th style="text-align:left">Meaning</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Ancestral Sampling</td>
<td style="text-align:left">A sample method, auto-regressive</td>
<td></td>
</tr>
<tr>
<td style="text-align:left">Score Distillation Sampling</td>
<td style="text-align:left">A sampling method(sampler) to generate samples from a diffusion model by <strong>optimizing a loss function</strong>. Basically, it utilizes(distills) the score function of a teacher diffusion model, to train a larger model, with the final result as a sample (as $t \to 0$).</td>
<td></td>
</tr>
<tr>
<td style="text-align:left"><a href="https://en.wikipedia.org/wiki/Informant_(statistics)">score function</a></td>
<td style="text-align:left">The gradient of the <strong>log</strong>-likelihood function</td>
<td></td>
</tr>
<tr>
<td style="text-align:left">$U(x) = -\log q(x)$</td>
<td style="text-align:left">An energy function. The lower the likelihood, the higher the energy</td>
<td></td>
</tr>
<tr>
<td style="text-align:left">$\mu_{\theta}(x_t, t)$</td>
<td style="text-align:left">parameterized model to predict $x_{t-1}$ at time $t$</td>
<td></td>
</tr>
<tr>
<td style="text-align:left">$p(x_{0:T})$</td>
<td style="text-align:left">the joint distribution of $x_{0}, x_{1} &hellip; x_{T}$</td>
<td></td>
</tr>
<tr>
<td style="text-align:left"><a href="https://en.wikipedia.org/wiki/Metropolis-adjusted_Langevin_algorithm">Langevin dynamic</a></td>
<td style="text-align:left">A Markov chain Monte Carlo(MCMC) method for obtraining random samples</td>
<td></td>
</tr>
<tr>
<td style="text-align:left"><a href="/">Fisher Divergence</a></td>
<td style="text-align:left"></td>
<td></td>
</tr>
</tbody>
</table>
<p>Most often, we don&rsquo;t care about the probability $q(x)$ of a certain input $x$, but how it changes through time: therefore, we can utilize score function(gradient, changes) $s(x):=\nabla_{x}\log q(x)$</p>
<blockquote>
<p>[!TIP]
It is also an advantage of modeling the <strong>score</strong>: don&rsquo;t have to make sure <strong>probability</strong> sum up to 1</p>
</blockquote>
<p>With $s(x)$ allowing us to sample from $q(x)$ using thermodynamics, our goals changes to: <strong>model $q(x)$</strong></p>
<p>$$
dx_{t} = \nabla \log q(x) d_{t} + d{W_{t}}
$$</p>
<h3 id="loss">Loss</h3>
<p>We learn a model $s_{\theta}$ to <strong>match</strong>(approximate) the <strong>score</strong> $\nabla \log q$:</p>
<p>$$
s_{\theta} \approx \nabla \log q(x)
$$
&ndash; This is score matching.</p>
<p>Typically, score matching is formalized as minimizing <strong>Fisher divergence</strong> function . By expanding the integral, and performing an integration by parts, we have our loss function:
$$
\mathcal{L} = \mathbb{E}<em>{q}[||s</em>{\theta}(x) - \nabla \log q(x)||^{2}]
$$
However, it&rsquo;s infeasible since it requires access to unknown score  $\nabla \log q(x)$.</p>
<p>Fortunately, we have <strong>score matching</strong> techniques(e.g. <a href="https://en.wikipedia.org/wiki/Scoring_rule#Hyv%C3%A4rinen_scoring_rule" title="Scoring rule">Hyvärinen scoring rule</a>) which minimize the Fisher divergence without knowledge of the gorund-truth score:
$$
\mathcal{L} = \mathbb{E}<em>{q}\left[\nabla</em>{x} s_{\theta}(x)+ \frac{1}{2}||s_{\theta}(x)||_{2}^{2}\right]
$$</p>
<p>Since $s_{\theta}$ is modeled by ourself, its output and gradients can be easily calculated. We use Monte-Carlo methods with gradient descent to optimize it.</p>
<h3 id="sample--inference">Sample / Inference</h3>
<p>But how do we generate a sample.</p>
<p>Once we have trained a score-based model $s_{\theta}(x)$, we can use an iterative procedure called <strong><a href="https://en.wikipedia.org/wiki/Metropolis-adjusted_Langevin_algorithm">Langevin dynamics</a></strong> to draw samples from it:
$$
x_{i + 1} \leftarrow x_{i} + \epsilon \nabla_{x} \log p(x) + \sqrt{2\epsilon} z_{i}, z_{i} \sim \mathcal{N}(0, I)
$$
Notice some white noise is injected, to avoid all samples collapse into some limited local optimas.</p>
<p>This seems decent, but in fact: in low-density regions, the estimated scores are inaccurate.</p>
<p>It&rsquo;s natural to augment the low-density regions by perturbing our datapoint: injecting $\mathcal{N}$. It can solve the problem in low-density, however since the training data is perturbed, the generated samples are too.</p>
<p>Multiple (decreasing) noise levels $\sigma$ are applied as an input to score funcion $s$, with the output of previous model $i$ as the input of the next model $i+1$. The whole process resembles an <strong>Anneald Langevn Dynamics</strong></p>
<h2 id="sde">SDE</h2>
<table>
<thead>
<tr>
<th style="text-align:left">Notation</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><a href="https://en.wikipedia.org/wiki/Stochastic_differential_equation">SDE</a></td>
<td style="text-align:left">A DE in which one or more of the terms is a <strong>stohastic</strong> process</td>
</tr>
<tr>
<td style="text-align:left">$\mathcal{U}(T_{a},T_{b})$</td>
<td style="text-align:left">Uniform distribution over the time interval $[T_{a}, T_{b}]$</td>
</tr>
<tr>
<td style="text-align:left">In <a href="/posts/ai/models/denoising-diffusion-models/#ddpm">DDPM</a>, we define $t$ as <em>discrete</em> timestamps, however it&rsquo;s more natural to model it as <em>continuous</em> time.</td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="forward-process-1">Forward Process</h3>
<p>With this premise, we model the forward process with <strong>Stochastic</strong> DE(Differential equation), but not funtion on timestamps:
$$
dx = f_{t}(x) + g_{t}dw
$$</p>
<h3 id="reverse-process-2">Reverse Process</h3>
<p>Similarly, we want to model $p(x_{t}|x_{t + \Delta{t}})$:
$$
\begin{align}
p(x_{t}|x_{t + \Delta{t}}) &amp;= \frac{p(x_{t + \Delta_{t}} | x_{t})p(x_{t})}{p(x_{t+ \Delta{t}})} \\
&amp;= p(x_{t + \Delta{t}} | x_{t})\exp(\log p(x_{t}) - \log p(x_{t+\Delta{t}})) \\
&amp;\propto \left(-\frac{||x_{t + \Delta_{t}} - x_{t} - f_{t}(x_{t})\Delta{t}||^{2}}{2g_{t}^{2}\Delta t}+  \log p(x_{t}) - \log p(x_{t+\Delta{t}})\right)
\end{align}
$$</p>
<p>In order to calculate the unknown diff, we apply <strong>Taylor expansion</strong>:
$$
\log p(x_{t+\Delta{t}}) \approx \log p(x_{t}) + (x_{t+\Delta t} - x_{t}) \cdot \nabla_{x_{t}}\log p(x_{t}) + \underbrace{\Delta t \frac{\partial \log p(x_{t})}{\partial t}}<em>{\text{$x</em>{t}$&rsquo;s deritive of $t$}}
$$</p>
<p>Update Equation 26-3 with it, we have:
$$
p(x_{t}|x_{t + \Delta{t}})  \sim \mathcal{N}(f_{t+\Delta t}(x_{t + \Delta t}) - g_{t + \Delta t}^{2}\nabla_{x_{t + \Delta t}} \log p(x_{t + \Delta t})\Delta t; g_{t + \Delta t}^{2}\Delta t I)
$$</p>
<p>and the SDE of <strong>reverse process</strong>:
$$
dx = [f_{t}(x) - g_{t}^{2}\nabla_{x}\log p_{t}(x)]dt + g_{t}dw
$$</p>
<h3 id="training">Training</h3>
<p>$$
\mathcal{L} = \mathbb{E}<em>{t \in \mathcal{U}(0, T)}\mathbb{E}</em>{p_{t}(x)}[\lambda(t)||\nabla_{x}\log p_{t}(x) - s_{\theta}(x,t)||^{2}_{2}]
$$
where:</p>
<ul>
<li>$\lambda : \mathbb{R} \to \mathbb{R}_{&gt;0}$  is a positive weighting function</li>
</ul>
<h2 id="probability-flow-ode">Probability flow ODE</h2>
<table>
<thead>
<tr>
<th style="text-align:left">Notation</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">PF(Probability flow) ODE</td>
<td style="text-align:left">The ODE of an SDE</td>
</tr>
</tbody>
</table>
<p>Despite capable of generating high-quality samples, <em>samplers</em> based on Langevin MCMC and SDE solvers do not provide a way to compute the exact log-likelihood of score-based generative models.</p>
<p>It has been proved that, it is possible to convert any SDE into an ODE(ordinary differential equation) without changing its marginal distributions $p_{t}(x)$</p>
<h3 id="forward-process-2">Forward Process</h3>
<p>With a sequence of complex calculations(including F-P function &amp; Dirac function), we have:</p>
<p>$$
dx = [f(x,t) - \frac{1}{2}(g^{2}(t)-\sigma_{t}^{2})\nabla_{x}\log p_{t}(x)]dt
$$</p>
<h3 id="reverse-process-3">Reverse Process</h3>
<p>The reverse process of PF-ODE is given by:</p>
<p>$$
dx = [f(x,t) - \frac{1}{2}g^{2}(t)\nabla_{x}\log p_{t}(x)]dt
$$</p>
<blockquote>
<p>[!TIP]
When $\nabla_{x}\log p_{t}(x)$ replaces $s_{\theta}(x,t)$, PF ODE becomes a special case of a neural ODE</p>
</blockquote>
<h2 id="samplers">Samplers</h2>
<h3 id="euclidean">Euclidean</h3>
<p>$$
\begin{equation}\left.\frac{d\boldsymbol{x}<em>t}{dt}\right|</em>{t=t_{n+1}}\approx \frac{\boldsymbol{x}<em>{t</em>{n+1}} - \boldsymbol{x}<em>{t_n}}{t</em>{n+1} - t_n}\end{equation}
$$
一阶近似</p>
<h3 id="heun-solver">Heun solver</h3>
<h3 id="dpm-solver">DPM solver</h3>
<h3 id="amed-solver">AMED solver</h3>
]]></content:encoded>
    </item>
    
    <item>
      <title>VAE</title>
      <link>https://mickjagger19.github.io/posts/ai/models/vae/</link>
      <pubDate>Sun, 14 Jan 2024 14:05:03 +0800</pubDate>
      
      <guid>https://mickjagger19.github.io/posts/ai/models/vae/</guid>
      <description>Takeaways from the maths of VAE</description>
      <content:encoded><![CDATA[<h2 id="terminology">Terminology</h2>
<table>
<thead>
<tr>
<th>Notations</th>
<th>Mean</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>$X \sim p_{r}$</td>
<td>the input data</td>
<td></td>
</tr>
<tr>
<td>$z$</td>
<td>the encoded latent</td>
<td></td>
</tr>
<tr>
<td>$\theta$</td>
<td>the parameterized model</td>
<td></td>
</tr>
<tr>
<td>$\phi$</td>
<td>the encoder</td>
<td></td>
</tr>
<tr>
<td>$p_{\theta}(x)$</td>
<td>the likelihood of the data-reconstruction</td>
<td></td>
</tr>
<tr>
<td>$p(z)$</td>
<td>the distribution of latent variable $z$ as a prior, often $\mathcal{N}(0,1)$</td>
<td></td>
</tr>
<tr>
<td>$q_{\phi}(z|x)$</td>
<td>variational distribution</td>
<td></td>
</tr>
<tr>
<td>$q_{\phi}(z|x)$</td>
<td>variational distribution</td>
<td></td>
</tr>
<tr>
<td><strong>MDL</strong>(Minimum Description Length)</td>
<td></td>
<td></td>
</tr>
<tr>
<td><a href="https://en.wikipedia.org/wiki/Information_content">Self-Information $I$</a></td>
<td>the amount of information, interpreted as the level of &ldquo;surprise&rdquo;<br>$$I(\mathcal{w}<em>{n}) = f(P(\mathcal{w}</em>{n})) = -\log(P(\mathcal{w}_{n})) \ge 0$$</td>
<td></td>
</tr>
<tr>
<td>Entropy $H(X)$</td>
<td>the average amount of information in a message. A measure of <strong>uncertainty</strong>. <br>$$H(X) = E[I(X)] = E[-\ln(P(X))]$$<br></td>
<td></td>
</tr>
</tbody>
</table>
<h2 id="background">Background</h2>
<p><a href="https://en.wikipedia.org/wiki/Autoencoder"><strong>AutoEncoder</strong></a> is proposed to compress data and reduct dimensionality as a generalization of PCA, and largely used in <strong>signal processing</strong>, until someone found new samples can be generated by adding noise to latents and decoded by decoder.</p>
<p>However, the ability of AutoEncoder to generate new samples by the distribution of the latents $z$, this is why &amp; when <strong>Variational AutoEncoder</strong> is developed.</p>
<blockquote>
<p>[!TIP]
AE is an approach of <strong>MDL</strong></p>
</blockquote>
<h2 id="requirements">Requirements</h2>
<ul>
<li>In order to be able to generate new samples using decoder, we will be happy if $z \sim \mathcal{N}(0, 1)$</li>
</ul>
<h2 id="modeling">Modeling</h2>
<p>We apply <strong>Maximum Likelihood Estimation</strong> here.</p>
<p><strong>Log Likelihood</strong> is defined as:
$$
Likelihood = \log P_{\theta}(X)
$$</p>
<p>which represents the ability of the model to reconstruct the input data.</p>
<p>Hence, from the definition of the loss function:</p>
<p>$$
\mathcal{L}(\theta) = - \mathbb E_{x \sim data}[\log p_{\theta}(x)]
$$</p>
<p>Normally, the $x\sim data$ is neglected.</p>
<p>Our goal is to minimize the loss function, in the mean time force encoder to encode $X$ as $z \sim \mathcal{N}(\mu, \sigma^{2}I)$</p>
<h3 id="implicit-model">Implicit Model</h3>
<p>We define $z$ as an implicit variable, making our model an <strong>implicit model</strong>.</p>
<p>Rewrite the log-likelihood:
$$
p_{\theta}(x) = \int{p_{\theta}}(x|z)p_{\theta }(z)dz
$$
where $\theta$ is the parameter of the implicit model (encoder and decoder).</p>
<p>However there&rsquo;s a common problem for implicit models: the integration relies on the exhaustion on implicit variable $z$.</p>
<p>In our case, as $z \sim \mathcal{N}(\mu, \sigma^{2}I)$, it is deem impossible.</p>
<h3 id="mc">MC</h3>
<p>Monte-Carlo is a method to approximate an intractable <del>equation</del>(integration) by sampling a lot of data ($p_{\theta}(x | z)$):
$$
\begin{align*}
p_{\theta}(x) &amp;= \int{p_{\theta}}(x|z)p_{\theta }(z)dz\\
&amp;\approx \frac{1}{m} \sum\limits_{j =1}^{m} p_{\theta}(x | z_{j})
\end{align*}
$$
But that does not enforce  $z \sim \mathcal{N}(\mu, \sigma^{2}I)$.</p>
<h3 id="variational-bayes">Variational Bayes</h3>
<h4 id="deriving-elbo">Deriving ELBO</h4>
<p>Considering the log-likelihood can be rewritten in the following process:</p>
<p>$$
\begin{align*}
\log p_{\theta}(x) &amp;= \log p_{\theta}(x) \int_{z}p_{\phi}(z|x)dz &amp;\text{Normalization}
\\
&amp;= \int_{z}p_{\theta}(z|x)\log p_{\theta}(x)dz
\\
&amp;=  \int_{z}p_{\theta}(z|x) \log \frac{p_{\theta}(x,z)}{p(z|x)} dz  &amp;\text{Bayes&rsquo; Theorem}
\\
&amp;= \int_{z}(p_{\theta}(z|x)\log p_{\theta}(x,z) - p_{\theta}(z|x)\log p(z|x))dz
\\
&amp;= \log p_{\theta}(x,z) - \log p_{\theta}(z|x)
\end{align*}
$$</p>
<p>Since the posterior $\log p_{\theta}(z|x)$ is intractable (only involves integration on latent variable $z$, see <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes&rsquo; Theorem</a>), a new distribution(which is easy to <em>learn</em>) $q_{\phi}(z|x)$ is used to approximate it, where $\phi$ is the encoder.</p>
<p>Let&rsquo;s continue by replacing:</p>

$$
\begin{align*}
\underbrace{\log p(x)}_{\text{evidence}} &= \log p_\theta(x,z) - \log q_{\phi}(z|x) \newline
&= \int_{z} q_{\phi}(z|x)\log\frac{p_{\theta}(x,z)}{q_{\phi}(z|x)}dz \newline
&= \int_{z}q_{\phi}(z|x)\log(\frac{p_{\theta}(x,z)}{q_{\phi}(z|x)} \cdot  \frac{q_{\phi}(z|x)}{p(z|x)})dz \newline
&= \int_{z}q_{\phi}(z|x)\log(\frac{p_{\theta}(x,z)}{q_{\phi}(z|x)})dz + \int_{z}q_{\phi}(z|x)\log(\frac{q_{\phi}(z|x)}{p(z|x)})dz \newline
&= \mathcal L(\theta,\phi; x) + D_{KL}(q_{\phi}, p_{\theta}) \newline
&\ge \underbrace{\mathcal L(\theta,\phi; x)}_{\text{ ELBO }} & \text{$D_{KL}\ge 0$} 
\end{align*} 
$$

<p>$\mathcal L(\theta, \phi; x) = \mathbb{E}_{z \sim q(.|x)}{\log \frac{p_{\theta}(x,z)}{q_{\phi}(z|x)}}$  is <strong>ELBO</strong>(Evidence Lower Bound), as it is the lower bound of evidence $\mathcal{L}(\theta)$, omitting the <strong>KL</strong> term. Maximizing ELBO is directly:</p>
<ul>
<li>maximizing log-likelihood</li>
<li>minimizing KL-Divergence of posterior $p_{\theta}$ and variational distribution $q_{\phi}$</li>
</ul>
<h4 id="maximizing-elbo">Maximizing ELBO</h4>
<p>And we can break it down further:

$$
\begin{align*}
\underbrace{\mathcal L(\theta, \phi; x)}_{\text{ELBO}} &= \int_{z}q_{\phi}(z|x)\log(\frac{p_{\theta}(x,z)}{q_{\phi}(z|x)})dz = \mathcal{H}[q_{\phi}(z|x)] + \mathbb{E}_{z}[p_{\theta}(x,z)] \\\\
&= \int_{z}q_{\phi}(z|x)\log(\frac{p(z) * p_{\theta}(x|z) }{q_{\phi}(z|x)})dz & \text{Bayes' Theorem}\\\\
&= \int_{z}q_{\phi}(z|x)\log\frac{p(z) }{q_{\phi}(z|x)}dz + \int_{z}q_{\phi}(z|x)\log p_{\theta}(x|z)dz\\\\ 
&= \underbrace{-D_{KL}(q_{\phi}(z|x), p(z))}_{\text{$\mathcal L_{reg}$}} + \underbrace{\mathbb E_{q_{\phi}(z|x)}[\log p_{\theta}(x|z)]}_{\text{$\mathcal L_{reconstruct}$}}\\\\ 
\end{align*}
$$
</p>
<blockquote>
<p>[!Note]
$\int_{z}p(z)*f(z)dz = \mathbb E_{z \sim p(.)}[f(z)]$, which is the expectation of p with z sampled from $p(z)$</p>
</blockquote>
<p>This is ELBO:</p>
<ul>
<li>$\mathcal L_{reg}$: the KL-divergence of variational distribution and prior distribution</li>
<li>$\mathcal L_{reconstruct}$: the Expectation of log reconstruct-likelihood under <em>variational distribution</em></li>
</ul>
<p>Since $\mathcal{L}(\theta) = -\log p(x) \le - \text{ELBO}$, by maximizing ELBO, we can indirectly minimize $L(\theta)$.</p>
<p>Hence, we define $\mathcal{L} = -\text{ELBO}$.</p>
<h3 id="training">Training</h3>
<p>$$
\begin{align*}
\text{ELBO} &amp;= \underbrace{-D_{KL}(q_{\phi}(z|x), p(z))}<em>{\text{$\mathcal L</em>{reg}$}} + \underbrace{\mathbb E_{q_{\phi}(z|x)}[\log p_{\theta}(x|z)]}<em>{\text{$\mathcal L</em>{reconstruct}$}}\\
&amp;= \underbrace{-D_{KL}(q_{\phi}(z|x), p(z))}<em>{\text{$\mathcal L</em>{reg}$}} + MSE(x, \hat x)
\end{align*}
$$</p>
<p>As $z$  is <strong>sampled</strong> from $\sim q_{\phi}(z|x)$, which is a variational distribution, the gradient of ELBO will not be able to propagate back to encoder $\phi$ (in-differentiable, chain rule).</p>
<p>Thus, <strong>re-parameterization</strong> is applied: $z = \mu + \epsilon \times \sigma, \hat z \sim \mathcal{N}(0, I)$, where $\phi(X) = (\mu, \epsilon)$. This way, the gradient is passed back to $\phi$, by representing $z$ with the output of $\phi$, where $z$ participates in the loss-calculation</p>
<h2 id="problems">Problems</h2>
<h3 id="blurry-output">Blurry output</h3>
<ul>
<li>the prior: $p(z) \sim \mathcal{N}(0, I)$</li>
<li>MSE is used to measure $L_{reconstruct}$</li>
<li></li>
</ul>
<p>DAE:
corrupt X，降低图片的冗余度（图片的冗余性一般都很高）</p>
<h2 id="dall-e">Dall E</h2>
<p>两阶段：</p>
<ol>
<li>clip 构造对比学习的正负样本对</li>
<li>文本 -&gt; clip encoder -&gt; text embedding -&gt; (diffusion) prior -&gt; image embedding -&gt; diffusion model decoder -&gt; image</li>
</ol>
<p>transformer encoder 本质上是自回归模型，可以基于自注意力和输入，自回归地生成同类型的内容</p>
<p>![[Pasted image 20230618153050.png]]</p>
<p>![[Pasted image 20230618154911.png]]</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Techniques of Training NNs</title>
      <link>https://mickjagger19.github.io/posts/ai/techniques-of-training-nns/</link>
      <pubDate>Fri, 12 Jan 2024 20:22:11 +0800</pubDate>
      
      <guid>https://mickjagger19.github.io/posts/ai/techniques-of-training-nns/</guid>
      <description>Describe and compare some common tricks of training NNs</description>
      <content:encoded><![CDATA[<h2 id="regularization">Regularization</h2>
<p>For most <strong>NNs</strong> today, they are trained to minimize a designed loss function, which quantizes the ability of the model.</p>
<p>Though being intuitive and reasonable, after a model is trained for a long time, a phenomenon called <strong>overfitting</strong> occurs. The model performed so well in the training set, that it behaves badly with more generalized data - whose distribution may differ from the training set.</p>
<p>There are several tricks we can adopt in training, to mitigate overfitting.</p>
<p>They are often known as <strong>regularization</strong></p>
<h3 id="weight-penalty">Weight Penalty</h3>
<p>The measurement of complexity:</p>
<ul>
<li>Number of parameter</li>
<li>Absolute value of parameters</li>
</ul>
<h3 id="dropout">Dropout</h3>
<p><strong>Dropout</strong> works in the training phase:</p>
<ol>
<li>In each batch, <strong>randomly</strong> disable some neurons with ratio <em>k</em>. The output of these deleted neurons became 0.</li>
<li>Multiply the output with <em>k</em>: $o = o * k$ to rectify</li>
</ol>
<p>The reason behind <strong>dropout</strong>:</p>
<h4 id="average">Average</h4>
<p>It can be interpreted as a special case of <strong>ensemble</strong> , taking the average of output of expertises(the active neurons)</p>
<h4 id="reduce-co-relation-between-neurons">Reduce Co-relation between neurons</h4>
<h4 id="evolution-theory">Evolution theory</h4>
<p>Consider the <strong>disable action</strong> as the environment mutation, the model adapts to this mutation(new environment) by breeding new breeds(the different combination of active neurons)</p>
<h4 id="variant-vanilla-dropout">Variant: vanilla dropout</h4>
<h2 id="normalization">Normalization</h2>
<table>
<thead>
<tr>
<th style="text-align:left">Notation</th>
<th style="text-align:left">Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">$B$</td>
<td style="text-align:left">Batch size</td>
</tr>
<tr>
<td style="text-align:left">$S$</td>
<td style="text-align:left">Sequence length</td>
</tr>
<tr>
<td style="text-align:left">$$</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
</tr>
</tbody>
</table>
<h3 id="batch-normalization">Batch Normalization</h3>
<p>Normalization across the $B$ dimension:</p>
<h3 id="layer-normalization">Layer Normalization</h3>
<h3 id="group-normalization">Group Normalization</h3>
<p>tan h: 当 |x| 比较小 时，接近线性网络</p>
<p>手段：</p>
<ul>
<li>向损失函数中引入正则项，将模型参数的范数作为惩罚</li>
<li>Dropout: 训练时，随机屏蔽（置0）并对结果进行 scale
<ul>
<li>原理：Don&rsquo;t rely on any one feature(neuron), spread out weights</li>
<li>随机淘汰网络中的一些单元，因此训练时，每层不会对任意一个神经元施加 <strong>太多权重</strong>，这种分散权重的方式防止了过拟合</li>
<li>迫使同层节点对输出承担或多或少的责任，增强模型的泛化性，因为它不会太依赖某些局部的特征</li>
</ul>
</li>
<li>Data augmentation
<ul>
<li>增加数据集的泛化性</li>
</ul>
</li>
<li>Early stopping
<ul>
<li>在权重 overfit 之前结束，但 error 较高</li>
</ul>
</li>
<li></li>
</ul>
<h2 id="normalize">Normalize</h2>
<p>对于随机输入数据，会导致<strong>成本函数</strong>对每个维度的scale 不一致(elongated)，导致参数调整出现很多 oscillate</p>
<ul>
<li>z-score</li>
<li>min-max
![[Attachments/AI/Techniques of Training NNs/IMG-20240121133309891.png]]</li>
<li>softmax: normalized exponential function</li>
</ul>
<h3 id="batch-norm">Batch Norm</h3>
<p>对<strong>每一批</strong>训练数据进行归一化，</p>
<p>effect:</p>
<ul>
<li><strong>reduce shift</strong> on train data</li>
<li>增加模型的鲁棒性，提升训练速度</li>
</ul>
<p>在 z/a 上作用</p>
<p>why work?</p>
<ul>
<li>learning on shifting input distribution: 偏移在神经网络中会产生累积，导致后面的层的输入不稳定</li>
</ul>
<h3 id="layer-norm">Layer Norm</h3>
<p>不是全量数据集，因为容易过拟合</p>
<h2 id="bias">Bias</h2>
<p>Inductive bias</p>
]]></content:encoded>
    </item>
    
    <item>
      <title>Bossa Nova Songs Progression</title>
      <link>https://mickjagger19.github.io/posts/music/bossa-nova-songs-progression/</link>
      <pubDate>Sat, 06 Jan 2024 21:15:22 +0800</pubDate>
      
      <guid>https://mickjagger19.github.io/posts/music/bossa-nova-songs-progression/</guid>
      <description>Chord progression of some well-known bossa-nova songs</description>
      <content:encoded><![CDATA[<script src="/js/opensheetmusicdisplay.min.js">

</script>
<script>

function load_and_render(path, div_id) {
    const osmd = new opensheetmusicdisplay.OpenSheetMusicDisplay(div_id);
    osmd.setOptions({defaultFontFamily: "Petaluma Script"});

    osmd.load(path).then(function() {
      osmd.setOptions({defaultFontFamily: "Petaluma Script"});
      osmd.EngravingRules.DefaultColorCursor = "currentColor";
      osmd.EngravingRules.DefaultFontFamily = "Petaluma Script";
      osmd.EngravingRules.RenderRehearsalMarks = false;
      osmd.EngravingRules.RenderTitle = false;
      osmd.EngravingRules.RenderLyricist = false;
      osmd.EngravingRules.RenderChordSymbols = true;
      osmd.EngravingRules.setChordSymbolLabelText(opensheetmusicdisplay.ChordSymbolEnum.diminishedseventh, "");

      osmd.render();
    });
}
</script>
<link rel="stylesheet" href="/css/main.css">
<h1 id="desafinado">Desafinado</h1>
<div class="osmd" id="osmd-container-Desafinado"></div>
<script>
load_and_render("/MusicXML/desafinado_musescore.musicxml", "osmd-container-Desafinado");
</script>
<iframe width="560" height="315" src="https://www.youtube.com/embed/oqHONL-LZ58?si=8qLHfRC5OjhtQejx" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<iframe width="560" height="315" src="https://www.youtube.com/embed/IWoOWpZujCc?si=rBlI_sTWn10EKodQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h1 id="one-note-samba">One Note Samba</h1>
<div class="osmd" id="osmd-container-One Note Samba"></div>
<script>
load_and_render("/MusicXML/One Note Samba.musicxml","osmd-container-One Note Samba");
</script>
<iframe width="560" height="315" src="https://www.youtube.com/embed/ptaqr5bZ2gA?si=FMcmszaGiuxOZs1Q" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h1 id="só-danço-samba">Só Danço Samba</h1>
<div class="osmd" id="osmd-container-So Danco Samba"></div>
<script>
load_and_render("/MusicXML/So Danco Samba.musicxml", "osmd-container-So Danco Samba");
</script>
<iframe src="https://www.soundslice.com/slices/P1jMc/embed-channelpost/" width="100%" height="320" frameBorder="0"></iframe>
<iframe width="560" height="315" src="https://www.youtube.com/embed/YJMEu1oxLjA?si=mu0rvLSbTI934dfM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<iframe width="560" height="315" src="https://www.youtube.com/embed/f6e6W304LhA?si=CUY1PaBZd3fY3c5H" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h1 id="doralice">Doralice</h1>
<div class="osmd" id="osmd-container-Doralice"></div>
<script>
load_and_render("/MusicXML/Doralice.musicxml", "osmd-container-Doralice");
</script>
<iframe src="https://www.soundslice.com/slices/1jkyc/embed-channelpost/" width="100%" height="320" frameBorder="0"></iframe>
<iframe width="560" height="315" src="https://www.youtube.com/embed/hU3IUUIZxEg?si=BuuJFM-MI6ZFFb-w" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h1 id="o-pato">O Pato</h1>
<div class="osmd" id="osmd-container-O Pato"></div>
<script>
load_and_render("/MusicXML/O Pato.musicxml", "osmd-container-O Pato");
</script>
<iframe src="https://www.soundslice.com/slices/dHYMc/embed-channelpost/" width="100%" height="320" frameBorder="0"></iframe>
<iframe width="560" height="315" src="https://www.youtube.com/embed/jkBk8U2ikf8?si=S6-IWYlE6gFjPyWY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h1 id="chega-de-saudade-no-more-blues">Chega De Saudade (No More Blues)</h1>
<div class="osmd" id="osmd-container-Chega De Saudade (No More Blues)"></div>
<script>
load_and_render("/MusicXML/Chega De Saudade (No More Blues).musicxml", "osmd-container-Chega De Saudade (No More Blues)");
</script>
<iframe width="560" height="315" src="https://www.youtube.com/embed/c6wlgDiyrLQ?si=SFnmysADjupwS56B" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h1 id="aguas-de-março">Aguas De Março</h1>
<iframe src="https://www.soundslice.com/slices/QSHcc/embed-channelpost/" width="100%" height="320" frameBorder="0"></iframe>
<h1 id="menina-flor">Menina Flor</h1>
<iframe src="https://www.soundslice.com/slices/DPLMc/embed-channelpost/" width="100%" height="320" frameBorder="0"></iframe>
<h1 id="corcovado">Corcovado</h1>
<iframe src="https://www.soundslice.com/slices/3Rbcc/embed-channelpost/" width="100%" height="320" frameBorder="0"></iframe>
]]></content:encoded>
    </item>
    
    <item>
      <title>Music of George Harrison</title>
      <link>https://mickjagger19.github.io/posts/music/music-of-george-harrison/</link>
      <pubDate>Sat, 06 Jan 2024 19:37:13 +0800</pubDate>
      
      <guid>https://mickjagger19.github.io/posts/music/music-of-george-harrison/</guid>
      <description>personal rankings of George Harrison&amp;rsquo;s Singles/Albums/Guitar Solos</description>
      <content:encoded><![CDATA[<link rel="stylesheet" href="/css/main.css">
<h1 id="single">Single</h1>
<h2 id="1-something">1. Something</h2>
<iframe width="560" height="315" src="https://www.youtube.com/embed/UelDrZ1aFeY?si=PaPM8YoThkbpquuE" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h2 id="2-here-comes-the-sun">2. Here Comes the Sun</h2>
<iframe width="560" height="315" src="https://www.youtube.com/embed/KQetemT1sWc?si=SmPBWEga3ou7gDNM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h2 id="3-while-my-guitar-gently-weeps">3. While My Guitar Gently Weeps</h2>
<iframe width="560" height="315" src="https://www.youtube.com/embed/VJDJs9dumZI?si=XMjvBDs71NjqWrjh" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h2 id="4-all-those-years-ago">4. All Those Years Ago</h2>
<iframe width="560" height="315" src="https://www.youtube.com/embed/eNL40ql4CYk?si=fuAQaEGwfqsm9yvU&amp;start=144" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h2 id="5-wah-wah">5. Wah-Wah</h2>
<iframe width="560" height="315" src="https://www.youtube.com/embed/NDVAQE7nplU?si=Ydjz3KdqNk_kqStL" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h1 id="album">Album</h1>
<p>Beatles records are excluded</p>
<h2 id="1-all-things-must-pass">1. All Things Must Pass</h2>
<h2 id="2-brainwashed">2. Brainwashed</h2>
<h2 id="3-george-harrison">3. George Harrison</h2>
<h2 id="4--living-in-the-material-world">4.  Living in the Material World</h2>
<h1 id="guitar-solo">Guitar Solo</h1>
<h2 id="1-dark-sweet-lady">1. Dark Sweet Lady</h2>
<iframe width="560" height="315" src="https://www.youtube.com/embed/Mqq_yEJlwDQ?si=gxQtr_L-y7tWevCx&amp;start=101" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h2 id="2-something">2. Something</h2>
<iframe width="560" height="315" src="https://www.youtube.com/embed/UelDrZ1aFeY?si=xUAGs_4t-_ddr0Ag&amp;start=98" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h2 id="3-the-light-that-has-lighted-the-world">3. The Light That Has Lighted the World</h2>
<iframe width="560" height="315" src="https://www.youtube.com/embed/Tcg_19WGPJs?si=_oKNGBCOx8naUBja&amp;start=82" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h2 id="4-give-me-love">4. Give me Love</h2>
<iframe width="560" height="315" src="https://www.youtube.com/embed/WMof20FwaOs?si=nveFJrXxggUvED6h&amp;start=110" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h2 id="5-any-road">5. Any Road</h2>
<iframe width="560" height="315" src="https://www.youtube.com/embed/r8fFdc-karA?si=FJcZcUzybW0JodEd&amp;start=210" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h2 id="6-cheer-down">6. Cheer Down</h2>
<iframe width="560" height="315" src="https://www.youtube.com/embed/Yk-5iu1QE0E?si=Ist4VWRXwviqgjM1&amp;start=144" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h2 id="7-stuck-inside-a-cloud">7. Stuck Inside a Cloud</h2>
<iframe width="560" height="315" src="https://www.youtube.com/embed/m1_zW9erCCM?si=0AlEcpq2-MykneL5&amp;start=111" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h2 id="8-crippled-inside">8. Crippled Inside</h2>
<iframe width="560" height="315" src="https://www.youtube.com/embed/EX7roXRmkOU?si=pmsqtfpwrlSk8zo0&amp;start=77" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h2 id="9-how-do-you-sleep">9. How Do You Sleep?</h2>
<iframe width="560" height="315" src="https://www.youtube.com/embed/teD9t-lO_o0?si=Vt8mZOYRjDYVdtav&amp;start=157" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h2 id="10-isnt-it-a-pity">10. Isn&rsquo;t it a Pity</h2>
<iframe width="560" height="315" src="https://www.youtube.com/embed/-n-LULDiJxk?si=nsReB-tVfznad_mC&amp;start=125" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<h2 id="11-rising-sun">11. Rising Sun</h2>
<iframe width="560" height="315" src="https://www.youtube.com/embed/BW4plsNcKQo?si=k-eBX-1xlgVfjTnX&amp;start=222" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>]]></content:encoded>
    </item>
    
    <item>
      <title></title>
      <link>https://mickjagger19.github.io/posts/attachments/music/mygears/img-20240720202647625.jpg/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://mickjagger19.github.io/posts/attachments/music/mygears/img-20240720202647625.jpg/</guid>
      <description></description>
      <content:encoded><![CDATA[]]></content:encoded>
    </item>
    
    <item>
      <title></title>
      <link>https://mickjagger19.github.io/posts/graphics/shaders/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://mickjagger19.github.io/posts/graphics/shaders/</guid>
      <description>This is a personal note of some useful shader tricks
Noises Value Noise Gradient Noise Perlin Simplex Noise31 float noise( in vec3 x ) // in [0,1] { vec3 p = floor(x); vec3 f = fract(x); f = f*f*(3.0-2.0*f); float n = p.x + p.y*57.0 + 113.0*p.z; float res = mix(mix(mix( hash(n+ 0.0), hash(n+ 1.0),f.x), mix( hash(n+ 57.0), hash(n+ 58.0),f.x),f.y), mix(mix( hash(n+113.0), hash(n+114.0),f.x), mix( hash(n+170.0), hash(n+171.0),f.x),f.y),f.z); return res; } [!Hash 是不连续的 noise] value noise ?</description>
      <content:encoded><![CDATA[<p>This is a personal note of some useful shader tricks</p>
<h2 id="noises">Noises</h2>
<h3 id="value-noise">Value Noise</h3>
<h3 id="gradient-noise">Gradient Noise</h3>
<h4 id="perlin">Perlin</h4>
<h4 id="simplex">Simplex</h4>
<h4 id="noise31">Noise31</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kt">float</span> <span class="nf">noise</span><span class="p">(</span> <span class="n">in</span> <span class="n">vec3</span> <span class="n">x</span> <span class="p">)</span> <span class="c1">// in [0,1]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">vec3</span> <span class="n">p</span> <span class="o">=</span> <span class="n">floor</span><span class="p">(</span><span class="n">x</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="n">vec3</span> <span class="n">f</span> <span class="o">=</span> <span class="n">fract</span><span class="p">(</span><span class="n">x</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">f</span> <span class="o">=</span> <span class="n">f</span><span class="o">*</span><span class="n">f</span><span class="o">*</span><span class="p">(</span><span class="mf">3.0</span><span class="o">-</span><span class="mf">2.0</span><span class="o">*</span><span class="n">f</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="kt">float</span> <span class="n">n</span> <span class="o">=</span> <span class="n">p</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">p</span><span class="p">.</span><span class="n">y</span><span class="o">*</span><span class="mf">57.0</span> <span class="o">+</span> <span class="mf">113.0</span><span class="o">*</span><span class="n">p</span><span class="p">.</span><span class="n">z</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="kt">float</span> <span class="n">res</span> <span class="o">=</span> <span class="n">mix</span><span class="p">(</span><span class="n">mix</span><span class="p">(</span><span class="n">mix</span><span class="p">(</span> <span class="n">hash</span><span class="p">(</span><span class="n">n</span><span class="o">+</span>  <span class="mf">0.0</span><span class="p">),</span> <span class="n">hash</span><span class="p">(</span><span class="n">n</span><span class="o">+</span>  <span class="mf">1.0</span><span class="p">),</span><span class="n">f</span><span class="p">.</span><span class="n">x</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                        <span class="n">mix</span><span class="p">(</span> <span class="n">hash</span><span class="p">(</span><span class="n">n</span><span class="o">+</span> <span class="mf">57.0</span><span class="p">),</span> <span class="n">hash</span><span class="p">(</span><span class="n">n</span><span class="o">+</span> <span class="mf">58.0</span><span class="p">),</span><span class="n">f</span><span class="p">.</span><span class="n">x</span><span class="p">),</span><span class="n">f</span><span class="p">.</span><span class="n">y</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                    <span class="n">mix</span><span class="p">(</span><span class="n">mix</span><span class="p">(</span> <span class="n">hash</span><span class="p">(</span><span class="n">n</span><span class="o">+</span><span class="mf">113.0</span><span class="p">),</span> <span class="n">hash</span><span class="p">(</span><span class="n">n</span><span class="o">+</span><span class="mf">114.0</span><span class="p">),</span><span class="n">f</span><span class="p">.</span><span class="n">x</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                        <span class="n">mix</span><span class="p">(</span> <span class="n">hash</span><span class="p">(</span><span class="n">n</span><span class="o">+</span><span class="mf">170.0</span><span class="p">),</span> <span class="n">hash</span><span class="p">(</span><span class="n">n</span><span class="o">+</span><span class="mf">171.0</span><span class="p">),</span><span class="n">f</span><span class="p">.</span><span class="n">x</span><span class="p">),</span><span class="n">f</span><span class="p">.</span><span class="n">y</span><span class="p">),</span><span class="n">f</span><span class="p">.</span><span class="n">z</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">res</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><blockquote>
<p>[!Hash 是不连续的 noise]
value noise ?</p>
</blockquote>
<h4 id="noise21">Noise21</h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">float</span> <span class="nf">hash11</span><span class="p">(</span> <span class="kt">float</span> <span class="n">n</span> <span class="p">)</span>    <span class="c1">// in [0,1]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">fract</span><span class="p">(</span><span class="n">sin</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="o">*</span><span class="mf">43758.5453</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">vec2</span> <span class="nf">hash22</span><span class="p">(</span> <span class="n">vec2</span> <span class="n">p</span> <span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">	<span class="n">p</span> <span class="o">=</span> <span class="n">vec2</span><span class="p">(</span><span class="n">dot</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="n">vec2</span><span class="p">(</span><span class="mf">127.1</span><span class="p">,</span><span class="mf">311.7</span><span class="p">)),</span> <span class="n">dot</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="n">vec2</span><span class="p">(</span><span class="mf">269.5</span><span class="p">,</span><span class="mf">183.3</span><span class="p">)));</span>
</span></span><span class="line"><span class="cl">	<span class="k">return</span> <span class="o">-</span><span class="mf">1.0</span> <span class="o">+</span> <span class="mf">2.0</span><span class="o">*</span><span class="n">fract</span><span class="p">(</span><span class="n">sin</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="o">*</span><span class="mf">43758.5453123</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">vec3</span> <span class="nf">hash33</span><span class="p">(</span><span class="n">vec3</span> <span class="n">p3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">	<span class="n">p3</span> <span class="o">=</span> <span class="n">fract</span><span class="p">(</span><span class="n">p3</span> <span class="o">*</span> <span class="n">vec3</span><span class="p">(</span><span class="mf">.1031</span><span class="p">,</span><span class="mf">.11369</span><span class="p">,</span><span class="mf">.13787</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">    <span class="n">p3</span> <span class="o">+=</span> <span class="n">dot</span><span class="p">(</span><span class="n">p3</span><span class="p">,</span> <span class="n">p3</span><span class="p">.</span><span class="n">yxz</span><span class="o">+</span><span class="mf">19.19</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="o">-</span><span class="mf">1.0</span> <span class="o">+</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="n">fract</span><span class="p">(</span><span class="n">vec3</span><span class="p">(</span><span class="n">p3</span><span class="p">.</span><span class="n">x</span><span class="o">+</span><span class="n">p3</span><span class="p">.</span><span class="n">y</span><span class="p">,</span> <span class="n">p3</span><span class="p">.</span><span class="n">x</span><span class="o">+</span><span class="n">p3</span><span class="p">.</span><span class="n">z</span><span class="p">,</span> <span class="n">p3</span><span class="p">.</span><span class="n">y</span><span class="o">+</span><span class="n">p3</span><span class="p">.</span><span class="n">z</span><span class="p">)</span><span class="o">*</span><span class="n">p3</span><span class="p">.</span><span class="n">zyx</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kt">float</span> <span class="nf">noise</span><span class="p">(</span> <span class="n">in</span> <span class="n">vec2</span> <span class="n">p</span> <span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="kt">float</span> <span class="n">K1</span> <span class="o">=</span> <span class="mf">0.366025404</span><span class="p">;</span> <span class="c1">// (sqrt(3)-1)/2;
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">const</span> <span class="kt">float</span> <span class="n">K2</span> <span class="o">=</span> <span class="mf">0.211324865</span><span class="p">;</span> <span class="c1">// (3-sqrt(3))/6;
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>	<span class="n">vec2</span> <span class="n">i</span> <span class="o">=</span> <span class="n">floor</span><span class="p">(</span><span class="n">p</span> <span class="o">+</span> <span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="n">x</span><span class="o">+</span><span class="n">p</span><span class="p">.</span><span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="n">K1</span><span class="p">);</span>	
</span></span><span class="line"><span class="cl">    <span class="n">vec2</span> <span class="n">a</span> <span class="o">=</span> <span class="n">p</span> <span class="o">-</span> <span class="n">i</span> <span class="o">+</span> <span class="p">(</span><span class="n">i</span><span class="p">.</span><span class="n">x</span><span class="o">+</span><span class="n">i</span><span class="p">.</span><span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="n">K2</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">vec2</span> <span class="n">o</span> <span class="o">=</span> <span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">x</span><span class="o">&gt;</span><span class="n">a</span><span class="p">.</span><span class="n">y</span><span class="p">)</span> <span class="o">?</span> <span class="n">vec2</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">)</span> <span class="o">:</span> <span class="n">vec2</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">);</span> <span class="c1">//vec2 of = 0.5 + 0.5*vec2(sign(a.x-a.y), sign(a.y-a.x));
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">vec2</span> <span class="n">b</span> <span class="o">=</span> <span class="n">a</span> <span class="o">-</span> <span class="n">o</span> <span class="o">+</span> <span class="n">K2</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="n">vec2</span> <span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">-</span> <span class="mf">1.0</span> <span class="o">+</span> <span class="mf">2.0</span><span class="o">*</span><span class="n">K2</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">vec3</span> <span class="n">h</span> <span class="o">=</span> <span class="n">max</span><span class="p">(</span><span class="mf">0.5</span><span class="o">-</span><span class="n">vec3</span><span class="p">(</span><span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">a</span><span class="p">),</span> <span class="n">dot</span><span class="p">(</span><span class="n">b</span><span class="p">,</span><span class="n">b</span><span class="p">),</span> <span class="n">dot</span><span class="p">(</span><span class="n">c</span><span class="p">,</span><span class="n">c</span><span class="p">)</span> <span class="p">),</span> <span class="mf">0.0</span> <span class="p">);</span>
</span></span><span class="line"><span class="cl">	<span class="n">vec3</span> <span class="n">n</span> <span class="o">=</span> <span class="n">h</span><span class="o">*</span><span class="n">h</span><span class="o">*</span><span class="n">h</span><span class="o">*</span><span class="n">h</span><span class="o">*</span><span class="n">vec3</span><span class="p">(</span> <span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">hash</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mf">0.0</span><span class="p">)),</span> <span class="n">dot</span><span class="p">(</span><span class="n">b</span><span class="p">,</span><span class="n">hash</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="n">o</span><span class="p">)),</span> <span class="n">dot</span><span class="p">(</span><span class="n">c</span><span class="p">,</span><span class="n">hash</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mf">1.0</span><span class="p">)));</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">dot</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">vec3</span><span class="p">(</span><span class="mf">70.0</span><span class="p">));</span>	
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><h4 id="voronoise-celular">Voronoise (celular)</h4>
<p>Great For <strong>爬行动物皮肤纹理</strong>或<strong>干燥地砖</strong></p>
<h2 id="composited-noise">Composited Noise</h2>
<h3 id="fbm">FBM</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">float</span> <span class="nf">fbm</span><span class="p">(</span> <span class="n">vec3</span> <span class="n">p</span> <span class="p">)</span>    <span class="c1">// in [0,1]
</span></span></span><span class="line"><span class="cl"><span class="c1"></span><span class="p">{</span>
</span></span><span class="line"><span class="cl">	<span class="k">if</span> <span class="p">(</span><span class="n">ANIM</span><span class="p">)</span> <span class="n">p</span> <span class="o">+=</span> <span class="n">iTime</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">float</span> <span class="n">f</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">f</span>  <span class="o">=</span> <span class="mf">0.5000</span><span class="o">*</span><span class="n">noise</span><span class="p">(</span> <span class="n">p</span> <span class="p">);</span> <span class="n">p</span> <span class="o">=</span> <span class="n">m</span><span class="o">*</span><span class="n">p</span><span class="o">*</span><span class="mf">2.02</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">f</span> <span class="o">+=</span> <span class="mf">0.2500</span><span class="o">*</span><span class="n">noise</span><span class="p">(</span> <span class="n">p</span> <span class="p">);</span> <span class="n">p</span> <span class="o">=</span> <span class="n">m</span><span class="o">*</span><span class="n">p</span><span class="o">*</span><span class="mf">2.03</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">f</span> <span class="o">+=</span> <span class="mf">0.1250</span><span class="o">*</span><span class="n">noise</span><span class="p">(</span> <span class="n">p</span> <span class="p">);</span> <span class="n">p</span> <span class="o">=</span> <span class="n">m</span><span class="o">*</span><span class="n">p</span><span class="o">*</span><span class="mf">2.01</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">f</span> <span class="o">+=</span> <span class="mf">0.0625</span><span class="o">*</span><span class="n">noise</span><span class="p">(</span> <span class="n">p</span> <span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">f</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kt">float</span> <span class="nf">fbm</span><span class="p">(</span><span class="n">vec2</span> <span class="n">uv</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">	<span class="kt">float</span> <span class="n">total</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">amplitude</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">7</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">		<span class="c1">// 每一次迭代，增加一层 noise，即添加一层 color
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>		<span class="c1">// amplitude 控制噪音的 数量
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>		<span class="c1">// frequency 控制尺度的降低
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>		<span class="n">total</span> <span class="o">+=</span> <span class="n">noise</span><span class="p">(</span><span class="n">uv</span><span class="p">)</span> <span class="o">*</span> <span class="n">amplitude</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">		<span class="n">uv</span> <span class="o">=</span> <span class="n">frequency</span> <span class="o">*</span> <span class="n">uv</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">		<span class="n">amplitude</span> <span class="o">*=</span> <span class="mf">0.</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">	<span class="p">}</span>
</span></span><span class="line"><span class="cl">	<span class="k">return</span> <span class="n">total</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>自然界的形状可以理解为 不同数量 * 不同尺度的形状 的叠加</p>
<blockquote>
<p>a simple sum of noise waves with <strong>increasing</strong> frequencies and <strong>decreasing</strong> amplitudes.</p>
</blockquote>
<p>















  
  
      
      
  <picture align=center  class="d-block text-center">
  <img class="img-fluid" src="https://mickjagger19.github.io/Attachments/Graphics/Shaders/IMG-20240718205918443.png?v=94562a842ccb99020a904374509ca22e" alt="" loading="lazy" height="466px" width="2704px" />
</picture>
</p>
<h2 id="shapes">Shapes</h2>
<h3 id="圆环">圆环</h3>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl">   <span class="n">col</span> <span class="o">=</span> <span class="mf">5.</span> <span class="o">/</span> <span class="n">abs</span><span class="p">(</span> <span class="n">length</span><span class="p">(</span> <span class="n">uv</span> <span class="o">+</span>  <span class="n">center</span> <span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span> <span class="p">)</span> <span class="o">/</span> <span class="mf">4e2</span><span class="p">;</span>
</span></span></code></pre></div><p>















  
  </p>
<p>















  
  
      
      
  <picture align=center  class="d-block text-center">
  <img class="img-fluid" src="https://mickjagger19.github.io/Attachments/Graphics/Shaders/IMG-20240718223124729.png?v=94562a842ccb99020a904374509ca22e" alt="" loading="lazy" height="162px" width="156px" />
</picture>
</p>
]]></content:encoded>
    </item>
    
    <item>
      <title></title>
      <link>https://mickjagger19.github.io/posts/immusia-todolist/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://mickjagger19.github.io/posts/immusia-todolist/</guid>
      <description>Immusia
光照增强
镜面反射
相机高度
Endless
Spatial size: 决定一小块的面积，其中一小块中的波浪细节固定
The width of the ocean surface area being simulated, in meters. This also determines the size of the generated mesh, or the displaced area. Of course, you can scale the object with the Ocean modifier in Object Mode to tweak the apparent size in your scene.
size: scale，(displace 下)扩大原有object,可能导致mesh细节丢失
Environments:
West lake Outside earth, rotating Plastic Beach Rooftop The Wall 其中真实场景考虑使用 photogremetry</description>
      <content:encoded><![CDATA[<p><strong>Immusia</strong></p>
<ul>
<li>
<p>光照增强</p>
</li>
<li>
<p>镜面反射</p>
</li>
<li>
<p>相机高度</p>
</li>
<li>
<p>Endless</p>
</li>
</ul>
<p>Spatial size: 决定一小块的面积，其中一小块中的波浪细节固定</p>
<p>The width of the ocean surface area being simulated, in meters. This also determines the size of the generated mesh, or the displaced area. Of course, you can scale the object with the Ocean modifier in Object Mode to tweak the apparent size in your scene.</p>
<p>size: scale，(displace 下)扩大原有object,可能导致mesh细节丢失</p>
<p><strong>Environments:</strong></p>
<ol>
<li>West lake</li>
<li>Outside earth, rotating</li>
<li>Plastic Beach</li>
<li>Rooftop</li>
<li>The Wall</li>
</ol>
<p>其中真实场景考虑使用 photogremetry</p>
<p>水面场景：待定</p>
<p>虚拟场景：待定</p>
<p><strong>Gesture:</strong></p>
<p>使用  Gesture 引导视频，考虑在模拟器中通过手势捕捉来录制</p>
<p>使用特定 gesture 完成 immersive space 的进入</p>
<p><strong>Sound Effects:</strong></p>
<ol>
<li>Artwork moves</li>
<li>Artwork fades</li>
<li>Environment choosing</li>
<li></li>
<li>Stereo videos
<ul>
<li>Strawberry fields</li>
<li>Imagine</li>
<li>Eric Clapton</li>
<li>Dirty Harry</li>
<li>Something</li>
<li>Song 2</li>
</ul>
</li>
<li>宣传视频</li>
<li>订阅管理</li>
</ol>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
