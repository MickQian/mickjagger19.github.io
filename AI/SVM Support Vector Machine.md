# Glossary
#### Norm 范数
向量范数： 向量大小
1-范数：各维元素的绝对值之和
2-范数：欧几里得范数，平方和开方



小样本方法
一个目标是对新样本数据进行 *二分类* 的（线性）、有监督、深度学习模型
凸优化问题的解是**全局最优**

场景：
* 测试数据 **线性可分** ：求出超平面、支持向量，约束为硬间隔尽量大。只有此时才为线性模型，即线性可分 VM
* 测试数据 **近似线性可分** ：软间隔最大化，引入松弛因子，线性 VM
* 测试数据 **线性不可分** ： 引入核函数，投射到高维空间，非线性 VM

间隔最大化的目的：基于误分类最小策略，求得的超平面构成最优分离，泛化能力最强

## Linear
求：
间隔最大化 ：正负超平面 的距离最大
$S = \frac{1}{2} || \omega^{2} ||  (1)$ 

$s.t.\ \     y * (wx + b) >= 1 (2)$

### Lagrange multipliers
寻找多元函数在一组约束下的极值的方法
通过引入拉格朗日乘子，可将有 $d$ 个变量与 $k$ 个**约束**条件的最优化问题转化为具有 $d+k$ 个变量的**无约束**优化问题求解.

简单推导：
设函数为$f(x)$，约束为 $g(x) \le0$

如果是等式约束： g(x) == 0
要使多元函数取得极值，极值点 $x^*$ 对于函数和约束的梯度必定同向（函数的等高线与约束线相切） 
则存在 $\lambda$ 使得:
$\nabla f(x^*) + \lambda\nabla g(x^*) = 0$
拉格朗日函数：
$L(x, \lambda) = f(x) + \lambda g(x)$
可以看出， 拉格朗日函数对 $x$ 的偏导 在 $x^*$ 处为 0，充分且必要关系，因此问题转换为 求 拉格朗日函数的极值（无约束优化）

但此处为不等式约束：
对于约束 $g(x) \le0$， 若 $g(x^*)$
*  == 0，等式约束
* < 0，不等约束不起作用，直接求 $\nabla f(x) = 0$

经过 Lagrange multipliers 之后的目标为：
![[Pasted image 20230611140541.png]]其中，min 代表间隔最大化，max 代表最大化

在满足 Slater 定理，且过程满足 KKT 条件时，可以转换为对偶问题：
![[Pasted image 20230611141039.png]]

先求内部最小，对参数求偏导分别等于0 后代入外部式子，得到：
![[Pasted image 20230611141149.png]]
此时要求 $\alpha^*$， 利用 SMO 算法

SMO： 每次选择两个变量 $\alpha$ 选取的两个变量所对应的样本之间间隔要尽可能大，因为这样更新会带给目标函数值更大的变化。**SMO算法之所以高效，是因为仅优化两个参数的过程实际上仅有一个约束条件，其中一个可由另一个表示，这样的二次规划问题具有闭式解。**






## 近似线性可分，线性 VM
一味追求线性可分可能导致过拟合
因此，允许少量样本分类错误
修改目标函数，引入松弛变量误差 $\epsilon$，注意此时 z < 1: 
![[Pasted image 20230611141623.png]]
hinge loss: 
$l = max(0, 1- z)$
引入惩罚参数  C: 
![[Pasted image 20230611141800.png]]

上式右边为 所有样本的损失函数，也即分类误差之和，也即在给定 w 下的误差

模型的目标是：损失函数最小化。因此如果 C 很大，模型会让损失尽量小，如果损失和为0，那么也变成硬间隔问题，追求样本分类正确而忽略间隔（正常的C 会导致模型选择一些误差点来获得稍大的间隔）；C 很小( == 0 ) 会导致 其重心放在 间隔最大化，而忽略样本分类正确（导致模型选择较大的间隔，然而误差点较多）


不同的 C，会使上式右边的极值点 $w^*$不同，也即最终超平面的结果不同，因此 C 的选取需要合理


求解方式类似 Linear VM
![[Pasted image 20230611144017.png]]

## 线性不可分，非线性 VM
将数据映射到高维空间，使其在该空间线性可分
核函数：输入为两个低位空间变量，输出为 两个变量映射到高维空间后的内积
验证： K(x,z) 对应的 gram 矩阵是半正定的，则 K 是正定核

常用 ：线性核（无法解决非线性可分问题）， RBF 核（计算慢）


无需求解真正的映射函数，而只需要知道其核函数， 因为
$K(x, y) = <\phi(x), \phi(y)>$
其中 $\phi(x)$ 表示 真正的映射函数，它们的内积可以通过核函数直接计算

而内积是 SVM 问题的常用操作，可以解决线性可分问题
