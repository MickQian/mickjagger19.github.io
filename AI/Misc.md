
### logits
对数分数

### cross entropy
衡量两个 distribution 分布之间的一致程度


embedding = encode ?

prior: $p(x | \theta$) 历史 -> 因
posterior: $p(\theta)$  果 -> 因
likelihood: $p(\theta | x)$   因 -> 果 


## Bayes 



