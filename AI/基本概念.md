## Activation Function 激活函数
非线性变换
* sigmoid
* tanh
* relu
* prelu
* elu
* maxout


## Normalization 归一化
* z-score
* min-max
	![[Pasted image 20230510221729.png]]
* softmax: normalized exponential function  

## Feed
### Feed forward


### Feed back

## Overfitting
在相对较少的数据集上训练大型网络
为了防止 过拟合，需要防止模型变得过于复杂： 
* 减少参数
* 惩罚复杂性: weight decay decay
	* $Loss = MSE + wd * sum(w^2) $
	* 把参数的平方和加入 Loss 函数中
	* 如何决定 wd(weight decay)?  0.1 左右
* dropout： regularization method
	* 迫使同层节点对输出承担或多或少的责任，增强模型的泛化性，因为它不会太依赖某些局部的特征
	* 方式：引入一个参数，指定某层某个节点输出被丢弃的概率(0.5)， rescale
* ensemble: 在同一数据集上拟合不同模型，对每个模型的预测取平均值
	* 问题：时间长


### Batch Normalization 
对**每一批**训练数据进行归一化，增加模型的鲁棒性
不是全量数据集，因为容易过拟合
 




